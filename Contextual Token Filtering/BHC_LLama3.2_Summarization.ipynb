{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215db1a4-1c1b-4539-a86c-72269d8a78e0",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121dc8ef-ef1c-462b-90dd-450a0c4245f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba1083-1d25-4380-850b-7c829c5e1615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'mimicivliza'  # Replace with your bucket name\n",
    "mimic_iv_bhc = f's3://{bucket_name}/sample_data_100.csv'\n",
    "\n",
    "# Load the files\n",
    "mimic_iv_bhc_100 = pd.read_csv(mimic_iv_bhc)\n",
    "\n",
    "# Display the data\n",
    "mimic_iv_bhc_100.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199bd63-cba4-4e32-aa70-b4647fadfe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a474a-5b2c-46bd-94e2-bdf6982bf803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use token from environment variable (safer)\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83855680-8a92-4de0-b01d-26fef625a6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mimic_iv_bhc_100.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9993a7-2e70-4555-8bf3-641d46cf8939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize summarization pipeline\n",
    "summarizer = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,  # Slightly more restrictive sampling\n",
    "    \"temperature\": 0.7,  # Less randomness for focused outputs\n",
    "    \"top_k\": 40,  # Fewer options for next token selection\n",
    "    \"max_new_tokens\": 200,  # Limit maximum length of generated text\n",
    "    \"repetition_penalty\": 1.1  # Discourage repeating content\n",
    "}\n",
    "\n",
    "# Few-shot examples for summarization\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine. She reported increasing discomfort despite over-the-counter pain relievers. Neurological exam was unremarkable, and there were no signs of cord compression. Pain management and radiation oncology were consulted, and palliative radiation therapy was planned. The patient also discussed advanced care planning during her stay.\",\n",
    "        \"summary\": \"A 45-year-old female with metastatic breast cancer presented with worsening back pain. Imaging showed thoracic spine fractures, and she received palliative radiation therapy.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"<SEX> M <SERVICE> CARDIOLOGY <CHIEF COMPLAINT> shortness of breath <HISTORY OF PRESENT ILLNESS> A 60-year-old male with a history of hypertension and diabetes presented with progressive shortness of breath over the past month. Echocardiogram showed reduced ejection fraction. The patient was started on diuretics and beta-blockers. Cardiology team planned for close outpatient follow-up.\",\n",
    "        \"summary\": \"\"  # Leave empty for model to infer\n",
    "    }\n",
    "]\n",
    "\n",
    "# Construct few-shot prompt\n",
    "def construct_prompt(input_text):\n",
    "    prompt = \"Summarize the following clinical note. Do not repeat input or prompt.\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nSummary: {example['summary']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Lists to store latency and throughput measurements\n",
    "latencies = []\n",
    "throughputs = []\n",
    "\n",
    "# Summarization function with timing\n",
    "def summarize_text(text):\n",
    "    prompt = construct_prompt(text)\n",
    "    \n",
    "    # Count input tokens\n",
    "    input_tokens = len(tokenizer.encode(prompt))\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    output = summarizer(prompt, **generation_params)[0][\"generated_text\"]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate latency in milliseconds\n",
    "    latency = (end_time - start_time) * 1000\n",
    "    \n",
    "    # Count output tokens (new tokens generated)\n",
    "    generated_text = output.split(\"Summary:\")[-1].strip()\n",
    "    output_tokens = len(tokenizer.encode(generated_text))\n",
    "    \n",
    "    # Calculate throughput (tokens per second)\n",
    "    throughput = output_tokens / (latency / 1000)\n",
    "    \n",
    "    # Store measurements\n",
    "    latencies.append(latency)\n",
    "    throughputs.append(throughput)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Apply summarization with progress bar\n",
    "tqdm.pandas(desc=\"Summarizing clinical notes\")  # Enable tqdm for pandas\n",
    "mimic_iv_bhc_100[\"generated_summary\"] = mimic_iv_bhc_100[\"input\"].progress_apply(summarize_text)\n",
    "\n",
    "# Calculate and print statistics\n",
    "mean_latency = np.mean(latencies)\n",
    "std_latency = np.std(latencies)\n",
    "mean_throughput = np.mean(throughputs)\n",
    "std_throughput = np.std(throughputs)\n",
    "\n",
    "print(\"\\n--- Performance Metrics ---\")\n",
    "print(f\"Throughput: {mean_throughput:.2f} ± {std_throughput:.2f} tokens/sec\")\n",
    "print(f\"Latency: {mean_latency:.2f} ± {std_latency:.2f} ms\")\n",
    "print(f\"\\nFor Table:\")\n",
    "print(f\"${mean_throughput:.2f} \\\\pm {std_throughput:.2f}$ & ${mean_latency:.2f} \\\\pm {std_latency:.2f}$ \\\\\\\\\")\n",
    "\n",
    "# Save results\n",
    "mimic_iv_bhc_100[\"generated_summary\"] = mimic_iv_bhc_100[\"input\"].progress_apply(summarize_text)\n",
    "mimic_iv_bhc_100.to_csv(\"generated_summaries_100.csv\", index=False)\n",
    "print(\"\\nSummarization complete. Results saved to 'generated_summaries_100.csv'.\")\n",
    "\n",
    "# Optional: Save metrics to a separate file\n",
    "with open(\"performance_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Model: {model_name}\\n\")\n",
    "    f.write(f\"Parameters: Token=200, Temp=0.7\\n\")\n",
    "    f.write(f\"Throughput: {mean_throughput:.2f} ± {std_throughput:.2f} tokens/sec\\n\")\n",
    "    f.write(f\"Latency: {mean_latency:.2f} ± {std_latency:.2f} ms\\n\")\n",
    "    f.write(f\"Table format: ${mean_throughput:.2f} \\\\pm {std_throughput:.2f}$ & ${mean_latency:.2f} \\\\pm {std_latency:.2f}$ \\\\\\\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a4c9e-f309-4a37-a451-08c69e2e91d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mimic_iv_bhc_100.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67480439-b0fd-43f7-ab11-b3a1612ab7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mimic_iv_bhc_100['input'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832afb94-3cee-44ef-8cc0-8ee6de77833d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mimic_iv_bhc_100['generated_summary'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30f2b7-5d64-4920-a2b2-951c6059c661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # Use FP16 for reduced memory\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Fix the padding warning\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize the pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine. She reported increasing discomfort despite over-the-counter pain relievers. Neurological exam was unremarkable, and there were no signs of cord compression. Pain management and radiation oncology were consulted, and palliative radiation therapy was planned. The patient also discussed advanced care planning during her stay.\",\n",
    "        \"target\": \"A 45-year-old female with metastatic breast cancer presented with worsening back pain. Imaging showed thoracic spine fractures, and she received palliative radiation therapy.\"\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Combine few-shot examples into a prompt template\n",
    "def construct_few_shot_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Summarize. Do not include prompt/Input in your generated summary\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,  # Slightly more restrictive sampling\n",
    "    \"temperature\": 0.7,  # Less randomness for focused outputs\n",
    "    \"top_k\": 40,  # Fewer options for next token selection\n",
    "    \"max_new_tokens\": 40,  # Limit maximum length of generated text\n",
    "    \"repetition_penalty\": 1.1  # Discourage repeating content\n",
    "}\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = mimic_iv_bhc_100[\"input\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "def clean_generated_text(generated_text):\n",
    "    # Extract only the generated summary part\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Process a batch of inputs\n",
    "def process_batch(batch):\n",
    "    try:\n",
    "        # Construct few-shot prompts for the batch\n",
    "        prompts = [construct_few_shot_prompt(input_text) for input_text in batch]\n",
    "        \n",
    "        # Generate outputs\n",
    "        outputs = summarizer(\n",
    "            prompts,\n",
    "            **generation_params,\n",
    "        )\n",
    "        return outputs if isinstance(outputs, list) else [outputs]\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"OOM error, retrying with batch size {len(batch)//2}\")\n",
    "            if len(batch) > 1:\n",
    "                half = len(batch) // 2\n",
    "                return (\n",
    "                    process_batch(batch[:half]) +\n",
    "                    process_batch(batch[half:])\n",
    "                )\n",
    "        raise e\n",
    "\n",
    "# Process with progress bar\n",
    "with tqdm(total=len(inputs), desc=\"Processing Rows\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        \n",
    "        # Process batch\n",
    "        summaries = process_batch(batch)\n",
    "        \n",
    "        # Extract and clean generated text dynamically\n",
    "        if isinstance(summaries[0], dict) and 'generated_text' in summaries[0]:\n",
    "            generated_texts = [clean_generated_text(summary['generated_text']) for summary in summaries]\n",
    "        elif isinstance(summaries[0], list) and isinstance(summaries[0][0], dict) and 'generated_text' in summaries[0][0]:\n",
    "            generated_texts = [clean_generated_text(summary[0]['generated_text']) for summary in summaries]\n",
    "        elif isinstance(summaries[0], str):\n",
    "            generated_texts = [clean_generated_text(summary) for summary in summaries]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected format for summaries.\")\n",
    "        \n",
    "        # Append cleaned summaries to results\n",
    "        generated_summaries.extend(generated_texts)\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update progress\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# Add generated summaries to DataFrame and save\n",
    "mimic_iv_bhc_100[\"generated_summary\"] = generated_summaries\n",
    "mimic_iv_bhc_100.to_csv(\"generated_summaries_100.csv\", index=False)\n",
    "print(\"Summaries saved to 'generated_summaries_100.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8f0b9-137f-4203-9bd3-18d7e20ae377",
   "metadata": {},
   "source": [
    "### Code with computational efficiency metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820e8ca-2ff7-4a4e-b777-bffba2f1f35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize summarization pipeline\n",
    "summarizer = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,  # Slightly more restrictive sampling\n",
    "    \"temperature\": 0.9,  # Less randomness for focused outputs\n",
    "    \"top_k\": 40,  # Fewer options for next token selection\n",
    "    \"max_new_tokens\": 300,  # Limit maximum length of generated text\n",
    "    \"repetition_penalty\": 1.1  # Discourage repeating content\n",
    "}\n",
    "\n",
    "# Data inputs\n",
    "inputs = mimic_iv_bhc_100[\"input\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "# Metrics tracking\n",
    "total_tokens_generated = 0\n",
    "total_input_tokens = 0\n",
    "total_time_spent = 0\n",
    "time_to_first_token = []\n",
    "throughput_list = []\n",
    "\n",
    "# Few-shot prompt constructor\n",
    "def construct_few_shot_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Please summarize the following input concisely:\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Process a batch of inputs\n",
    "def process_batch(batch):\n",
    "    prompts = [construct_few_shot_prompt(input_text) for input_text in batch]\n",
    "    batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "    batch_start_time = time.time()  # Start timing for the batch\n",
    "    outputs = []\n",
    "\n",
    "    # Generate outputs with timing for TTFT\n",
    "    for prompt in prompts:\n",
    "        single_start_time = time.time()  # Start timing for TTFT\n",
    "        output = summarizer(prompt, **generation_params)\n",
    "        single_end_time = time.time()  # End timing for TTFT\n",
    "\n",
    "        ttft = single_end_time - single_start_time  # Time to first token\n",
    "        ttft_list.append(ttft)  # Store TTFT for this input\n",
    "        outputs.append(output)\n",
    "\n",
    "    batch_end_time = time.time()  # End timing for the batch\n",
    "    batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "    # Flatten nested outputs if necessary\n",
    "    if isinstance(outputs[0], list):\n",
    "        outputs = [item for sublist in outputs for item in sublist]\n",
    "\n",
    "    # Extract summaries and count output tokens\n",
    "    if isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
    "        summaries = [output['generated_text'] for output in outputs]\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected format for outputs: {outputs}\")\n",
    "\n",
    "    # Update metrics\n",
    "    global total_tokens_generated, total_input_tokens, total_time_spent, total_summaries\n",
    "    total_tokens_generated += batch_output_tokens\n",
    "    total_input_tokens += batch_input_tokens\n",
    "    total_time_spent += batch_time\n",
    "    total_summaries += len(batch)  # Count the number of summaries processed\n",
    "    time_per_input.append(batch_time / len(batch))  # Latency per input (full summary time)\n",
    "    throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_time)  # Tokens/second\n",
    "\n",
    "    return summaries\n",
    "\n",
    "# Initialize metrics\n",
    "total_summaries = 0\n",
    "time_per_input = []  # Latency per summary\n",
    "ttft_list = []  # Time to first token\n",
    "throughput_list = []\n",
    "\n",
    "# Process inputs in batches\n",
    "batch_size = 8\n",
    "with tqdm(total=len(inputs), desc=\"Processing Rows\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        generated_summaries.extend(process_batch(batch))\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(time_per_input) / len(time_per_input)  # Average time per summary\n",
    "average_ttft = sum(ttft_list) / len(ttft_list)  # Average TTFT\n",
    "average_throughput = sum(throughput_list) / len(throughput_list)  # Average tokens per second\n",
    "token_efficiency = total_tokens_generated / total_input_tokens  # Output/Input token ratio\n",
    "\n",
    "# Print metrics\n",
    "print(f\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_tokens_generated}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "mimic_iv_bhc_100[\"generated_summary\"] = generated_summaries\n",
    "mimic_iv_bhc_100.to_csv(\"generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9123090-a28f-4f06-a6b2-6b11546ec6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8e75c-ffd0-4abe-919f-0b3a6ca0f533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965c0d7-580e-4b98-aa11-36463ee06c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011ac18-7527-4cb0-b23e-eb0dc9dca337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf59c1-b9e5-42e7-8c01-90836dc22557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b11220-4f29-4c55-b678-ca92febc02af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23752dc7-c1be-44ec-9a0a-bb49408e7f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation of long text\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping of content\n",
    "\n",
    "print(mimic_iv_bhc_100['generated_summary'].iloc[9])  # Replace 9 with the desired row index\n",
    "  # Remember: Index starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9dbc10-e1b2-4226-b7f6-3e148508c78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
