{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0917618-2dd8-4857-aada-c25325a8b1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f4fc3-4de8-423d-99a8-0c6aefa2a25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'mimicivliza'  # Replace with your bucket name\n",
    "mimic_iv_bhc = f's3://{bucket_name}/sample_data_100.csv'\n",
    "\n",
    "# Load the files\n",
    "mimic_iv_bhc_100 = pd.read_csv(mimic_iv_bhc)\n",
    "\n",
    "# Display the data\n",
    "#mimic_iv_bhc_100.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8ea37-f6b4-4274-9345-b529891dc5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"DataFrame shape:\", mimic_iv_bhc_100.shape)\n",
    "print(\"\\nColumns:\", mimic_iv_bhc_100.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20627f7-dd51-433a-8d02-d25fdf107fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation of long text\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping of content\n",
    "\n",
    "#print(mimic_iv_bhc_100.iloc[9])  # Replace 9 with the desired row index\n",
    "  # Remember: Index starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d45d7-d40c-4d26-9316-eef7e7b3d50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54588208-3321-4f5b-a9d9-9fc93cc5813a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use token from environment variable (safer)\n",
    "login(os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab7e68-bc56-4e89-9e98-fa008e38d44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2f5bf-c97f-4946-9e6b-3f86dcc1a6cd",
   "metadata": {},
   "source": [
    "### Configuring 8-Bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9017d-5be4-41c2-a179-c90fd635f430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configure quantization (8-bit)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "print(\"Environment setup and quantization configuration done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cc74f-7d2c-42c2-8a9f-d072ce047ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "with tqdm(total=2, desc=\"Initializing Model and Tokenizer\", unit=\"step\") as pbar:\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        output_attentions=True,  # Enable attention outputs for AGTD\n",
    "        return_dict_in_generate=True  # Ensures attention outputs are generated\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    pbar.update(1)\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270764e1-12ad-4413-a0b3-2ceace756614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(\"Summarization pipeline initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42d27e-60f8-4210-a376-8ef17833531a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine few-shot examples into a prompt template\n",
    "\n",
    "\n",
    "def construct_few_shot_prompt(row_input):\n",
    "    \"\"\"\n",
    "    Constructs a few-shot prompt dynamically using the row input.\n",
    "\n",
    "    Args:\n",
    "    - row_input (str): The input text from the dataframe row.\n",
    "\n",
    "    Returns:\n",
    "    - str: The constructed prompt for the model.\n",
    "    \"\"\"\n",
    "    prompt = \"You are a medical expert. Please summarize the following input concisely:\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {row_input}\\nSummary:\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc277d6-fa56-4527-8790-d16d402a5203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Optional\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "class AGTDSummarizer:\n",
    "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    def calculate_importance(\n",
    "        self, \n",
    "        tokens: torch.Tensor, \n",
    "        alpha: float = 0.5\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate token importance scores using attention weights and positional bias.\n",
    "        \n",
    "        Args:\n",
    "            tokens: Input token ids.\n",
    "            alpha: Weight factor for layer importance.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of importance scores for each token.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(tokens, output_attentions=True)\n",
    "                attentions = outputs.attentions\n",
    "                \n",
    "                importance_scores = torch.zeros(tokens.size(-1), device=tokens.device)\n",
    "                num_layers = len(attentions)\n",
    "                \n",
    "                # Generate positional weights (linear decay from 1.0 to 0.5)\n",
    "                position_weights = torch.linspace(1.0, 0.5, steps=tokens.size(-1), device=tokens.device)\n",
    "\n",
    "                for l, layer_attention in enumerate(attentions):\n",
    "                    # Calculate layer weight with position-based scaling\n",
    "                    layer_weight = alpha + (1 - alpha) * (l + 1) / num_layers\n",
    "                    \n",
    "                    # Average attention across heads and batches\n",
    "                    avg_attention = layer_attention.mean(dim=1).squeeze()\n",
    "                    token_importance = avg_attention.mean(dim=-1)\n",
    "                    \n",
    "                    # Add positional weighting to importance scores\n",
    "                    importance_scores += layer_weight * token_importance * position_weights\n",
    "                \n",
    "                return importance_scores\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating importance scores: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def summarize(\n",
    "        self,\n",
    "        text: str,\n",
    "        retention_ratio: float = 0.7,\n",
    "        alpha: float = 0.5,\n",
    "        max_length: int = 2048\n",
    "    ) -> Tuple[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Perform Attention-Guided Token Dropping to summarize text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to summarize.\n",
    "            retention_ratio: Fraction of tokens to retain.\n",
    "            alpha: Weight factor for layer importance.\n",
    "            max_length: Maximum input length.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (summarized text, importance scores).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize input text\n",
    "            tokens = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Calculate number of tokens to keep\n",
    "            n = tokens.input_ids.size(-1)\n",
    "            k = int(n * retention_ratio)\n",
    "\n",
    "            # Get importance scores and top k indices\n",
    "            importance_scores = self.calculate_importance(tokens.input_ids, alpha)\n",
    "            _, indices = torch.sort(importance_scores, descending=True)\n",
    "            keep_indices = sorted(indices[:k].tolist())\n",
    "\n",
    "            # Create reduced token sequence\n",
    "            reduced_tokens = tokens.input_ids[0][keep_indices]\n",
    "            \n",
    "            # Decode back to text\n",
    "            reduced_text = self.tokenizer.decode(reduced_tokens)\n",
    "\n",
    "            return reduced_text, importance_scores.tolist()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in summarization process: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def test_summarizer(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    test_text: Optional[str] = None\n",
    "):\n",
    "    \"\"\"Test the AGTD summarizer with sample text.\"\"\"\n",
    "    \n",
    "    if test_text is None:\n",
    "        test_text = \"<SEX> F <SERVICE> SURGERY <CHIEF COMPLAINT> abdominal pain\"\n",
    "    \n",
    "    summarizer = AGTDSummarizer(model, tokenizer)\n",
    "    \n",
    "    try:\n",
    "        reduced_text, importance_scores = summarizer.summarize(\n",
    "            test_text,\n",
    "            retention_ratio=0.7\n",
    "        )\n",
    "        \n",
    "        print(\"Original Text:\\n\", test_text)\n",
    "        print(\"\\nReduced Text:\\n\", reduced_text)\n",
    "        print(\"\\nToken Importance Scores:\", importance_scores[:10])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing summarizer: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58db52-339b-42ba-9c63-b275716ec868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_dataset_with_agtd(\n",
    "    df: pd.DataFrame,\n",
    "    summarizer: AGTDSummarizer,\n",
    "    input_column: str = \"input\",\n",
    "    retention_ratio: float = 0.7,\n",
    "    alpha: float = 0.5,\n",
    "    max_length: int = 2048\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply AGTD to a dataset and add reduced text and importance scores as new columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe with an `input` column containing text data.\n",
    "        summarizer: An instance of the AGTDSummarizer class.\n",
    "        input_column: Column name in the dataframe containing the input text.\n",
    "        retention_ratio: Fraction of tokens to retain during summarization.\n",
    "        alpha: Weight factor for layer importance.\n",
    "        max_length: Maximum input length for tokenization.\n",
    "    \n",
    "    Returns:\n",
    "        Updated dataframe with new columns: `reduced_text` and `importance_scores`.\n",
    "    \"\"\"\n",
    "    reduced_texts = []\n",
    "    importance_scores_list = []\n",
    "\n",
    "    print(\"Processing dataset with AGTD...\")\n",
    "    with tqdm(total=len(df), desc=\"Processing Dataset\", unit=\"row\") as pbar:\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[input_column]\n",
    "            try:\n",
    "                # Summarize using AGTDSummarizer\n",
    "                reduced_text, importance_scores = summarizer.summarize(\n",
    "                    text,\n",
    "                    retention_ratio=retention_ratio,\n",
    "                    alpha=alpha,\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                reduced_texts.append(reduced_text)\n",
    "                importance_scores_list.append(importance_scores)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {text[:50]}... Error: {e}\")\n",
    "                reduced_texts.append(\"\")\n",
    "                importance_scores_list.append([])\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Add results as new columns to the dataframe\n",
    "    df[\"reduced_text\"] = reduced_texts\n",
    "    df[\"importance_scores\"] = importance_scores_list\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df9e80-99aa-49c7-8c67-44c5f5259667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the dataset to reduce text dynamically\n",
    "processed_df = process_dataset_with_agtd(\n",
    "    df=mimic_iv_bhc_100,\n",
    "    summarizer=AGTDSummarizer(model, tokenizer),\n",
    "    input_column=\"input\",  # The column containing input text\n",
    "    retention_ratio=0.7,  # Retain 80% of the most important tokens\n",
    "    alpha=0.5,            # Importance weighting factor\n",
    "    max_length=2048       # Max tokenization length\n",
    ")\n",
    "\n",
    "# View one reduced text\n",
    "print(processed_df[\"reduced_text\"].iloc[0])  # Replace 0 with the desired row index\n",
    "# Save the processed DataFrame to a CSV file\n",
    "processed_df.to_csv(\"processed_reduced_texts.csv\", index=False)\n",
    "\n",
    "print(\"Processed DataFrame saved to 'processed_reduced_texts.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d59a6-ecd0-400c-bcd8-9053018a76b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "Reduced_Text = \"processed_reduced_texts.csv\"\n",
    "processed_df.to_csv(Reduced_Text, index=False)\n",
    "\n",
    "print(f\"Reduced text have been saved to '{Reduced_Text}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7d8cd-a257-47bc-8681-9147374d0f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#processed_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116842e1-e78e-4587-9e4e-e2a41fc4ca56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct prompts for the batch\n",
    "        prompts = [f\"You are a medical expert. {text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Generate summaries and measure TTFT and latency\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "            output = summarizer(prompt, **generation_params)\n",
    "            single_end_time = time.time()\n",
    "            ttft_list.append(single_end_time - single_start_time)  # Time to first token\n",
    "            summaries.append(output[0][\"generated_text\"])  # Extract generated text\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # Calculate batch metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total time for the batch\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # Update global metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # Store summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "processed_df.to_csv(\"generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12719c2-6705-44dc-b62d-f1a50e4b4d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eaae9b-8642-49b8-a02f-000efb378642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6562d3-8b74-4df0-80c7-29f6d5e26d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8f8b9-1bd7-4d5f-bbad-f30e3f7b382c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8828d-9976-4afd-adf5-5eda7183da86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d1e38-a4fa-44aa-91ac-314fe4c4fe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9ca37-1b97-4293-807b-508c970b62c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142c00c-dd24-42df-815e-fd0393c46602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00323c51-2bfc-4626-b744-068a701af091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25b28f-d162-467c-ac34-b1c7a5453c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153f13f-49da-464a-9fed-ebfe849db1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine.\",\n",
    "        \"target\": \"The patient was admitted to oncology for worsening back pain. Imaging revealed metastatic cancer with thoracic spine involvement. She was started on pain management and referred for palliative radiation therapy.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"<SEX> M <SERVICE> CARDIOLOGY <CHIEF COMPLAINT> chest pain <HISTORY OF PRESENT ILLNESS> A 55-year-old male presented with chest pain radiating to the left arm and jaw. Initial ECG showed ST-segment elevation in the inferior leads. Troponin levels were elevated.\",\n",
    "        \"target\": \"The patient presented to cardiology with chest pain consistent with acute myocardial infarction. He was taken emergently to the cath lab for primary PCI, and a stent was placed in the right coronary artery.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Define parse_summary_output\n",
    "def parse_summary_output(output):\n",
    "    if isinstance(output, dict) and \"generated_text\" in output:\n",
    "        return output[\"generated_text\"].split(\"Summary:\")[-1].strip()\n",
    "    elif isinstance(output, str):\n",
    "        return output.split(\"Summary:\")[-1].strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Few-shot prompt constructor\n",
    "def construct_few_shot_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Please summarize the following input concisely:\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct prompts for the batch\n",
    "        prompts = [construct_few_shot_prompt(text) for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Generate summaries and measure TTFT and latency\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "            output = summarizer(prompt, **generation_params)\n",
    "            single_end_time = time.time()\n",
    "            ttft_list.append(single_end_time - single_start_time)  # Time to first token\n",
    "            summaries.append(parse_summary_output(output[0]))\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # Calculate batch metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total time for the batch\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # Update global metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # Store summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "processed_df.to_csv(\"generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe7258-9e88-4d7f-8913-ecd6c8f25f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine.\",\n",
    "        \"target\": \"The patient was admitted to oncology for worsening back pain. Imaging revealed metastatic cancer with thoracic spine involvement. She was started on pain management and referred for palliative radiation therapy.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"<SEX> M <SERVICE> CARDIOLOGY <CHIEF COMPLAINT> chest pain <HISTORY OF PRESENT ILLNESS> A 55-year-old male presented with chest pain radiating to the left arm and jaw. Initial ECG showed ST-segment elevation in the inferior leads. Troponin levels were elevated.\",\n",
    "        \"target\": \"The patient presented to cardiology with chest pain consistent with acute myocardial infarction. He was taken emergently to the cath lab for primary PCI, and a stent was placed in the right coronary artery.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Define parse_summary_output\n",
    "def parse_summary_output(output):\n",
    "    if isinstance(output, dict) and \"generated_text\" in output:\n",
    "        return output[\"generated_text\"].split(\"Summary:\")[-1].strip()\n",
    "    elif isinstance(output, str):\n",
    "        return output.split(\"Summary:\")[-1].strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct prompts for the batch\n",
    "        prompts = [construct_few_shot_prompt(text) for text in batch]\n",
    "\n",
    "        # Generate summaries for the batch\n",
    "        summaries = summarizer(prompts, **generation_params)\n",
    "\n",
    "        # Parse and store summaries\n",
    "        for summary in summaries:\n",
    "            generated_summaries.append(parse_summary_output(summary[0]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Inspect the summaries\n",
    "#print(\"\\nSample of generated sammaries:\")\n",
    "#print(processed_df[[\"reduced_text\", \"generated_summary\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55aac67-8b33-4f5f-94f4-fc39d02ea995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_df['generated_summary'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c6cb3-82f7-4d6b-841e-ec664e3a5d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "output_file_path = \"generated_summaries.csv\"  # Define the desired output file path\n",
    "processed_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Summaries saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c3b40-8504-4bb9-a052-5343239575ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure your processed DataFrame has the reduced text column\n",
    "if \"reduced_text\" not in processed_df.columns or \"generated_summary\" not in processed_df.columns:\n",
    "    raise ValueError(\"The 'reduced_text' or 'generated_summary' column is missing from the DataFrame.\")\n",
    "\n",
    "# Initialize variables for latency calculation\n",
    "latencies = []\n",
    "\n",
    "print(\"Measuring latency for CPTF+LLM...\")\n",
    "for reduced_text in tqdm(processed_df[\"reduced_text\"], desc=\"Evaluating Latency\"):\n",
    "    try:\n",
    "        # Measure latency per input\n",
    "        latency_start = time.time()\n",
    "        prompt = construct_few_shot_prompt(reduced_text)  # Use the reduced text for the prompt\n",
    "        generated_summary = summarizer(prompt, **generation_params)[0][\"generated_text\"]\n",
    "        latency_end = time.time()\n",
    "\n",
    "        # Append latency for the input\n",
    "        latencies.append(latency_end - latency_start)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing input: {reduced_text[:50]}... Error: {e}\")\n",
    "        latencies.append(float('inf'))  # Add infinity for failed cases\n",
    "\n",
    "# Calculate average latency\n",
    "valid_latencies = [lat for lat in latencies if lat != float('inf')]\n",
    "average_latency = sum(valid_latencies) / len(valid_latencies) if valid_latencies else float('inf')\n",
    "\n",
    "# Print the average latency\n",
    "print(f\"Average Latency for CPTF+LLM: {average_latency:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d968207-218c-4fee-9db4-060b1e9996d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1a20f-5bbc-4fe9-b710-ac09deba3cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38c24d-9c25-45ff-871b-d2e4179b9764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af5327-6ce9-4376-a03b-66444d384aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df6701-4ee6-47c7-bd0b-fa5f4b3a1f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceaaa64-c8f6-451a-b402-7ea6f7708087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the processed DataFrame\n",
    "processed_df = pd.read_csv(\"generated_summaries.csv\")  # Replace with actual file path\n",
    "\n",
    "# Ensure required columns exist\n",
    "if \"reduced_text\" not in processed_df.columns or \"generated_summary\" not in processed_df.columns:\n",
    "    raise ValueError(\"The 'reduced_text' or 'generated_summary' column is missing from the DataFrame.\")\n",
    "\n",
    "# Initialize tokenizer and sentence embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize metrics\n",
    "total_tokens = 0\n",
    "total_meaningful_tokens = 0\n",
    "total_irr = 0\n",
    "summaries_count = len(processed_df)\n",
    "\n",
    "# Start time for throughput measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate each row\n",
    "for _, row in tqdm(processed_df.iterrows(), desc=\"Evaluating Summaries\", total=summaries_count):\n",
    "    input_text = row[\"reduced_text\"]\n",
    "    generated_summary = row[\"generated_summary\"]\n",
    "\n",
    "    # Tokenization metrics\n",
    "    total_tokens += len(tokenizer.tokenize(generated_summary))\n",
    "    total_meaningful_tokens += len(tokenizer.tokenize(generated_summary.strip()))\n",
    "\n",
    "    # Information Retention Ratio (IRR)\n",
    "    input_embedding = sbert_model.encode([input_text])\n",
    "    summary_embedding = sbert_model.encode([generated_summary])\n",
    "    similarity = cosine_similarity(input_embedding, summary_embedding)[0][0]\n",
    "    total_irr += similarity\n",
    "\n",
    "# Calculate total time taken\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Final metrics\n",
    "average_te = total_meaningful_tokens / total_tokens if total_tokens > 0 else 0\n",
    "average_irr = total_irr / summaries_count if summaries_count > 0 else 0\n",
    "average_ttft = total_time / summaries_count if summaries_count > 0 else 0\n",
    "throughput = summaries_count / total_time if total_time > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average Token Efficiency (TE): {average_te:.4f}\")\n",
    "print(f\"Average Information Retention Ratio (IRR): {average_irr:.4f}\")\n",
    "print(f\"Total Time to First Token (TTFT): {average_ttft:.4f} seconds\")\n",
    "print(f\"Throughput: {throughput:.2f} summaries/second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993edf8-15f6-4050-894b-de2da7cf8f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 8  # Or any size suitable for your memory\n",
    "batched_latencies = []\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Evaluating Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"].iloc[i:i+batch_size].tolist()\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        prompts = [construct_few_shot_prompt(text) for text in batch]\n",
    "        summaries = summarizer(prompts, **generation_params)\n",
    "        end_time = time.time()\n",
    "        batched_latencies.append(end_time - start_time)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}: {e}\")\n",
    "        batched_latencies.append(float('inf'))\n",
    "average_batch_latency = sum(batched_latencies) / len(batched_latencies)\n",
    "print(f\"Average Batch Latency: {average_batch_latency:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3cc32-5350-4cf0-a8ff-268b666dde7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
