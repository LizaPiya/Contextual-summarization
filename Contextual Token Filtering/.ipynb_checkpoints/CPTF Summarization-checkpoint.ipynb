{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3470902-6191-49f2-990c-3bb9668a35b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f60c1d1-468b-41da-9781-8634049b40e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id                                              input  \\\n",
      "0  16002318-DS-17  <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...   \n",
      "1   15638884-DS-4  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Augment...   \n",
      "2  12435705-DS-14  <SEX> M <SERVICE> MEDICINE <ALLERGIES> ibuprof...   \n",
      "3   12413577-DS-4  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
      "4  17967161-DS-29  <SEX> M <SERVICE> SURGERY <ALLERGIES> lisinopr...   \n",
      "\n",
      "                                              target  input_tokens  \\\n",
      "0  This is a ___ yo F admitted to the hospital af...          1195   \n",
      "1  Mr. ___ is a ___ yo man with CAD with prior MI...          3496   \n",
      "2  Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...          5591   \n",
      "3  On ___, Ms. ___ was admitted to the gynecology...          1119   \n",
      "4  Mr. ___ underwent an angiogram on ___ which sh...          3307   \n",
      "\n",
      "   target_tokens                                       reduced_text  \\\n",
      "0             75  <|begin_of_text|><SEX> F <SERVICE> SURGERY <AL...   \n",
      "1           1143  <|begin_of_text|><SEX> M <SERVICE> MEDICINE <A...   \n",
      "2           1098  <|begin_of_text|><SEX> M <SERVICE> MEDICINE <A...   \n",
      "3            221  <|begin_of_text|><SEX> F <SERVICE> OBSTETRICS/...   \n",
      "4            439  <|begin_of_text|><SEX> M <SERVICE> SURGERY <AL...   \n",
      "\n",
      "                                   importance_scores  \n",
      "0  [0.010251283645629883, 0.010246990248560905, 0...  \n",
      "1  [0.0059814453125, 0.005979984533041716, 0.0059...  \n",
      "2  [0.0059814453125, 0.005979984533041716, 0.0059...  \n",
      "3  [0.010935068130493164, 0.0109328031539917, 0.0...  \n",
      "4  [0.0059814453125, 0.005979984533041716, 0.0059...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "processed_df = pd.read_csv(\"processed_reduced_texts.csv\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(processed_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f9e8b8-beeb-41e3-bf21-4628ca229d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94395844-7106-40fe-aad5-9e7dce09f3a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use token from environment variable (safer)\n",
    "login(os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d07e2c-98fe-405f-b8b5-96007ea65bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab1a9d1-21aa-44c5-bae3-858709206187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup and quantization configuration done.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configure quantization (8-bit)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "print(\"Environment setup and quantization configuration done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9ea36d-cc3d-4fec-904c-01e80e2d12a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer: 100%|██████████| 2/2 [00:03<00:00,  1.88s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "with tqdm(total=2, desc=\"Initializing Model and Tokenizer\", unit=\"step\") as pbar:\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16       \n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    pbar.update(1)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d39979-4953-4f2f-aeec-39ba2f022e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization pipeline initialized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(\"Summarization pipeline initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a40b3e-5a88-4e69-ab23-1f7ee7ed8123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  40%|████      | 10/25 [08:08<11:43, 46.89s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Batches: 100%|██████████| 25/25 [20:38<00:00, 49.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational Efficiency Metrics:\n",
      "Average Latency (Time per Summary): 12.38 ± 1.25 seconds\n",
      "Average Throughput: 139.10 ± 10.04 tokens/second\n",
      "\n",
      "For LaTeX Table:\n",
      "$139.10 \\pm 10.04$ & $12.38 \\pm 1.25$ \\\\\n",
      "\n",
      "Summaries saved to 'summarization_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Stronger Prompt to Prevent Hallucination\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "You are an expert at writing clinical summary. Summarize the input text in a very cohesive manner. Maintain storytelling manner. \n",
    "Example 1:\n",
    "Input: 45-year-old male with diabetes presented with chest pain and shortness of breath. ECG showed myocardial infarction. Patient treated with aspirin and admitted to cardiac unit.\n",
    "Summary: The patient was male with diabetes, chest pain.MI was confirmed by ECG, treated with aspirin, admitted.\n",
    "Example 2:\n",
    "Input: 72-year-old female with history of hypertension and stroke admitted with slurred speech, left-sided weakness. MRI confirmed acute ischemic stroke. Started on anticoagulation and monitored in ICU.\n",
    "Summary: female with hypertension, stroke, slurred speech, left-sided weakness, ischemic stroke confirmed by MRI, started on anticoagulation.\n",
    "Example 3 (Correcting Hallucination):\n",
    "Input: 60-year-old female with chronic kidney disease, admitted for worsening kidney function. Lab tests showed elevated creatinine. Dialysis started.\n",
    "Summary: [To be Generated]\n",
    "Do not add or infer anything from the few shot example or prompt. If unsure, say 'UNKNOWN':\n",
    "\"\"\"\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.7,  # Lowered to reduce hallucination\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"return_full_text\": False\n",
    "}\n",
    "\n",
    "batch_size = 4  # Consider system capacity\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "# Initialize latency and throughput lists\n",
    "latency_list = []\n",
    "throughput_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "    prompts = [f\"{FEW_SHOT_EXAMPLES}\\n{text}\" for text in batch]\n",
    "    \n",
    "    # Calculate tokens for the input batch\n",
    "    batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "    \n",
    "    # Measure batch start time\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    # Generate summaries\n",
    "    outputs = summarizer(prompts, **generation_params)\n",
    "    summaries = [output[0][\"generated_text\"].strip() for output in outputs]\n",
    "    \n",
    "    # Measure batch end time\n",
    "    batch_end_time = time.time()\n",
    "    \n",
    "    # Compute batch-level metrics\n",
    "    batch_latency = batch_end_time - batch_start_time  # Already in seconds\n",
    "    batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "    \n",
    "    # Store latency per summary (in seconds)\n",
    "    latency_per_summary = batch_latency / len(batch)\n",
    "    latency_list.append(latency_per_summary)\n",
    "    \n",
    "    # Store throughput for the batch (tokens per second)\n",
    "    batch_throughput = (batch_input_tokens + batch_output_tokens) / batch_latency\n",
    "    throughput_list.append(batch_throughput)\n",
    "    \n",
    "    # Store summaries\n",
    "    generated_summaries.extend(summaries)\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "std_latency = np.std(latency_list) if len(latency_list) > 1 else 0\n",
    "\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "std_throughput = np.std(throughput_list) if len(throughput_list) > 1 else 0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.2f} ± {std_latency:.2f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} ± {std_throughput:.2f} tokens/second\")\n",
    "\n",
    "# Print formatted for LaTeX table\n",
    "print(\"\\nFor LaTeX Table:\")\n",
    "print(f\"${average_throughput:.2f} \\\\pm {std_throughput:.2f}$ & ${average_latency:.2f} \\\\pm {std_latency:.2f}$ \\\\\\\\\")\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Save results\n",
    "processed_df.to_csv(\"summarization_results.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'summarization_results.csv'\")\n",
    "\n",
    "# Optional: Save metrics to a separate file\n",
    "with open(\"efficiency_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Parameters: Token=200, Temp=0.7\\n\")\n",
    "    f.write(f\"Throughput: {average_throughput:.2f} ± {std_throughput:.2f} tokens/second\\n\")\n",
    "    f.write(f\"Latency: {average_latency:.2f} ± {std_latency:.2f} seconds\\n\")\n",
    "    f.write(f\"\\nFor LaTeX Table:\\n\")\n",
    "    f.write(f\"${average_throughput:.2f} \\\\pm {std_throughput:.2f}$ & ${average_latency:.2f} \\\\pm {std_latency:.2f}$ \\\\\\\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ababbc-b499-485a-8349-cca296214228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b1415-fd1e-4ce4-82b6-77611fd851d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "316a79f8-ef74-4712-9ca0-409dc8f71d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  31%|███       | 4/13 [00:00<00:00, 34.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summaries for batch starting at index 0: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 8: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 16: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 24: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 32: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 40: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 48: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 56: name 'output' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 13/13 [00:00<00:00, 34.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summaries for batch starting at index 64: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 72: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 80: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 88: name 'output' is not defined\n",
      "Error generating summaries for batch starting at index 96: name 'output' is not defined\n",
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 0\n",
      "Total Output Tokens: 0\n",
      "Total Time Spent: 0.00 seconds\n",
      "Average Latency (Time per Summary): 0.0000 seconds\n",
      "Average Throughput: 0.00 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summaries saved to 'FLP_fixed_ttft_fewshot_summaries.csv'\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ✅ Your Few-Shot Prompt (Updated)\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "You are an EXPERT AT WRITING CLINICAL SUMMARY. Summarize the input text in a very cohesive manner.Do not include anything prompt the prompt or input in the summary.\n",
    "Example 1:\n",
    "Input: male with diabetes presented with chest pain and shortness of breath. ECG showed myocardial infarction. Patient treated with aspirin and admitted to cardiac unit.\n",
    "Summary: male with diabetes, chest pain, MI confirmed by ECG, treated with aspirin, admitted.\n",
    "\n",
    "Example 2:\n",
    "Input: female with history of hypertension and stroke admitted with slurred speech, left-sided weakness. MRI confirmed acute ischemic stroke. Started on anticoagulation and monitored in ICU.\n",
    "Summary:female with hypertension, stroke, slurred speech, left-sided weakness, ischemic stroke confirmed by MRI, started on anticoagulation.\n",
    "\n",
    "Example 3:\n",
    "Input: 60-year-old female with chronic kidney disease, admitted for worsening kidney function. Lab tests showed elevated creatinine. Dialysis started.\n",
    "Summary:[To be generated]\n",
    " \n",
    "Now summarize the following clinical note using only provided input. Do not add anything from the prompt. If unsure, say 'UNKNOWN':\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ✅ Generation parameters (Fixed)\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"use_cache\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # Ensures valid padding\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on memory\n",
    "\n",
    "# ✅ Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Batch Processing (WITH `tqdm` Only)\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # ✅ Keep Few-Shot Prompt\n",
    "        prompts = [f\"{FEW_SHOT_EXAMPLES}\\n{text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "\n",
    "            \n",
    "            # ✅ Store Summary\n",
    "            summaries.append(output[\"generated_text\"].strip())\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # ✅ Compute Metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total batch time\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # ✅ Update Global Metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # ✅ Store Summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries if failed\n",
    "\n",
    "# ✅ Add Summaries to DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# ✅ Compute Final Metrics\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "\n",
    "# ✅ Print Final Metrics (Only Once)\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "\n",
    "\n",
    "# ✅ Save Results\n",
    "processed_df.to_csv(\"FLP_fixed_ttft_fewshot_summaries.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_fixed_ttft_fewshot_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb3fa5-d632-403d-9ad9-8fbf03d2c8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b18c-e669-490a-afee-f07320c3fdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3ec88-8095-46a3-b980-c487f2f339cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd02aa-38b8-40cd-8ec0-27b4b31caaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c87140cf-818a-4873-bc42-c7b99b1c505a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  20%|██        | 5/25 [00:00<00:00, 49.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summaries for batch starting at index 0: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 4: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 8: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 12: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 16: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 20: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 24: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 28: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 32: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 36: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Batches:  44%|████▍     | 11/25 [00:00<00:00, 51.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summaries for batch starting at index 40: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  68%|██████▊   | 17/25 [00:00<00:00, 51.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summaries for batch starting at index 44: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 48: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 52: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 56: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 60: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 64: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 68: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 72: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 76: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 80: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 25/25 [00:00<00:00, 49.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summaries for batch starting at index 84: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 88: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 92: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "Error generating summaries for batch starting at index 96: The following `model_kwargs` are not used by the model: ['stream'] (note: typos in the generate arguments will also show up in this list)\n",
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 0\n",
      "Total Output Tokens: 0\n",
      "Total Time Spent: 0.00 seconds\n",
      "Average Latency (Time per Summary): 0.0000 seconds\n",
      "Average TTFT (Time to First Token): 0.0000 seconds\n",
      "Average Throughput: 0.00 tokens/second\n",
      "Token Efficiency (TE): 0.0000\n",
      "\n",
      "Summaries saved to 'FLP_fixed_ttft_fewshot_summaries.csv'\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ✅ YOUR ORIGINAL FEW-SHOT PROMPT (RESTORED)\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "You are an EXPERT AT WRITING CLINICAL SUMMARY. Summarize the input text in a very cohesive manner. Maintain storytelling style. \n",
    "Example 1:\n",
    "Input: 45-year-old male with diabetes presented with chest pain and shortness of breath. ECG showed myocardial infarction. Patient treated with aspirin and admitted to cardiac unit.\n",
    "Summary: 45-year-old male with diabetes, chest pain, MI confirmed by ECG, treated with aspirin, admitted.\n",
    "\n",
    "Example 2:\n",
    "Input: 72-year-old female with history of hypertension and stroke admitted with slurred speech, left-sided weakness. MRI confirmed acute ischemic stroke. Started on anticoagulation and monitored in ICU.\n",
    "Summary: 72-year-old female with hypertension, stroke, slurred speech, left-sided weakness, ischemic stroke confirmed by MRI, started on anticoagulation.\n",
    "\n",
    "Example 3 (Correcting Hallucination):\n",
    "Input: 60-year-old female with chronic kidney disease, admitted for worsening kidney function. Lab tests showed elevated creatinine. Dialysis started.\n",
    "Summary: 60-year-old female with kidney disease, worsening function, elevated creatinine, started on dialysis.\n",
    "\n",
    "Now summarize the following clinical note using only its information. Do not add or infer anything. If unsure, say 'UNKNOWN':\n",
    "\"\"\"\n",
    "\n",
    "# ✅ GENERATION PARAMETERS\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"stream\": True  # Enables real-time token streaming for TTFT\n",
    "}\n",
    "\n",
    "batch_size = 4  # Balanced batch size for 8-bit models\n",
    "\n",
    "# ✅ METRICS TRACKING\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# ✅ FUNCTION TO MEASURE TTFT USING STREAMING INFERENCE\n",
    "def measure_ttft(prompt):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Stream the response and capture first token\n",
    "    for response in summarizer(prompt, **generation_params):\n",
    "        first_token_time = time.time()\n",
    "        return first_token_time - start_time, response  # Return TTFT & first token\n",
    "\n",
    "# ✅ BATCH PROCESSING\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # ✅ KEEP YOUR FEW-SHOT PROMPT UNCHANGED\n",
    "        prompts = [f\"{FEW_SHOT_EXAMPLES}\\n{text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Measure batch start time\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "\n",
    "            # ✅ GET TTFT AND FIRST TOKEN VIA STREAMING\n",
    "            ttft, output = measure_ttft(prompt)\n",
    "            ttft_list.append(ttft)\n",
    "\n",
    "            # ✅ CONTINUE GENERATING REMAINING TOKENS\n",
    "            generated_text = output[0][\"generated_text\"].strip()\n",
    "            summaries.append(generated_text)\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # ✅ COMPUTE METRICS\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total batch time\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # ✅ UPDATE GLOBAL METRICS\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # ✅ STORE SUMMARIES\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# ✅ ADD SUMMARIES TO DATAFRAME\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# ✅ METRICS CALCULATION\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# ✅ PRINT METRICS\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# ✅ SAVE RESULTS\n",
    "processed_df.to_csv(\"FLP_fixed_ttft_fewshot_summaries.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_fixed_ttft_fewshot_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ba452c-7ed3-42a6-b425-366dc1cffd55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   8%|▊         | 2/25 [01:11<13:14, 34.53s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Batches: 100%|██████████| 25/25 [14:55<00:00, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 156585\n",
      "Total Output Tokens: 12955\n",
      "Total Time Spent: 895.29 seconds\n",
      "Average Latency (Time per Summary): 8.9529 seconds\n",
      "Average TTFT (Time to First Token): 8.9529 seconds\n",
      "Average Throughput: 193.03 tokens/second\n",
      "Token Efficiency (TE): 0.0827\n",
      "\n",
      "Summaries saved to 'FLP_strict_generated_summaries.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Stronger Prompt to Prevent Hallucination\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "You are an expert at writing clinical summary. Summarize the input text in a very cohesive manner. Maintain storytelling manner. \n",
    "Example 1:\n",
    "Input: 45-year-old male with diabetes presented with chest pain and shortness of breath. ECG showed myocardial infarction. Patient treated with aspirin and admitted to cardiac unit.\n",
    "Summary: 45-year-old male with diabetes, chest pain, MI confirmed by ECG, treated with aspirin, admitted.\n",
    "\n",
    "Example 2:\n",
    "Input: 72-year-old female with history of hypertension and stroke admitted with slurred speech, left-sided weakness. MRI confirmed acute ischemic stroke. Started on anticoagulation and monitored in ICU.\n",
    "Summary: 72-year-old female with hypertension, stroke, slurred speech, left-sided weakness, ischemic stroke confirmed by MRI, started on anticoagulation.\n",
    "\n",
    "Example 3 (Correcting Hallucination):\n",
    "Input: 60-year-old female with chronic kidney disease, admitted for worsening kidney function. Lab tests showed elevated creatinine. Dialysis started.\n",
    "INCORRECT Summary: 60-year-old female with kidney disease, received kidney transplant. \n",
    "CORRECT Summary: 60-year-old female with kidney disease, worsening function, elevated creatinine, started on dialysis.\n",
    "\n",
    "Do not add or infer anything from the few shot example. If unsure, say 'UNKNOWN':\n",
    "\"\"\"\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.6,  # Lowered temperature to reduce hallucination\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"repetition_penalty\": 1.2,  # Increased to avoid prompt repetition\n",
    "    \"return_full_text\": False\n",
    "}\n",
    "\n",
    "batch_size = 4  # Balanced batch size for 8-bit models\n",
    "\n",
    "# Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct Few-Shot Prompt with input text\n",
    "        prompts = [f\"{FEW_SHOT_EXAMPLES}\\n{text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Measure batch start time\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "\n",
    "            # Generate text while measuring TTFT correctly\n",
    "            output = summarizer(prompt, **generation_params)\n",
    "            first_token_time = time.time()  # Capture time when first token is generated\n",
    "\n",
    "            # Store TTFT (time to first token)\n",
    "            ttft_list.append(first_token_time - single_start_time)\n",
    "\n",
    "            # Extract generated text correctly\n",
    "            generated_text = output[0][\"generated_text\"].strip()\n",
    "            summaries.append(generated_text)\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # Compute batch-level metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total batch time\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # Update global metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # Store summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "processed_df.to_csv(\"FLP_strict_generated_summaries.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_strict_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "237ec8b6-e5c5-430d-94f2-61314f78be0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summaries saved to 'FLP_generated_summaries_with_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "processed_df.to_csv(\"FLP_generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8805fd0-e5bf-4486-aef9-acd78bc2b06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863e87aa-b74b-46ff-b2cc-d09b5d3fc139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 13/13 [10:30<00:00, 48.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 129585\n",
      "Total Output Tokens: 139251\n",
      "Total Time Spent: 629.88 seconds\n",
      "Average Latency (Time per Summary): 6.3091 seconds\n",
      "Average TTFT (Time to First Token): 6.2988 seconds\n",
      "Average Throughput: 428.38 tokens/second\n",
      "Token Efficiency (TE): 1.0746\n",
      "\n",
      "Summaries saved to 'generated_summaries_with_metrics.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct prompts for the batch\n",
    "        prompts = [f\"You are a medical expert. {text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Generate summaries and measure TTFT and latency\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "            output = summarizer(prompt, **generation_params)\n",
    "            single_end_time = time.time()\n",
    "            ttft_list.append(single_end_time - single_start_time)  # Time to first token\n",
    "            summaries.append(output[0][\"generated_text\"])  # Extract generated text\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # Calculate batch metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total time for the batch\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # Update global metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # Store summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "processed_df.to_csv(\"generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d46cfe-8560-479c-9598-31b099b4511c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
