{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b70b59-330f-4c29-bce2-ffade9f15a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q nltk bert-score\n",
    "!pip install -q rouge-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416553b1-6d47-4441-8303-68d4c28941e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved CSV file\n",
    "Summaries_mimic_iv_bhc_100 = pd.read_csv(\"generated_summaries_100.csv\")\n",
    "\n",
    "# Verify the data\n",
    "print(Summaries_mimic_iv_bhc_100.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fa5a96-9133-40ff-8693-5c9b51b22a12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient underwent cystoscopy and transurethral resection of the prostate due to benign prostatic hypertrophy. Postoperative diagnosis confirmed benign prostatic hypertrophy and chronic kidney disease. The patient is being followed up with a nurse practitioner who will monitor their condition and provide discharge instructions.\n"
     ]
    }
   ],
   "source": [
    "print(Summaries_mimic_iv_bhc_100['generated_summary'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80c9d1-16eb-4c0b-87e3-621fadfe9fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Summaries_mimic_iv_bhc_100.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9a7f6-a6d5-4bb8-b0b3-530a0a39263e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score\n",
    "from rouge_metric import PyRouge\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return ' '.join(text.strip().lower().split())  # Lowercase, strip spaces, normalize.\n",
    "\n",
    "def compute_bleu_scores(reference, candidate):\n",
    "    \"\"\"Compute BLEU-1 and BLEU-2 scores.\"\"\"\n",
    "    try:\n",
    "        smoothing_function = SmoothingFunction().method1\n",
    "        # Compute BLEU-1\n",
    "        bleu1 = sentence_bleu(\n",
    "            [reference.split()],\n",
    "            candidate.split(),\n",
    "            weights=(1.0, 0, 0, 0),  # Only unigrams\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "        # Compute BLEU-2\n",
    "        bleu2 = sentence_bleu(\n",
    "            [reference.split()],\n",
    "            candidate.split(),\n",
    "            weights=(0.5, 0.5, 0, 0),  # Unigrams and bigrams only\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "        return bleu1 * 100, bleu2 * 100  # Convert to percentages\n",
    "    except Exception as e:\n",
    "        print(f\"BLEU Error: {e}\")\n",
    "        print(f\"Reference: '{reference[:50]}...'\")\n",
    "        print(f\"Candidate: '{candidate[:50]}...'\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "def compute_rouge_l(reference, candidate):\n",
    "    \"\"\"Compute ROUGE-L score.\"\"\"\n",
    "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True, rouge_w=False,\n",
    "                    rouge_w_weight=1.2, rouge_s=False, rouge_su=False, skip_gap=4)\n",
    "    try:\n",
    "        scores = rouge.evaluate([candidate], [[reference]])\n",
    "        return scores['rouge-l']['f'] * 100  # Convert to percentage\n",
    "    except Exception as e:\n",
    "        print(f\"ROUGE-L Error: {e}\")\n",
    "        print(f\"Reference: '{reference[:50]}...'\")\n",
    "        print(f\"Candidate: '{candidate[:50]}...'\")\n",
    "        return 0.0\n",
    "\n",
    "def compute_bert_score_batched(references, candidates, batch_size=32):\n",
    "    \"\"\"Compute BERTScore in batches.\"\"\"\n",
    "    all_P, all_R, all_F1 = [], [], []\n",
    "    for i in range(0, len(references), batch_size):\n",
    "        batch_refs = references[i:i + batch_size]\n",
    "        batch_cands = candidates[i:i + batch_size]\n",
    "        try:\n",
    "            P, R, F1 = score(batch_cands, batch_refs, lang=\"en\", verbose=False)\n",
    "            all_P.extend([p * 100 for p in P.tolist()])  # Convert to percentages\n",
    "            all_R.extend([r * 100 for r in R.tolist()])  # Convert to percentages\n",
    "            all_F1.extend([f * 100 for f in F1.tolist()])  # Convert to percentages\n",
    "        except Exception as e:\n",
    "            print(f\"BERTScore Error in batch {i}: {e}\")\n",
    "            batch_len = len(batch_refs)\n",
    "            all_P.extend([0.0] * batch_len)\n",
    "            all_R.extend([0.0] * batch_len)\n",
    "            all_F1.extend([0.0] * batch_len)\n",
    "    return all_P, all_R, all_F1\n",
    "\n",
    "def evaluate_summaries(df):\n",
    "    bleu1_scores, bleu2_scores, rouge_l_scores = [], [], []\n",
    "    print(\"Computing BLEU and ROUGE-L scores...\")\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Processing Rows\", unit=\"row\") as pbar:\n",
    "        for _, row in df.iterrows():\n",
    "            reference = clean_text(row['target'])\n",
    "            candidate = clean_text(row['generated_summary'])\n",
    "            \n",
    "            if not reference or not candidate:\n",
    "                print(f\"Empty text - Reference: '{reference}', Candidate: '{candidate}'\")\n",
    "                bleu1_scores.append(0.0)\n",
    "                bleu2_scores.append(0.0)\n",
    "                rouge_l_scores.append(0.0)\n",
    "            else:\n",
    "                bleu1, bleu2 = compute_bleu_scores(reference, candidate)\n",
    "                bleu1_scores.append(bleu1)\n",
    "                bleu2_scores.append(bleu2)\n",
    "                rouge_l_scores.append(compute_rouge_l(reference, candidate))\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(\"\\nComputing BERTScore...\")\n",
    "    references = [clean_text(text) for text in df['target'].tolist()]\n",
    "    candidates = [clean_text(text) for text in df['generated_summary'].tolist()]\n",
    "    bert_p, bert_r, bert_f1 = compute_bert_score_batched(references, candidates)\n",
    "    \n",
    "    # Add all scores to DataFrame\n",
    "    df['bleu1'] = bleu1_scores\n",
    "    df['bleu2'] = bleu2_scores\n",
    "    df['rouge_l'] = rouge_l_scores\n",
    "    df['bert_p'] = bert_p\n",
    "    df['bert_r'] = bert_r\n",
    "    df['bert_f1'] = bert_f1\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"\\nEvaluation Metrics (in percentages):\")\n",
    "    print(\"Average BLEU-1:\", df['bleu1'].mean(), \"%\")\n",
    "    print(\"Average BLEU-2:\", df['bleu2'].mean(), \"%\")\n",
    "    print(\"Average ROUGE-L:\", df['rouge_l'].mean(), \"%\")\n",
    "    print(\"Average BERT P:\", df['bert_p'].mean(), \"%\")\n",
    "    print(\"Average BERT R:\", df['bert_r'].mean(), \"%\")\n",
    "    print(\"Average BERT F1:\", df['bert_f1'].mean(), \"%\")\n",
    "    \n",
    "    # Print standard deviations\n",
    "    print(\"\\nStandard Deviations (in percentages):\")\n",
    "    print(\"BLEU-1 Std:\", df['bleu1'].std(), \"%\")\n",
    "    print(\"BLEU-2 Std:\", df['bleu2'].std(), \"%\")\n",
    "    print(\"ROUGE-L Std:\", df['rouge_l'].std(), \"%\")\n",
    "    print(\"BERT F1 Std:\", df['bert_f1'].std(), \"%\")\n",
    "    # Assuming you have columns for BERT Precision and BERT Recall in your DataFrame\n",
    "    print(\"BERT Precision Std:\", df['bert_p'].std(), \"%\")\n",
    "    print(\"BERT Recall Std:\", df['bert_r'].std(), \"%\")\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    Summaries_mimic_iv_bhc_100 = evaluate_summaries(Summaries_mimic_iv_bhc_100)\n",
    "    Summaries_mimic_iv_bhc_100.to_csv(\"evaluation_results.csv\", index=False)\n",
    "    print(\"\\nResults saved to 'evaluation_results.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in evaluation process: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01069a88-ceb0-4dfc-b2b0-910feb3bd23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
