{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c4ef46-c959-4728-aad5-ee0af63977fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m163.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface_hub, tokenizers, transformers\n",
      "Successfully installed huggingface_hub-0.27.1 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.47.1\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers huggingface_hub\n",
    "!pip install --upgrade accelerate\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbe9137-f1a9-4fe8-ab95-bea2b8c9339a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet langchain neo4j langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b086aa34-6607-4eb9-aab3-f567b8a03149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (3.11.11)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (0.3.14)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (0.3.29)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.24.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Downloading marshmallow-3.24.2-py3-none-any.whl (49 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.14 marshmallow-3.24.2 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3701fc5a-cc3f-4bd4-835e-7ef3c64e18c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>target_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16002318-DS-17</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; Iodine /...</td>\n",
       "      <td>This is a ___ yo F admitted to the hospital af...</td>\n",
       "      <td>1195</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15638884-DS-4</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Augment...</td>\n",
       "      <td>Mr. ___ is a ___ yo man with CAD with prior MI...</td>\n",
       "      <td>3496</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12435705-DS-14</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; ibuprof...</td>\n",
       "      <td>Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...</td>\n",
       "      <td>5591</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12413577-DS-4</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; OBSTETRICS/GYNECOLOGY &lt;ALLER...</td>\n",
       "      <td>On ___, Ms. ___ was admitted to the gynecology...</td>\n",
       "      <td>1119</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17967161-DS-29</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; lisinopr...</td>\n",
       "      <td>Mr. ___ underwent an angiogram on ___ which sh...</td>\n",
       "      <td>3307</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16956007-DS-20</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; Codeine ...</td>\n",
       "      <td>Mr. ___ is a ___ who underwent an exploratory ...</td>\n",
       "      <td>4168</td>\n",
       "      <td>1209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16919911-DS-15</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Penicil...</td>\n",
       "      <td>This is a ___ year old female with a recent di...</td>\n",
       "      <td>2059</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15682570-DS-25</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; No Know...</td>\n",
       "      <td>___ w/ h/o CAD ___ CABG LIMA to LAD, SVG to D1...</td>\n",
       "      <td>2215</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12135369-DS-24</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Compazi...</td>\n",
       "      <td>Ms ___ is a ___ year old woman with a history ...</td>\n",
       "      <td>2132</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11906321-DS-20</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; NEUROSURGERY &lt;ALLERGIES&gt; Pat...</td>\n",
       "      <td>The patient was admitted to the neurosurgery s...</td>\n",
       "      <td>2347</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17384661-DS-20</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; UROLOGY &lt;ALLERGIES&gt; No Known...</td>\n",
       "      <td>Mr. ___ was admitted to Dr. ___ Urology servic...</td>\n",
       "      <td>1452</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11162709-DS-18</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Sulfa (...</td>\n",
       "      <td>___ yoM with PMH notable for hx of left ICA an...</td>\n",
       "      <td>2469</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19473355-DS-13</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; No Known...</td>\n",
       "      <td>Mr. ___ is a ___ year old male with AAA who wa...</td>\n",
       "      <td>1593</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14711846-DS-15</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Gold Sa...</td>\n",
       "      <td>Ms. ___ was admitted for diagnosis and treatme...</td>\n",
       "      <td>3140</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18094090-DS-35</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Penicil...</td>\n",
       "      <td>Patient is a ___ year old woman with history o...</td>\n",
       "      <td>2346</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16047293-DS-34</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Penicil...</td>\n",
       "      <td>___ year-old male with PMH of diastolic heart ...</td>\n",
       "      <td>1747</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15690160-DS-11</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; NEUROLOGY &lt;ALLERGIES&gt; No Dru...</td>\n",
       "      <td>Her neuro exam has been asymptomatic since adm...</td>\n",
       "      <td>2438</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18477272-DS-18</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; OBSTETRICS/GYNECOLOGY &lt;ALLER...</td>\n",
       "      <td>On ___, Ms. ___ was admitted to the gynecology...</td>\n",
       "      <td>1192</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10476129-DS-14</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; No Know...</td>\n",
       "      <td>BRIEF COURSE: ___ with limited PMH presented w...</td>\n",
       "      <td>2998</td>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19868904-DS-20</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; NEUROLOGY &lt;ALLERGIES&gt; No Kno...</td>\n",
       "      <td>Ms. ___ was admitted to the neurology service ...</td>\n",
       "      <td>2072</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           note_id                                              input  \\\n",
       "0   16002318-DS-17  <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...   \n",
       "1    15638884-DS-4  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Augment...   \n",
       "2   12435705-DS-14  <SEX> M <SERVICE> MEDICINE <ALLERGIES> ibuprof...   \n",
       "3    12413577-DS-4  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
       "4   17967161-DS-29  <SEX> M <SERVICE> SURGERY <ALLERGIES> lisinopr...   \n",
       "5   16956007-DS-20  <SEX> M <SERVICE> SURGERY <ALLERGIES> Codeine ...   \n",
       "6   16919911-DS-15  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Penicil...   \n",
       "7   15682570-DS-25  <SEX> M <SERVICE> MEDICINE <ALLERGIES> No Know...   \n",
       "8   12135369-DS-24  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Compazi...   \n",
       "9   11906321-DS-20  <SEX> M <SERVICE> NEUROSURGERY <ALLERGIES> Pat...   \n",
       "10  17384661-DS-20  <SEX> M <SERVICE> UROLOGY <ALLERGIES> No Known...   \n",
       "11  11162709-DS-18  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Sulfa (...   \n",
       "12  19473355-DS-13  <SEX> M <SERVICE> SURGERY <ALLERGIES> No Known...   \n",
       "13  14711846-DS-15  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Gold Sa...   \n",
       "14  18094090-DS-35  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Penicil...   \n",
       "15  16047293-DS-34  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Penicil...   \n",
       "16  15690160-DS-11  <SEX> F <SERVICE> NEUROLOGY <ALLERGIES> No Dru...   \n",
       "17  18477272-DS-18  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
       "18  10476129-DS-14  <SEX> F <SERVICE> MEDICINE <ALLERGIES> No Know...   \n",
       "19  19868904-DS-20  <SEX> F <SERVICE> NEUROLOGY <ALLERGIES> No Kno...   \n",
       "\n",
       "                                               target  input_tokens  \\\n",
       "0   This is a ___ yo F admitted to the hospital af...          1195   \n",
       "1   Mr. ___ is a ___ yo man with CAD with prior MI...          3496   \n",
       "2   Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...          5591   \n",
       "3   On ___, Ms. ___ was admitted to the gynecology...          1119   \n",
       "4   Mr. ___ underwent an angiogram on ___ which sh...          3307   \n",
       "5   Mr. ___ is a ___ who underwent an exploratory ...          4168   \n",
       "6   This is a ___ year old female with a recent di...          2059   \n",
       "7   ___ w/ h/o CAD ___ CABG LIMA to LAD, SVG to D1...          2215   \n",
       "8   Ms ___ is a ___ year old woman with a history ...          2132   \n",
       "9   The patient was admitted to the neurosurgery s...          2347   \n",
       "10  Mr. ___ was admitted to Dr. ___ Urology servic...          1452   \n",
       "11  ___ yoM with PMH notable for hx of left ICA an...          2469   \n",
       "12  Mr. ___ is a ___ year old male with AAA who wa...          1593   \n",
       "13  Ms. ___ was admitted for diagnosis and treatme...          3140   \n",
       "14  Patient is a ___ year old woman with history o...          2346   \n",
       "15  ___ year-old male with PMH of diastolic heart ...          1747   \n",
       "16  Her neuro exam has been asymptomatic since adm...          2438   \n",
       "17  On ___, Ms. ___ was admitted to the gynecology...          1192   \n",
       "18  BRIEF COURSE: ___ with limited PMH presented w...          2998   \n",
       "19  Ms. ___ was admitted to the neurology service ...          2072   \n",
       "\n",
       "    target_tokens  \n",
       "0              75  \n",
       "1            1143  \n",
       "2            1098  \n",
       "3             221  \n",
       "4             439  \n",
       "5            1209  \n",
       "6             208  \n",
       "7             451  \n",
       "8             416  \n",
       "9             316  \n",
       "10            171  \n",
       "11            420  \n",
       "12            259  \n",
       "13            415  \n",
       "14            528  \n",
       "15            144  \n",
       "16             18  \n",
       "17            154  \n",
       "18           1119  \n",
       "19            221  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'mimicivliza'  # Replace with your bucket name\n",
    "mimic_iv_bhc = f's3://{bucket_name}/sample_data_100.csv'\n",
    "\n",
    "# Load the files\n",
    "mimic_iv_bhc_100 = pd.read_csv(mimic_iv_bhc)\n",
    "\n",
    "# Display the data\n",
    "mimic_iv_bhc_100.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9eeea42-620c-4cc8-a492-8afd894b5c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine / Thallium-201 / Blue Dye / Iodine-Iodine Containing <ATTENDING> ___. <CHIEF COMPLAINT> Morbid obesity, BMI of 51 <MAJOR SURGICAL OR INVASIVE PROCEDURE> lap gastric bypass <HISTORY OF PRESENT ILLNESS> The patient is a ___ woman with history of obesity, multiple medical problems with a history of 7 pound weight loss and regain. Comorbid conditions include sleep apnea, hypothyroidism, back pain, iron deficiency anemia and headaches. The patient has significant allergies particularly to the blue dye and iodine. The patient was evaluated at ___ ___ ___ Program deemed a good candidate for surgical weight loss. She understands the risks, benefits and alternatives of weight loss surgery. She agrees to diet, exercise, support group and lifelong medical follow-up particularly for B12, calcium and folate levels. <PAST MEDICAL HISTORY> Past medical history includes sleep apnea, hypothyroidism, back pain, urticaria for which she is on chronic steroids, iron deficiency anemia, and headaches <SOCIAL HISTORY> ___ <FAMILY HISTORY> Family history of obesity with mother. <PHYSICAL EXAM> PHYSICAL EXAM ON ADMISSION: GEN: no acute distress BMI is 51. Alert and oriented. HEENT: Neck is supple. PULM: Breathing comfortably. Lungs clear to auscultation. CV: RRR ABD: Abdomen is soft and nontender with a lower abdominal scar. Upper and lower extremities, good range of motion, good strength. Gait and station normal. <PERTINENT RESULTS> ___ 12: 07PM HCT-36.3 <MEDICATIONS ON ADMISSION> Levothyroxine 100 mcg daily for hypothyroid; Zyrtec 10 mg twice a day as needed, Benadryl 25 mg at bedtime for seasonal allergies, chronic hives; Albuterol sulfate two puffs q 6 hours if needed for wheezing (has not used so far); Fioricet 50 mg-325 mg-40 mg once a day as needed for migraine headaches; Clobetasol 0.05% ointment to affected areas twice a day for rash; its Tylenol ___ mg-650 mg every 4 to 6 hours as needed for pain/headache; Ferrous sulfate 325 mg for iron deficiency, multivitamin once a day and vitamin D 1000 units once a day for nutritional supplementation <DISCHARGE MEDICATIONS> 1. Roxicet ___ mg/5 mL Solution Sig: Ten (10) mls PO every four (4) hours. Disp: *500 ccs* Refills: *0* 2. Colace 50 mg/5 mL Liquid Sig: ___ mls PO twice a day. Disp: *500 mls* Refills: *0* 3. Zantac 15 mg/mL Syrup Sig: Five (5) mls PO twice a day. Disp: *500 mls* Refills: *2* 4. Levothyroxine 100 mcg Tablet Sig: One (1) Tablet PO DAILY (Daily). 5. Albuterol Sulfate 90 mcg/Actuation HFA Aerosol Inhaler Sig: ___ Puffs Inhalation Q6H (every 6 hours) as needed for wheeze. 6. Diphenhydramine HCl 25 mg Capsule Sig: One (1) Capsule PO Q4H (every 4 hours) as needed for itch/hives. <DISCHARGE DISPOSITION> Home <DISCHARGE DIAGNOSIS> Morbid Obesity <DISCHARGE CONDITION> Afebrile with vital signs stable <FOLLOWUP INSTRUCTIONS> ___ <DISCHARGE INSTRUCTIONS> You are being discharged on medications to treat the pain from your operation. These medications will make you drowsy and impair your ability to drive a motor vehicle or operate machinery safely. You MUST refrain from such activities while taking these medications. Please call your doctor or return to the emergency room if you have any of the following: * You experience new chest pain, pressure, squeezing or tightness. * New or worsening cough or wheezing. * If you are vomiting and cannot keep in fluids or your medications. * You are getting dehydrated due to continued vomiting, diarrhea or other reasons. Signs of dehydration include dry mouth, rapid heartbeat or feeling dizzy or faint when standing. * You see blood or dark/black material when you vomit or have a bowel movement. * You have shaking chills, or a fever greater than 101.5 (F) degrees or 38(C) degrees. * Any serious change in your symptoms, or any new symptoms that concern you. * Please resume all regular home medications and take any new meds as ordered. Activity: No heavy lifting of items ___ pounds for 6 weeks. You may resume moderate exercise at your discretion, no abdominal exercises. Wound Care: You may shower 48 hours after surgery, no tub baths or swimming. If there is clear drainage from your incisions, cover with clean, dry gauze. Your steri-strips will fall off on their own. Please remove any remaining strips ___ days after surgery. Please call the doctor if you have increased pain, swelling, redness, or drainage from the incision sites. Please stay on Bariatric stage 3 diet until your follow appointment. Please do not drink from a straw or chew gum. Please take Flintstones chewable tablets daily. \n"
     ]
    }
   ],
   "source": [
    "print(mimic_iv_bhc_100['input'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be15f8c2-c86a-4bd7-943b-1801eb3e7b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note_id                                       16002318-DS-17\n",
      "input      <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extract 'note_id' and 'input' columns and save to a new dataframe\n",
    "kg_dataframe = mimic_iv_bhc_100[['note_id', 'input']]\n",
    "\n",
    "# Display the first few rows of the new dataframe to verify\n",
    "kg_dataframe.head(2)\n",
    "print(kg_dataframe[['note_id', 'input']].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52c0b1-a68a-45ce-89b5-5df591ffa94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a5ccb6a-6f87-468d-8468-cac411494aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07267b76-5751-4836-b5f1-3cf40f4682cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Graphdb configuration\n",
    "NEO4J_URI=\"neo4j+s://8a886660.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"9FYDkCTM2Vq4qxWFFwik0uYP6BJ-fReP9XOYj-oDqZ4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3164b7-1da3-46c8-8d61-3bf6f8924f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NEO4J_URI\"]=NEO4J_URI\n",
    "os.environ[\"NEO4J_USERNAME\"]=NEO4J_USERNAME\n",
    "os.environ[\"NEO4J_PASSWORD\"]=NEO4J_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af39e42-b92b-479d-83c6-27a9d7d19896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30333/658885258.py:2: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  neo4j_graph=Neo4jGraph(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "neo4j_graph=Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "672f8be5-78ae-466d-b4e2-9948d515f663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x7f9793e16590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neo4j_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ee3c01-6b1b-4718-ac39-ee5d34c9bbd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting json-repair\n",
      "  Downloading json_repair-0.35.0-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading json_repair-0.35.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: json-repair\n",
      "Successfully installed json-repair-0.35.0\n"
     ]
    }
   ],
   "source": [
    "!pip install json-repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e35b20-6a01-4732-b697-8086e7484da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_experimental\n",
      "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain_experimental) (0.3.14)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain_experimental) (0.3.29)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.14)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.24.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.14->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.5)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
      "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
      "Installing collected packages: langchain_experimental\n",
      "Successfully installed langchain_experimental-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed085862-a8a0-4bb4-997a-02247741423a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac7317f-77b4-4f40-bc12-25e08e2e221e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1: Set Up Logging and Define Configuration\n",
    "##Run this block to initialize logging and define configuration parameters for the graph builder.\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"Configuration for the knowledge graph building process\"\"\"\n",
    "    batch_size: int = 5\n",
    "    max_retries: int = 3\n",
    "    confidence_threshold: float = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1835e566-2715-45bd-ad54-b6c22bf4fb05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527319e966d046988bd1513ebb009d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a18acbea6742d4b4e8f171a079ce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94abf4e2318465196d584c0ed1b1179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412e02238e74409c99acfaccdf867867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce7505e422e4e8d8ab83eae348448c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a47fb3f0d7e490bb05fb91687a2a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_30333/226699493.py:30: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm_pipeline = HuggingFacePipeline(pipeline=text_pipe)\n",
      "INFO:__main__:LLM pipeline initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Initialize the LLM Pipeline\n",
    "#This step initializes the model, tokenizer, and the pipeline used for LLM-based text generation.\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "# Initialize model and tokenizer with Llama model\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"  # Automatically selects appropriate precision\n",
    ")\n",
    "\n",
    "# Initialize pipeline\n",
    "text_pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm_pipeline = HuggingFacePipeline(pipeline=text_pipe)\n",
    "logger.info(\"LLM pipeline initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0abc1c3-cecc-4589-b4e7-5547b96c6a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Neo4j connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Initialize Neo4j Connection\n",
    "#Connect to the Neo4j database and verify the connection\n",
    "\n",
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "# Replace with your Neo4j credentials\n",
    "## Graphdb configuration\n",
    "NEO4J_URI=\"neo4j+s://8a886660.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"9FYDkCTM2Vq4qxWFFwik0uYP6BJ-fReP9XOYj-oDqZ4\"\n",
    "\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# Verify the connection\n",
    "logger.info(\"Neo4j connection established successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ec2511f-a165-4190-ba4a-eb1ca8303a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "INFO:__main__:Pipeline initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set() set()\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Initialize the Graph Builder\n",
    "#Instantiate the ClinicalKnowledgeGraphBuilder class and initialize the pipeline.\n",
    "\n",
    "class ClinicalKnowledgeGraphBuilder:\n",
    "    def __init__(self, model, tokenizer, graph, config=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.graph = graph\n",
    "        self.config = config or GraphConfig()\n",
    "        \n",
    "        # Initialize LLM pipeline\n",
    "        self._init_pipeline()\n",
    "        \n",
    "        # Initialize sets for dynamic nodes and relationships\n",
    "        self.allowed_nodes = set()\n",
    "        self.allowed_relationships = set()\n",
    "\n",
    "    def _init_pipeline(self):\n",
    "        \"\"\"Initialize the LLM pipeline with optimized settings\"\"\"\n",
    "        try:\n",
    "            text_pipe = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            self.llm = HuggingFacePipeline(pipeline=text_pipe)\n",
    "            logger.info(\"Pipeline initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline initialization failed: {e}\")\n",
    "            raise\n",
    "\n",
    "# Instantiate the graph builder\n",
    "graph_builder = ClinicalKnowledgeGraphBuilder(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    graph=neo4j_graph\n",
    ")\n",
    "\n",
    "print(graph_builder.allowed_nodes, graph_builder.allowed_relationships)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a0b9e0-9d21-476b-b7c3-b9d17d06df9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 5: Process Sample Data with process_data\n",
    "#Add the process_data function to the ClinicalKnowledgeGraphBuilder class and test it with a small sample dataframe.\n",
    "\n",
    "def process_data(self, dataframe: pd.DataFrame, text_column: str = 'input'):\n",
    "    \"\"\"Process the dataframe to extract entities and relationships\"\"\"\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        logger.info(f\"Processing row {idx}\")\n",
    "        text_input = row[text_column]\n",
    "\n",
    "        if not isinstance(text_input, str) or not text_input.strip():\n",
    "            logger.warning(f\"Skipping empty or invalid input in row {idx}\")\n",
    "            continue\n",
    "\n",
    "        # Convert text into a Document object\n",
    "        doc = Document(page_content=text_input)\n",
    "\n",
    "        try:\n",
    "            # Extract graph data using the transformer\n",
    "            graph_data = self.graph_transformer.process_response(doc)\n",
    "            logger.debug(f\"Graph data structure: {graph_data}\")  # Debug log to see structure\n",
    "            \n",
    "            # Extract nodes\n",
    "            if hasattr(graph_data, 'nodes'):\n",
    "                nodes = graph_data.nodes\n",
    "                for node in nodes:\n",
    "                    self.allowed_nodes.add(node.type)\n",
    "            else:\n",
    "                logger.warning(f\"No nodes found in row {idx}\")\n",
    "                nodes = []\n",
    "\n",
    "            # Extract relationships\n",
    "            relationships = []\n",
    "            if hasattr(graph_data, 'relationships'):\n",
    "                relationships = graph_data.relationships\n",
    "                for rel in relationships:\n",
    "                    self.allowed_relationships.add(rel.type)\n",
    "            elif hasattr(graph_data, 'edges'):\n",
    "                relationships = graph_data.edges\n",
    "                for edge in relationships:\n",
    "                    self.allowed_relationships.add(edge.type)\n",
    "            else:\n",
    "                logger.warning(f\"No relationships/edges found in row {idx}\")\n",
    "\n",
    "            # Populate the graph with extracted data\n",
    "            self._populate_graph(nodes, relationships)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process row {idx}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5506a24a-637b-4bd7-91a9-67892cf60ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing row 0\n",
      "ERROR:__main__:Failed to process row 0: 'ClinicalKnowledgeGraphBuilder' object has no attribute 'graph_transformer'\n",
      "INFO:__main__:Processing row 1\n",
      "ERROR:__main__:Failed to process row 1: 'ClinicalKnowledgeGraphBuilder' object has no attribute 'graph_transformer'\n",
      "INFO:__main__:Processing row 2\n",
      "ERROR:__main__:Failed to process row 2: 'ClinicalKnowledgeGraphBuilder' object has no attribute 'graph_transformer'\n"
     ]
    }
   ],
   "source": [
    "#Step 6  Test process_data Function.\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create a sample dataframe\n",
    "sample_data = pd.DataFrame({\n",
    "    \"input\": [\n",
    "        \"The patient is diagnosed with diabetes and prescribed metformin.\",\n",
    "        \"The patient reports symptoms of fever and headache.\",\n",
    "        \"The treatment included paracetamol and antibiotics for infection.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Add the function to the ClinicalKnowledgeGraphBuilder class\n",
    "ClinicalKnowledgeGraphBuilder.process_data = process_data\n",
    "\n",
    "# Process the sample data\n",
    "graph_builder.process_data(sample_data, text_column=\"input\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21156cd9-82b7-44ad-a475-0b8f74fd61c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d05144-9676-4289-abc9-141a8507b7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6ae5a-2415-4954-8613-c638fc84118a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db6d1f-cd53-435b-af9b-f22b0593d43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ae2026-71a5-41f7-b4e7-9bb9ddde1a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_experimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMGraphTransformer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Neo4jGraph\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_experimental'"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"Configuration for the knowledge graph building process\"\"\"\n",
    "    batch_size: int = 5\n",
    "    max_retries: int = 3\n",
    "    confidence_threshold: float = 0.3\n",
    "\n",
    "class ClinicalKnowledgeGraphBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        graph: Neo4jGraph,\n",
    "        config: Optional[GraphConfig] = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.graph = graph\n",
    "        self.config = config or GraphConfig()\n",
    "        \n",
    "        # Initialize LLM pipeline\n",
    "        self._init_pipeline()\n",
    "        \n",
    "        # Initialize sets for dynamic nodes and relationships\n",
    "        self.allowed_nodes = set()\n",
    "        self.allowed_relationships = set()\n",
    "\n",
    "    def _init_pipeline(self):\n",
    "        \"\"\"Initialize the LLM pipeline with optimized settings\"\"\"\n",
    "        try:\n",
    "            text_pipe = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            self.llm = HuggingFacePipeline(pipeline=text_pipe)\n",
    "\n",
    "            # Configure the graph transformer with explicit schema\n",
    "            self.graph_transformer = LLMGraphTransformer(\n",
    "                llm=self.llm,\n",
    "                allowed_nodes=[\"Disease\", \"Symptom\", \"Treatment\", \"Medication\", \"Patient\"],\n",
    "                allowed_relationships=[\"HAS_SYMPTOM\", \"TREATS\", \"PRESCRIBED\", \"DIAGNOSED_WITH\"]\n",
    "            )\n",
    "            logger.info(\"Pipeline and graph transformer initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize pipeline: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_data(self, dataframe: pd.DataFrame, text_column: str = 'input'):\n",
    "        \"\"\"Process the dataframe to extract entities and relationships\"\"\"\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            logger.info(f\"Processing row {idx}\")\n",
    "            text_input = row[text_column]\n",
    "\n",
    "            if not isinstance(text_input, str) or not text_input.strip():\n",
    "                logger.warning(f\"Skipping empty or invalid input in row {idx}\")\n",
    "                continue\n",
    "\n",
    "            doc = Document(page_content=text_input)\n",
    "\n",
    "            try:\n",
    "                graph_data = self.graph_transformer.process_response(doc)\n",
    "                logger.debug(f\"Graph data structure: {graph_data}\")  # Debug log to see structure\n",
    "                \n",
    "                # Process nodes\n",
    "                if hasattr(graph_data, 'nodes'):\n",
    "                    nodes = graph_data.nodes\n",
    "                    for node in nodes:\n",
    "                        self.allowed_nodes.add(node.type)\n",
    "                else:\n",
    "                    logger.warning(f\"No nodes found in row {idx}\")\n",
    "                    nodes = []\n",
    "\n",
    "                # Process relationships/edges\n",
    "                relationships = []\n",
    "                if hasattr(graph_data, 'relationships'):\n",
    "                    relationships = graph_data.relationships\n",
    "                    for rel in relationships:\n",
    "                        self.allowed_relationships.add(rel.type)\n",
    "                elif hasattr(graph_data, 'edges'):\n",
    "                    relationships = graph_data.edges\n",
    "                    for edge in relationships:\n",
    "                        self.allowed_relationships.add(edge.type)\n",
    "                else:\n",
    "                    logger.warning(f\"No relationships/edges found in row {idx}\")\n",
    "\n",
    "                self._populate_graph(nodes, relationships)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process row {idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def _populate_graph(self, nodes, relationships):\n",
    "        \"\"\"Populate the Neo4j graph with extracted data\"\"\"\n",
    "        try:\n",
    "            # Create nodes\n",
    "            for node in nodes:\n",
    "                if not hasattr(node, 'type') or not hasattr(node, 'id'):\n",
    "                    logger.warning(f\"Skipping invalid node: {node}\")\n",
    "                    continue\n",
    "                    \n",
    "                cypher = \"\"\"\n",
    "                MERGE (n:{node_type} {{id: $id}})\n",
    "                SET n += $props\n",
    "                \"\"\".format(node_type=node.type)\n",
    "                \n",
    "                props = node.attributes if hasattr(node, 'attributes') else {}\n",
    "                \n",
    "                self.graph.query(\n",
    "                    cypher,\n",
    "                    parameters={\"id\": node.id, \"props\": props}\n",
    "                )\n",
    "\n",
    "            # Create relationships\n",
    "            for rel in relationships:\n",
    "                if not all(hasattr(rel, attr) for attr in ['type', 'source', 'target']):\n",
    "                    logger.warning(f\"Skipping invalid relationship: {rel}\")\n",
    "                    continue\n",
    "                    \n",
    "                cypher = \"\"\"\n",
    "                MATCH (a:{source_type} {{id: $source_id}})\n",
    "                MATCH (b:{target_type} {{id: $target_id}})\n",
    "                MERGE (a)-[r:{rel_type}]->(b)\n",
    "                \"\"\".format(\n",
    "                    source_type=rel.source.type,\n",
    "                    target_type=rel.target.type,\n",
    "                    rel_type=rel.type\n",
    "                )\n",
    "                \n",
    "                self.graph.query(\n",
    "                    cypher,\n",
    "                    parameters={\n",
    "                        \"source_id\": rel.source.id,\n",
    "                        \"target_id\": rel.target.id\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in _populate_graph: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_dynamic_constraints(self):\n",
    "        \"\"\"Set up Neo4j schema with dynamic constraints\"\"\"\n",
    "        for node_type in self.allowed_nodes:\n",
    "            try:\n",
    "                self.graph.query(f\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:{node_type}) REQUIRE n.id IS UNIQUE\")\n",
    "                logger.info(f\"Created constraint for node type: {node_type}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error creating constraint for node {node_type}: {e}\")\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# Initialize model and tokenizer with Llama model\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # Using FP16 for efficiency\n",
    ")\n",
    "\n",
    "# Initialize graph builder\n",
    "graph_builder = ClinicalKnowledgeGraphBuilder(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    graph=neo4j_graph\n",
    ")\n",
    "\n",
    "# Process your already loaded mimic_iv_bhc_100 dataframe\n",
    "graph_builder.process_data(mimic_iv_bhc_100, text_column='input')\n",
    "graph_builder.setup_dynamic_constraints()\n",
    "\n",
    "logger.info(\"Knowledge graph construction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78727ee-a4b2-4c0f-bfc9-3e114dae2305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# Get sample nodes of each type\n",
    "result = neo4j_graph.query(\"\"\"\n",
    "MATCH (n)\n",
    "WITH labels(n)[0] as node_type, collect(n)[0..5] as samples\n",
    "RETURN node_type, samples\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sample nodes by type:\")\n",
    "for row in result:\n",
    "    print(f\"\\n{row['node_type']}:\")\n",
    "    for node in row['samples']:\n",
    "        print(f\"  - {node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddcdad35-501a-4997-b8d7-afa7b7fcb1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 68\n",
      "Total relationships: 0\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "# Get total number of nodes\n",
    "node_count = neo4j_graph.query(\"\"\"\n",
    "MATCH (n)\n",
    "RETURN count(n) as count\n",
    "\"\"\")\n",
    "\n",
    "# Get total number of relationships\n",
    "rel_count = neo4j_graph.query(\"\"\"\n",
    "MATCH ()-[r]->()\n",
    "RETURN count(r) as count\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Total nodes: {node_count[0]['count']}\")\n",
    "print(f\"Total relationships: {rel_count[0]['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cffbc6f9-bb45-4de5-85d9-939273052960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LLMGraphTransformer in module langchain_experimental.graph_transformers.llm:\n",
      "\n",
      "class LLMGraphTransformer(builtins.object)\n",
      " |  LLMGraphTransformer(llm: langchain_core.language_models.base.BaseLanguageModel, allowed_nodes: List[str] = [], allowed_relationships: Union[List[str], List[Tuple[str, str, str]]] = [], prompt: Optional[langchain_core.prompts.chat.ChatPromptTemplate] = None, strict_mode: bool = True, node_properties: Union[bool, List[str]] = False, relationship_properties: Union[bool, List[str]] = False, ignore_tool_usage: bool = False, additional_instructions: str = '') -> None\n",
      " |  \n",
      " |  Transform documents into graph-based documents using a LLM.\n",
      " |  \n",
      " |  It allows specifying constraints on the types of nodes and relationships to include\n",
      " |  in the output graph. The class supports extracting properties for both nodes and\n",
      " |  relationships.\n",
      " |  \n",
      " |  Args:\n",
      " |      llm (BaseLanguageModel): An instance of a language model supporting structured\n",
      " |        output.\n",
      " |      allowed_nodes (List[str], optional): Specifies which node types are\n",
      " |        allowed in the graph. Defaults to an empty list, allowing all node types.\n",
      " |      allowed_relationships (List[str], optional): Specifies which relationship types\n",
      " |        are allowed in the graph. Defaults to an empty list, allowing all relationship\n",
      " |        types.\n",
      " |      prompt (Optional[ChatPromptTemplate], optional): The prompt to pass to\n",
      " |        the LLM with additional instructions.\n",
      " |      strict_mode (bool, optional): Determines whether the transformer should apply\n",
      " |        filtering to strictly adhere to `allowed_nodes` and `allowed_relationships`.\n",
      " |        Defaults to True.\n",
      " |      node_properties (Union[bool, List[str]]): If True, the LLM can extract any\n",
      " |        node properties from text. Alternatively, a list of valid properties can\n",
      " |        be provided for the LLM to extract, restricting extraction to those specified.\n",
      " |      relationship_properties (Union[bool, List[str]]): If True, the LLM can extract\n",
      " |        any relationship properties from text. Alternatively, a list of valid\n",
      " |        properties can be provided for the LLM to extract, restricting extraction to\n",
      " |        those specified.\n",
      " |      ignore_tool_usage (bool): Indicates whether the transformer should\n",
      " |        bypass the use of structured output functionality of the language model.\n",
      " |        If set to True, the transformer will not use the language model's native\n",
      " |        function calling capabilities to handle structured output. Defaults to False.\n",
      " |      additional_instructions (str): Allows you to add additional instructions\n",
      " |        to the prompt without having to change the whole prompt.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |          from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
      " |          from langchain_core.documents import Document\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |  \n",
      " |          llm=ChatOpenAI(temperature=0)\n",
      " |          transformer = LLMGraphTransformer(\n",
      " |              llm=llm,\n",
      " |              allowed_nodes=[\"Person\", \"Organization\"])\n",
      " |  \n",
      " |          doc = Document(page_content=\"Elon Musk is suing OpenAI\")\n",
      " |          graph_documents = transformer.convert_to_graph_documents([doc])\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, llm: langchain_core.language_models.base.BaseLanguageModel, allowed_nodes: List[str] = [], allowed_relationships: Union[List[str], List[Tuple[str, str, str]]] = [], prompt: Optional[langchain_core.prompts.chat.ChatPromptTemplate] = None, strict_mode: bool = True, node_properties: Union[bool, List[str]] = False, relationship_properties: Union[bool, List[str]] = False, ignore_tool_usage: bool = False, additional_instructions: str = '') -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  async aconvert_to_graph_documents(self, documents: Sequence[langchain_core.documents.base.Document], config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> List[langchain_community.graphs.graph_document.GraphDocument]\n",
      " |      Asynchronously convert a sequence of documents into graph documents.\n",
      " |  \n",
      " |  async aprocess_response(self, document: langchain_core.documents.base.Document, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> langchain_community.graphs.graph_document.GraphDocument\n",
      " |      Asynchronously processes a single document, transforming it into a\n",
      " |      graph document.\n",
      " |  \n",
      " |  convert_to_graph_documents(self, documents: Sequence[langchain_core.documents.base.Document], config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> List[langchain_community.graphs.graph_document.GraphDocument]\n",
      " |      Convert a sequence of documents into graph documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (Sequence[Document]): The original documents.\n",
      " |          kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Sequence[GraphDocument]: The transformed documents as graphs.\n",
      " |  \n",
      " |  process_response(self, document: langchain_core.documents.base.Document, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> langchain_community.graphs.graph_document.GraphDocument\n",
      " |      Processes a single document, transforming it into a graph document using\n",
      " |      an LLM based on the model's schema and constraints.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LLMGraphTransformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738da57-b36e-4daa-b565-2290009b95f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"Configuration for the knowledge graph building process\"\"\"\n",
    "    batch_size: int = 5\n",
    "    max_retries: int = 3\n",
    "    confidence_threshold: float = 0.5\n",
    "\n",
    "class ClinicalKnowledgeGraphBuilder:\n",
    "    \"\"\"\n",
    "    A builder for creating clinical knowledge graphs from MIMIC-IV data\n",
    "    using LLMGraphTransformer and Neo4j.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        graph: Neo4jGraph,\n",
    "        config: Optional[GraphConfig] = None\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.graph = graph\n",
    "        self.config = config or GraphConfig()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # Initialize LLM pipeline\n",
    "        self._init_pipeline()\n",
    "\n",
    "        # Placeholder for dynamic nodes and relationships\n",
    "        self.allowed_nodes = set()\n",
    "        self.allowed_relationships = set()\n",
    "\n",
    "    def _init_pipeline(self):\n",
    "        \"\"\"Initialize the LLM pipeline with optimized settings\"\"\"\n",
    "        text_pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=self.model_name,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        self.llm = HuggingFacePipeline(pipeline=text_pipe)\n",
    "\n",
    "        # Configure the graph transformer\n",
    "        self.graph_transformer = LLMGraphTransformer(\n",
    "            llm=self.llm\n",
    "        )\n",
    "        logger.info(\"Pipeline and graph transformer initialized\")\n",
    "\n",
    "    def process_data(self, dataframe: pd.DataFrame):\n",
    "        \"\"\"Process the dataframe to extract entities and relationships\"\"\"\n",
    "        for _, row in dataframe.iterrows():\n",
    "            text_input = row['input']\n",
    "            graph_data = self.graph_transformer.run(text_input)\n",
    "\n",
    "            for node in graph_data.nodes:\n",
    "                self.allowed_nodes.add(node.type)\n",
    "            for edge in graph_data.edges:\n",
    "                self.allowed_relationships.add(edge.type)\n",
    "\n",
    "            self._populate_graph(graph_data)\n",
    "\n",
    "    def _populate_graph(self, graph_data):\n",
    "        \"\"\"Populate the Neo4j graph with extracted data\"\"\"\n",
    "        for node in graph_data.nodes:\n",
    "            self.graph.query(\n",
    "                f\"MERGE (n:{node.type} {{id: '{node.id}'}}) SET n += {{props}}\",\n",
    "                parameters={\"props\": node.attributes}\n",
    "            )\n",
    "        for edge in graph_data.edges:\n",
    "            self.graph.query(\n",
    "                f\"MATCH (a:{edge.source.type} {{id: '{edge.source.id}'}}), (b:{edge.target.type} {{id: '{edge.target.id}'}}) \\\n",
    "                MERGE (a)-[r:{edge.type}]->(b)\"\n",
    "            )\n",
    "\n",
    "    def setup_dynamic_constraints(self):\n",
    "        \"\"\"Set up Neo4j schema with dynamic constraints\"\"\"\n",
    "        for node_type in self.allowed_nodes:\n",
    "            try:\n",
    "                self.graph.query(f\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:{node_type}) REQUIRE n.id IS UNIQUE\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error creating constraint for node {node_type}: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "# Assuming you have a Neo4jGraph instance and the mimic_iv_bhc_100 dataframe\n",
    "neo4j_graph = Neo4jGraph(url=\"neo4j+s://8a886660.databases.neo4j.io\", username=\"neo4j\", password=\"9FYDkCTM2Vq4qxWFFwik0uYP6BJ-fReP9XOYj-oDqZ4\")\n",
    "\n",
    "\n",
    "\n",
    "# Process the data and build the graph\n",
    "graph_builder.process_data(mimic_iv_bhc_100)\n",
    "graph_builder.setup_dynamic_constraints()\n",
    "\n",
    "logger.info(\"Knowledge graph construction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b68ce-38b9-4dc6-94c0-65417293e13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5f316-698a-4544-811f-1f2fe0bac581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6424b2a-fd06-42ef-8ef3-b31d51a9b9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea8e66-3b55-45a2-bc8d-7c04e36a2590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d452dac-ca64-42a3-9ccc-92bb8abb939f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2009d-2006-44d8-ba4e-6368223fc6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15068cd2-d91e-46e5-ad23-7f2fa2bf370f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d056ce8-3368-4587-baea-79d736a302bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0956e-1e7d-4dc2-8119-8d7321f70043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80806b90-4fa2-4440-a5a0-b4bfaf3e09d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8125c7-687a-4005-86f0-e660fb3a149a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426a13a-2e34-460b-bc3e-101de3d32907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480169f-8341-488b-a459-2843cb396ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadeae2-85b6-41ea-ac30-cd55837f1744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67629779-3a7b-41e1-a1b1-3c8fe9e0f9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ac5f2-ed31-4f81-86ed-457cc2a24543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf543af-69dc-4f2b-9083-ebafa21bb063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain-core langchain-community langchain-experimental transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9ed24-18d2-4ab1-ad6c-c6ac32acd589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"Configuration for the knowledge graph building process\"\"\"\n",
    "    batch_size: int = 5\n",
    "    max_retries: int = 3\n",
    "    confidence_threshold: float = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c5a02-867a-4823-8ad4-7b006181ed75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "class ClinicalKnowledgeGraphBuilder:\n",
    "    \"\"\"\n",
    "    A builder for creating clinical knowledge graphs from MIMIC-IV data\n",
    "    using LLMGraphTransformer and Neo4j.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        graph: Neo4jGraph,\n",
    "        config: Optional[GraphConfig] = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.graph = graph\n",
    "        self.config = config or GraphConfig()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model.name_or_path)\n",
    "\n",
    "        # Initialize LLM pipeline\n",
    "        self._init_pipeline()\n",
    "\n",
    "        # Set up graph constraints and indexes\n",
    "        self._setup_graph_schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e0afd-77b4-4632-bab8-5c1822d50c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    def _init_pipeline(self):\n",
    "        \"\"\"Initialize the LLM pipeline with optimized settings\"\"\"\n",
    "        text_pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        self.llm = HuggingFacePipeline(pipeline=text_pipe)\n",
    "\n",
    "        # Configure the graph transformer\n",
    "        self.graph_transformer = LLMGraphTransformer(\n",
    "            llm=self.llm,\n",
    "            allowed_nodes=[\"Patient\", \"Condition\", \"Medication\", \"Procedure\",\n",
    "                           \"Service\", \"Lab\", \"Allergy\", \"Symptom\"],\n",
    "            allowed_relationships=[\n",
    "                \"HAS_CONDITION\", \"PRESCRIBED\", \"UNDERWENT\", \"ADMITTED_TO\",\n",
    "                \"HAS_ALLERGY\", \"EXHIBITS\", \"RECEIVED\", \"RESULTED_IN\"\n",
    "            ]\n",
    "        )\n",
    "        logger.info(\"Pipeline and graph transformer initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559de93-6991-4585-8e3a-330bdbdbec80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    def _setup_graph_schema(self):\n",
    "        \"\"\"Set up Neo4j schema with constraints and indexes\"\"\"\n",
    "        constraints = [\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Patient) REQUIRE p.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Condition) REQUIRE c.name IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Medication) REQUIRE m.name IS UNIQUE\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (p:Patient) ON (p.sex)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (p:Patient) ON (p.service)\"\n",
    "        ]\n",
    "\n",
    "        for constraint in constraints:\n",
    "            try:\n",
    "                self.graph.query(constraint)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error creating constraint: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24108e2-fba1-4afa-bf39-81a415406c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _preprocess_clinical_note(self, note: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess clinical notes from input field with XML-like tags.\n",
    "    Enhanced to better handle MIMIC-IV format.\n",
    "    \"\"\"\n",
    "    fields = {}\n",
    "    xml_tags = ['SEX', 'SERVICE', 'ALLERGIES', 'PROCEDURES', 'MEDICATIONS']\n",
    "\n",
    "    for tag in xml_tags:\n",
    "        pattern = f'<{tag}>(.*?)<'\n",
    "        match = re.search(pattern, note, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            fields[tag.lower()] = match.group(1).strip()\n",
    "\n",
    "    context = []\n",
    "    if fields.get('sex'):\n",
    "        context.append(f\"Patient Sex: {fields['sex']}\")\n",
    "    if fields.get('service'):\n",
    "        context.append(f\"Medical Service: {fields['service']}\")\n",
    "    if fields.get('allergies'):\n",
    "        allergies = fields['allergies'].strip()\n",
    "        if allergies.lower() not in ['none', 'no known allergies', 'nka']:\n",
    "            context.append(f\"Allergies: {allergies}\")\n",
    "\n",
    "    cleaned_text = re.sub(r'<.*?>', '', note)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    full_text = \" | \".join(context)\n",
    "    return f\"{full_text}\\n\\nClinical Note:\\n{cleaned_text}\" if full_text else cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed01ea9-433b-471f-b9f6-a5aa4fd49ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f1141-a056-4fc1-ab2c-ccd71a5ea3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c0ef3-43c0-45cc-b68e-16858f05df42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c6cea-b516-4519-bead-9b68349332d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250b2e5-4099-465b-84a9-cfcbac734ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e82fe9-d1b9-4774-b6a9-470e1e227a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd8567-c101-4949-95b0-53973a4a8710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8e0c4-a108-4af2-b2dc-684a65940ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e49de3-0599-46ee-9e2d-d06edbbdb2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.docstore.document import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"Configuration for the knowledge graph building process\"\"\"\n",
    "    batch_size: int = 5\n",
    "    max_retries: int = 3\n",
    "    confidence_threshold: float = 0.7\n",
    "\n",
    "class ClinicalKnowledgeGraphBuilder:\n",
    "    \"\"\"\n",
    "    A builder for creating clinical knowledge graphs from MIMIC-IV data\n",
    "    using LLMGraphTransformer and Neo4j.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        graph: Neo4jGraph,\n",
    "        config: Optional[GraphConfig] = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.graph = graph\n",
    "        self.config = config or GraphConfig()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model.name_or_path)\n",
    "\n",
    "        # Initialize LLM pipeline\n",
    "        self._init_pipeline()\n",
    "\n",
    "        # Set up graph constraints and indexes\n",
    "        self._setup_graph_schema()\n",
    "\n",
    "    def _init_pipeline(self):\n",
    "        \"\"\"Initialize the LLM pipeline with optimized settings\"\"\"\n",
    "        text_pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        llm = HuggingFacePipeline(pipeline=text_pipe)\n",
    "\n",
    "        # Configure the graph transformer with medical domain specifics\n",
    "        self.graph_transformer = LLMGraphTransformer(\n",
    "            llm=llm,\n",
    "            allowed_nodes=[\"Patient\", \"Condition\", \"Medication\", \"Procedure\",\n",
    "                           \"Service\", \"Lab\", \"Allergy\", \"Symptom\"],\n",
    "            allowed_relationships=[\n",
    "                \"HAS_CONDITION\", \"PRESCRIBED\", \"UNDERWENT\", \"ADMITTED_TO\",\n",
    "                \"HAS_ALLERGY\", \"EXHIBITS\", \"RECEIVED\", \"RESULTED_IN\"\n",
    "            ]\n",
    "        )\n",
    "        logger.info(\"Pipeline and graph transformer initialized\")\n",
    "\n",
    "    def _setup_graph_schema(self):\n",
    "        \"\"\"Set up Neo4j schema with constraints and indexes\"\"\"\n",
    "        constraints = [\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Patient) REQUIRE p.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Condition) REQUIRE c.name IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Medication) REQUIRE m.name IS UNIQUE\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (p:Patient) ON (p.sex)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS FOR (p:Patient) ON (p.service)\"\n",
    "        ]\n",
    "\n",
    "        for constraint in constraints:\n",
    "            try:\n",
    "                self.graph.query(constraint)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error creating constraint: {e}\")\n",
    "\n",
    "    def _preprocess_clinical_note(self, note: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess clinical notes from input field with XML-like tags.\n",
    "        Enhanced to better handle MIMIC-IV format.\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "        xml_tags = ['SEX', 'SERVICE', 'ALLERGIES', 'PROCEDURES', 'MEDICATIONS']\n",
    "\n",
    "        for tag in xml_tags:\n",
    "            pattern = f'<{tag}>(.*?)<'\n",
    "            match = re.search(pattern, note, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                fields[tag.lower()] = match.group(1).strip()\n",
    "\n",
    "        self._create_structured_nodes(fields)\n",
    "\n",
    "        context = []\n",
    "        if fields.get('sex'):\n",
    "            context.append(f\"Patient Sex: {fields['sex']}\")\n",
    "        if fields.get('service'):\n",
    "            context.append(f\"Medical Service: {fields['service']}\")\n",
    "        if fields.get('allergies'):\n",
    "            allergies = fields['allergies'].strip()\n",
    "            if allergies.lower() not in ['none', 'no known allergies', 'nka']:\n",
    "                context.append(f\"Allergies: {allergies}\")\n",
    "\n",
    "        cleaned_text = re.sub(r'<.*?>', '', note)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "        full_text = \" | \".join(context)\n",
    "        return f\"{full_text}\\n\\nClinical Note:\\n{cleaned_text}\" if full_text else cleaned_text\n",
    "\n",
    " \n",
    "    def process_note(self, note_id: str, note_text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single clinical note and extract graph elements\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Preprocess the note\n",
    "            processed_text = self._preprocess_clinical_note(note_text)\n",
    "\n",
    "            # Create a single document with the processed text\n",
    "            doc = Document(\n",
    "                page_content=processed_text,\n",
    "                metadata={\"source\": note_id}\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Processing note {note_id}\")\n",
    "            try:\n",
    "                # Convert the document into graph format\n",
    "                graph_docs = self.graph_transformer.convert_to_graph_documents([doc])\n",
    "\n",
    "                # Handle graph_docs properly\n",
    "                if not isinstance(graph_docs, list):\n",
    "                    graph_docs = [graph_docs]\n",
    "\n",
    "                logger.info(f\"Extracted graph elements\")\n",
    "\n",
    "                # Process elements\n",
    "                elements = []\n",
    "                for doc in graph_docs:\n",
    "                    if hasattr(doc, \"to_dict\"):  # Use a method or attribute to extract data\n",
    "                        elements.append(doc.to_dict())  # Adjust based on `GraphDocument` API\n",
    "                    else:\n",
    "                        logger.warning(f\"GraphDocument object has no 'to_dict' method\")\n",
    "\n",
    "                if elements:\n",
    "                    unique_elements = self._merge_graph_elements(elements)\n",
    "                    logger.info(f\"Processed note {note_id}, found {len(unique_elements)} unique elements\")\n",
    "                    return unique_elements\n",
    "                else:\n",
    "                    logger.warning(f\"No elements found in note {note_id}\")\n",
    "                    return []\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting graph elements: {e}\")\n",
    "                return []\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing note {note_id}: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def _create_structured_nodes(self, fields: Dict[str, str]):\n",
    "        try:\n",
    "            if fields.get('service'):\n",
    "                self.graph.query(\n",
    "                    \"MERGE (s:Service {name: $service_name})\",\n",
    "                    {\"service_name\": fields['service']}\n",
    "                )\n",
    "\n",
    "            if fields.get('allergies'):\n",
    "                allergies = [a.strip() for a in fields['allergies'].split(',')]\n",
    "                for allergy in allergies:\n",
    "                    if allergy.lower() not in ['none', 'no known allergies', 'nka']:\n",
    "                        self.graph.query(\n",
    "                            \"MERGE (a:Allergy {name: $allergy_name})\",\n",
    "                            {\"allergy_name\": allergy}\n",
    "                        )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating structured nodes: {e}\")\n",
    "\n",
    "    def _merge_graph_elements(self, elements: List[Dict]) -> List[Dict]:\n",
    "        merged = {}\n",
    "        for element in elements:\n",
    "            key = f\"{element['type']}:{element.get('name', '')}\"\n",
    "            if key not in merged:\n",
    "                merged[key] = element\n",
    "            else:\n",
    "                merged[key]['properties'].update(element.get('properties', {}))\n",
    "                merged[key]['relationships'].extend(element.get('relationships', []))\n",
    "        return list(merged.values())\n",
    "\n",
    "    def build_graph(self, df: pd.DataFrame):\n",
    "        logger.info(f\"Starting to process {len(df)} clinical notes...\")\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            try:\n",
    "                logger.info(f\"\\nProcessing note {row['note_id']} ({idx + 1}/{len(df)})\")\n",
    "                elements = self.process_note(row['note_id'], row['input'])\n",
    "                if elements:\n",
    "                    self._update_graph_with_elements(elements)\n",
    "                else:\n",
    "                    logger.warning(f\"No elements found in note {row['note_id']}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing note {row['note_id']}: {e}\")\n",
    "\n",
    "        logger.info(\"Graph construction completed\")\n",
    "\n",
    "    def _update_graph_with_elements(self, elements: List[Dict]):\n",
    "        try:\n",
    "            nodes = [e for e in elements if e['type'] in self.graph_transformer.allowed_nodes]\n",
    "            relationships = [e for e in elements if e['type'] in self.graph_transformer.allowed_relationships]\n",
    "\n",
    "            for node in nodes:\n",
    "                query, params = self._build_node_cypher(node)\n",
    "                self.graph.query(query, **params)\n",
    "\n",
    "            for rel in relationships:\n",
    "                query, params = self._build_relationship_cypher(rel)\n",
    "                self.graph.query(query, **params)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating graph: {e}\")\n",
    "\n",
    "    def _build_node_cypher(self, node: Dict) -> str:\n",
    "        props = {k: v for k, v in node.get('properties', {}).items() if v is not None and v != ''}\n",
    "        props['source'] = node['source']\n",
    "\n",
    "        query = f\"\"\"\n",
    "        MERGE (n:{node['type']} {{name: $name}})\n",
    "        SET n += $props\n",
    "        \"\"\"\n",
    "        return query, {\"name\": node['name'], \"props\": props}\n",
    "\n",
    "    def _build_relationship_cypher(self, rel: Dict) -> str:\n",
    "        query = f\"\"\"\n",
    "        MATCH (a:{rel['start']['type']} {{name: $start_name}})\n",
    "        MATCH (b:{rel['end']['type']} {{name: $end_name}})\n",
    "        MERGE (a)-[r:{rel['type']}]->(b)\n",
    "        SET r += $props\n",
    "        \"\"\"\n",
    "        return query, {\n",
    "            \"start_name\": rel['start']['name'],\n",
    "            \"end_name\": rel['end']['name'],\n",
    "            \"props\": rel.get('properties', {})\n",
    "        }\n",
    "\n",
    "    def validate_graph(self) -> Dict[str, Any]:\n",
    "        validation_queries = {\n",
    "            \"node_counts\": \"MATCH (n) RETURN labels(n) as type, count(*) as count\",\n",
    "            \"relationship_counts\": \"MATCH ()-[r]->() RETURN type(r) as type, count(*) as count\",\n",
    "            \"orphaned_nodes\": \"MATCH (n) WHERE NOT (n)--() RETURN labels(n) as type, count(*) as count\"\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        for name, query in validation_queries.items():\n",
    "            results[name] = self.graph.query(query)\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ce835-7dbe-45c1-a6fa-b51f5cc480d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install json-repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066008db-482d-40a9-b6d5-6a270794b6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Knowledge Graph Builder\n",
    "kg_builder = ClinicalKnowledgeGraphBuilder(\n",
    "    model=model,\n",
    "    graph=graph\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532ed9a-74b2-4ee5-aad3-99cc324443ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the knowledge graph with the dataset\n",
    "kg_builder.build_graph(mimic_iv_bhc_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167a500-a0d6-4558-a975-bcf8b078e60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0c701-77e8-43a6-9167-a4f7236d3b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-huggingface\n",
    "from langchain_huggingface import HuggingFacePipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f5472-6863-4458-958e-04b44c73ddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
