{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825838cd-aeac-4593-8920-e02e2a24e768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub langchain_community\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q neo4j\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161b54d3-791b-4cd4-91be-89900c0e18e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce108d38-28a3-416b-9d1f-faf8105fc7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-neo4j in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-neo4j) (0.3.23)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-neo4j) (0.3.51)\n",
      "Requirement already satisfied: neo4j<6.0.0,>=5.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-neo4j) (5.28.1)\n",
      "Requirement already satisfied: neo4j-graphrag<2.0.0,>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-neo4j) (1.6.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.7->langchain-neo4j) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.7->langchain-neo4j) (0.3.28)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.7->langchain-neo4j) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.7->langchain-neo4j) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.7->langchain-neo4j) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.7->langchain-neo4j) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.7->langchain-neo4j) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain-neo4j) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain-neo4j) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain-neo4j) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain-neo4j) (4.12.2)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from neo4j<6.0.0,>=5.25.0->langchain-neo4j) (2024.1)\n",
      "Requirement already satisfied: fsspec<2025.0.0,>=2024.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (2024.12.0)\n",
      "Requirement already satisfied: json-repair<0.40.0,>=0.39.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (0.39.1)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (5.4.0)\n",
      "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (6.0.12.20250402)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain-neo4j) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain-neo4j) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain-neo4j) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain-neo4j) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain-neo4j) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain-neo4j) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain-neo4j) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.7->langchain-neo4j) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain-neo4j) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-neo4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4cecdb2-9491-4594-8dcf-bc25c2153a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_neo4j import Neo4jGraph  # Updated import statement\n",
    "\n",
    "# Graphdb configuration\n",
    "NEO4J_URI = \"neo4j+s://9fc92ebb.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"aYQ1i5kRDCdrnjKP2DKe0hCg6M8I-sWVWi4ntsKiMhM\"\n",
    "\n",
    "# Setting environment variables for Neo4j\n",
    "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
    "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
    "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD\n",
    "\n",
    "# Creating a Neo4j graph instance\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    ")\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "neo4j_graph=Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4d962d6-9fc0-4221-809d-f4ce31bdcfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x7f896adbb1f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neo4j_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84896e12-325a-4b65-9e74-e0157df207f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                              output  \\\n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...   \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...   \n",
      "2  Subjective:\\n- No known allergies to medicatio...   \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...   \n",
      "4  Subjective:\\n- Presenting with chest pain for ...   \n",
      "\n",
      "                                     id  \\\n",
      "0  39a26c55-f710-4272-8609-2a725ef6d068   \n",
      "1  b1448089-3c41-423a-9737-0ed25e0e99c8   \n",
      "2  78cf9b57-3a1f-41df-ba55-af3bbe843228   \n",
      "3  44be7957-be23-4cc9-a5e2-cb7139dc2079   \n",
      "4  3324cd95-1729-4b0b-8916-dba010ac811e   \n",
      "\n",
      "                                            entities  \\\n",
      "0  {'PROBLEM': ['radiculopathy', 'the pain', 'wea...   \n",
      "1  {'PROBLEM': ['known sick contacts', 'flu - lik...   \n",
      "2  {'PROBLEM': ['known allergies'], 'TREATMENT': ...   \n",
      "3  {'PROBLEM': ['hee', 'sp', 'blood', 'symptoms',...   \n",
      "4  {'PROBLEM': ['significant stress', 'chronic co...   \n",
      "\n",
      "                                            problems  \\\n",
      "0  ['radiculopathy', 'the pain', 'weakness in the...   \n",
      "1  ['known sick contacts', 'flu - like illness', ...   \n",
      "2                                ['known allergies']   \n",
      "3  ['hee', 'sp', 'blood', 'symptoms', 'w', 'um', ...   \n",
      "4  ['significant stress', 'chronic conditions', '...   \n",
      "\n",
      "                                          treatments  \\\n",
      "0  ['anti - inflammatory medications', 'treatment...   \n",
      "1  ['appendect', 'omy', 'infection control measur...   \n",
      "2                                    ['medications']   \n",
      "3                                   ['multivitamin']   \n",
      "4      ['advil', 'tu', 'ms', 'tylenol', 'treatment']   \n",
      "\n",
      "                                               tests  \n",
      "0  ['differential diagnoses', 'x - rays of the lo...  \n",
      "1                 ['sw', 'covid', 'swab', 'testing']  \n",
      "2                                                 []  \n",
      "3                   ['covid', 'swab', 'temperature']  \n",
      "4  ['physical examination', 's signs', 'vital', '...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "NER_df = pd.read_csv(\"processed_output.csv\")\n",
    "\n",
    "# Check the data\n",
    "print(NER_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb79409c-5ca6-47d9-824b-dd7d474dd8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['the ant bait', 'borax', 'ant bait', 'medications']\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_df['treatments'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a853f0d-b038-4de0-b26e-324dc7553b19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input         100\n",
       "output        100\n",
       "id            100\n",
       "entities       95\n",
       "problems       84\n",
       "treatments     51\n",
       "tests          41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bd5c0bd-50b1-4b05-b481-f6962ab4c341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Knowledge Graph: 100%|██████████| 100/100 [15:55<00:00,  9.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to escape and preprocess text for Cypher queries\n",
    "def escape_text(text):\n",
    "    return text.replace(\"'\", \" \").replace('\"', ' ')  # Escape both single and double quotes\n",
    "\n",
    "# Function to build the Knowledge Graph for SOAP dataset\n",
    "def build_knowledge_graph_soap(df, neo4j_graph):\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building Knowledge Graph\"):\n",
    "        # Use existing ID from the DataFrame\n",
    "        patient_id = escape_text(str(row['id']))\n",
    "        \n",
    "        # Extract and preprocess data\n",
    "        problems = [escape_text(problem) for problem in row['problems']]\n",
    "        treatments = [escape_text(treatment) for treatment in row['treatments']]\n",
    "        tests = [escape_text(test) for test in row['tests']]\n",
    "\n",
    "        # Create Patient node\n",
    "        patient_query = f\"\"\"\n",
    "        MERGE (p:Patient {{id: '{patient_id}'}})\n",
    "        \"\"\"\n",
    "        neo4j_graph.query(patient_query)\n",
    "\n",
    "        # Create Problem nodes and relationships\n",
    "        for problem in problems:\n",
    "            problem_query = f\"\"\"\n",
    "            MERGE (pr:Problem {{title: '{problem}'}})\n",
    "            MERGE (p)-[:HAS_PROBLEM]->(pr)\n",
    "            \"\"\"\n",
    "            neo4j_graph.query(problem_query)\n",
    "\n",
    "        # Create Treatment nodes and relationships\n",
    "        for treatment in treatments:\n",
    "            treatment_query = f\"\"\"\n",
    "            MERGE (tr:Treatment {{title: '{treatment}'}})\n",
    "            MERGE (p)-[:WAS_TREATED_WITH]->(tr)\n",
    "            \"\"\"\n",
    "            neo4j_graph.query(treatment_query)\n",
    "\n",
    "        # Create Test nodes and relationships\n",
    "        for test in tests:\n",
    "            test_query = f\"\"\"\n",
    "            MERGE (t:Test {{title: '{test}'}})\n",
    "            MERGE (p)-[:UNDERWENT_TEST]->(t)\n",
    "            \"\"\"\n",
    "            neo4j_graph.query(test_query)\n",
    "\n",
    "# Example usage, assuming 'neo4j_graph' is already connected and 'df' is your DataFrame\n",
    "build_knowledge_graph_soap(NER_df, neo4j_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10230ab6-996c-472f-adb0-be48d517314f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Counts by Type:\n",
      "Node Type: None (No label), Count: {record['count']}\n",
      "Node Type: Patient, Count: 100\n",
      "Node Type: Problem, Count: 37\n",
      "Node Type: Treatment, Count: 34\n",
      "Node Type: Test, Count: 31\n",
      "\n",
      "Relationship Counts by Type:\n",
      "Relationship Type: HAS_PROBLEM, Count: 37\n",
      "Relationship Type: WAS_TREATED_WITH, Count: 34\n",
      "Relationship Type: UNDERWENT_TEST, Count: 31\n",
      "Error retrieving graph statistics: 'list' object has no attribute 'single'\n"
     ]
    }
   ],
   "source": [
    "def get_graph_statistics(neo4j_graph):\n",
    "    try:\n",
    "        # Query to count nodes by type\n",
    "        node_count_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN labels(n) AS node_type, COUNT(*) AS count\n",
    "        ORDER BY count DESC\n",
    "        \"\"\"\n",
    "\n",
    "        # Query to count relationships by type\n",
    "        relationship_count_query = \"\"\"\n",
    "        MATCH ()-[r]->()\n",
    "        RETURN TYPE(r) AS relationship_type, COUNT(*) AS count\n",
    "        ORDER BY count DESC\n",
    "        \"\"\"\n",
    "\n",
    "        # Query to count total nodes\n",
    "        total_nodes_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN COUNT(*) AS total_nodes\n",
    "        \"\"\"\n",
    "\n",
    "        # Query to count total relationships\n",
    "        total_relationships_query = \"\"\"\n",
    "        MATCH ()-[r]->()\n",
    "        RETURN COUNT(*) AS total_relationships\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute queries and fetch results\n",
    "        print(\"Node Counts by Type:\")\n",
    "        node_counts = neo4j_graph.query(node_count_query)\n",
    "        for record in node_counts:\n",
    "            node_type = record['node_type']\n",
    "            if node_type:  # Check if the node_type list is not empty\n",
    "                print(f\"Node Type: {node_type[0]}, Count: {record['count']}\")\n",
    "            else:\n",
    "                print(\"Node Type: None (No label), Count: {record['count']}\")  # Handle nodes with no labels\n",
    "\n",
    "        print(\"\\nRelationship Counts by Type:\")\n",
    "        relationship_counts = neo4j_graph.query(relationship_count_query)\n",
    "        for record in relationship_counts:\n",
    "            print(f\"Relationship Type: {record['relationship_type']}, Count: {record['count']}\")\n",
    "\n",
    "        total_nodes_result = neo4j_graph.query(total_nodes_query).single()\n",
    "        total_nodes = total_nodes_result['total_nodes'] if total_nodes_result else 0\n",
    "\n",
    "        total_relationships_result = neo4j_graph.query(total_relationships_query).single()\n",
    "        total_relationships = total_relationships_result['total_relationships'] if total_relationships_result else 0\n",
    "\n",
    "        print(\"\\nTotal Counts:\")\n",
    "        print(f\"Total Nodes: {total_nodes}\")\n",
    "        print(f\"Total Relationships: {total_relationships}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving graph statistics: {e}\")\n",
    "\n",
    "# Call the function\n",
    "get_graph_statistics(neo4j_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66a4a4a3-adc6-4aaf-896a-96e4537be83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>id</th>\n",
       "      <th>entities</th>\n",
       "      <th>problems</th>\n",
       "      <th>treatments</th>\n",
       "      <th>tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon, champ, how you holding up? Goo...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Lower back pain, radi...</td>\n",
       "      <td>39a26c55-f710-4272-8609-2a725ef6d068</td>\n",
       "      <td>{'PROBLEM': ['radiculopathy', 'the pain', 'wea...</td>\n",
       "      <td>['radiculopathy', 'the pain', 'weakness in the...</td>\n",
       "      <td>['anti - inflammatory medications', 'treatment...</td>\n",
       "      <td>['differential diagnoses', 'x - rays of the lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What brings you in here today? Hi, I'm um, I'm...</td>\n",
       "      <td>Subjective:\\n- Presenting with dry cough for 1...</td>\n",
       "      <td>b1448089-3c41-423a-9737-0ed25e0e99c8</td>\n",
       "      <td>{'PROBLEM': ['known sick contacts', 'flu - lik...</td>\n",
       "      <td>['known sick contacts', 'flu - like illness', ...</td>\n",
       "      <td>['appendect', 'omy', 'infection control measur...</td>\n",
       "      <td>['sw', 'covid', 'swab', 'testing']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you have any known allergies to medications...</td>\n",
       "      <td>Subjective:\\n- No known allergies to medicatio...</td>\n",
       "      <td>78cf9b57-3a1f-41df-ba55-af3bbe843228</td>\n",
       "      <td>{'PROBLEM': ['known allergies'], 'TREATMENT': ...</td>\n",
       "      <td>['known allergies']</td>\n",
       "      <td>['medications']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Good afternoon, champ, how you holding up? Goo...   \n",
       "1  What brings you in here today? Hi, I'm um, I'm...   \n",
       "2  Do you have any known allergies to medications...   \n",
       "\n",
       "                                              output  \\\n",
       "0  Subjective:\\n- Symptoms: Lower back pain, radi...   \n",
       "1  Subjective:\\n- Presenting with dry cough for 1...   \n",
       "2  Subjective:\\n- No known allergies to medicatio...   \n",
       "\n",
       "                                     id  \\\n",
       "0  39a26c55-f710-4272-8609-2a725ef6d068   \n",
       "1  b1448089-3c41-423a-9737-0ed25e0e99c8   \n",
       "2  78cf9b57-3a1f-41df-ba55-af3bbe843228   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'PROBLEM': ['radiculopathy', 'the pain', 'wea...   \n",
       "1  {'PROBLEM': ['known sick contacts', 'flu - lik...   \n",
       "2  {'PROBLEM': ['known allergies'], 'TREATMENT': ...   \n",
       "\n",
       "                                            problems  \\\n",
       "0  ['radiculopathy', 'the pain', 'weakness in the...   \n",
       "1  ['known sick contacts', 'flu - like illness', ...   \n",
       "2                                ['known allergies']   \n",
       "\n",
       "                                          treatments  \\\n",
       "0  ['anti - inflammatory medications', 'treatment...   \n",
       "1  ['appendect', 'omy', 'infection control measur...   \n",
       "2                                    ['medications']   \n",
       "\n",
       "                                               tests  \n",
       "0  ['differential diagnoses', 'x - rays of the lo...  \n",
       "1                 ['sw', 'covid', 'swab', 'testing']  \n",
       "2                                                 []  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e905794d-6305-4053-9990-8e5acdd51d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                        reduced_text  \\\n",
      "0  <|begin_of_text|>Good afternoon, champ, how yo...   \n",
      "1  <|begin_of_text|>What brings you in here today...   \n",
      "2  <|begin_of_text|>Do you have any known allergi...   \n",
      "3  <|begin_of_text|>How may I help you today? Yea...   \n",
      "4  <|begin_of_text|>It sounds like that you're ex...   \n",
      "\n",
      "                                              output  \\\n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...   \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...   \n",
      "2  Subjective:\\n- No known allergies to medicatio...   \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...   \n",
      "4  Subjective:\\n- Presenting with chest pain for ...   \n",
      "\n",
      "                                   generated_summary  \n",
      "0  A 75-year-old man presents with chronic lower ...  \n",
      "1  smell was fine, but now it's gone. Okay, okay....  \n",
      "2  A 35-year-old woman presents with symptoms of ...  \n",
      "3  A 35-year-old male patient presents with a 4-d...  \n",
      "4  least. OK, thank you for taking care of me today.  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   input              100 non-null    object\n",
      " 1   reduced_text       100 non-null    object\n",
      " 2   output             100 non-null    object\n",
      " 3   generated_summary  100 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reduced = pd.read_csv(\"soap_cptf_generated_summaries.csv\")\n",
    "# Display the first few rows\n",
    "print(reduced.head())\n",
    "\n",
    "# Check DataFrame info\n",
    "print(reduced.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b0542d9-5508-4ce8-bc71-33d3a4948d47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                              output  \\\n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...   \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...   \n",
      "2  Subjective:\\n- No known allergies to medicatio...   \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...   \n",
      "4  Subjective:\\n- Presenting with chest pain for ...   \n",
      "\n",
      "                                     id  \\\n",
      "0  39a26c55-f710-4272-8609-2a725ef6d068   \n",
      "1  b1448089-3c41-423a-9737-0ed25e0e99c8   \n",
      "2  78cf9b57-3a1f-41df-ba55-af3bbe843228   \n",
      "3  44be7957-be23-4cc9-a5e2-cb7139dc2079   \n",
      "4  3324cd95-1729-4b0b-8916-dba010ac811e   \n",
      "\n",
      "                                            entities  \\\n",
      "0  {'PROBLEM': ['radiculopathy', 'the pain', 'wea...   \n",
      "1  {'PROBLEM': ['known sick contacts', 'flu - lik...   \n",
      "2  {'PROBLEM': ['known allergies'], 'TREATMENT': ...   \n",
      "3  {'PROBLEM': ['hee', 'sp', 'blood', 'symptoms',...   \n",
      "4  {'PROBLEM': ['significant stress', 'chronic co...   \n",
      "\n",
      "                                            problems  \\\n",
      "0  ['radiculopathy', 'the pain', 'weakness in the...   \n",
      "1  ['known sick contacts', 'flu - like illness', ...   \n",
      "2                                ['known allergies']   \n",
      "3  ['hee', 'sp', 'blood', 'symptoms', 'w', 'um', ...   \n",
      "4  ['significant stress', 'chronic conditions', '...   \n",
      "\n",
      "                                          treatments  \\\n",
      "0  ['anti - inflammatory medications', 'treatment...   \n",
      "1  ['appendect', 'omy', 'infection control measur...   \n",
      "2                                    ['medications']   \n",
      "3                                   ['multivitamin']   \n",
      "4      ['advil', 'tu', 'ms', 'tylenol', 'treatment']   \n",
      "\n",
      "                                               tests  \\\n",
      "0  ['differential diagnoses', 'x - rays of the lo...   \n",
      "1                 ['sw', 'covid', 'swab', 'testing']   \n",
      "2                                                 []   \n",
      "3                   ['covid', 'swab', 'temperature']   \n",
      "4  ['physical examination', 's signs', 'vital', '...   \n",
      "\n",
      "                                        reduced_text  \n",
      "0  <|begin_of_text|>Good afternoon, champ, how yo...  \n",
      "1  <|begin_of_text|>What brings you in here today...  \n",
      "2  <|begin_of_text|>Do you have any known allergi...  \n",
      "3  <|begin_of_text|>How may I help you today? Yea...  \n",
      "4  <|begin_of_text|>It sounds like that you're ex...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'NER_df' and 'reduced' are your existing DataFrames\n",
    "\n",
    "# Ensure both dataframes are sorted or indexed in the same way if they are not already\n",
    "NER_df = NER_df.reset_index(drop=True)\n",
    "reduced = reduced.reset_index(drop=True)\n",
    "\n",
    "# Merge the 'reduced_text' column from 'reduced' into 'NER_df'\n",
    "NER_df['reduced_text'] = reduced['reduced_text']\n",
    "\n",
    "# Verify the merge by displaying the updated DataFrame\n",
    "print(NER_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cb9fa-151c-43f4-af57-206ab27e5c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e79a3b-d66f-4140-87dd-efd0e0e17693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bde81-5f05-49b6-bef8-535a6746ffe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a26702fb-a290-4f67-95e5-947a5ad38027",
   "metadata": {},
   "source": [
    "### Summarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19b312c2-8de2-492f-9729-f26fdc5e88d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "207b966f-5da2-42c4-bf11-83886073a879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73d1b8e1-5d78-4e86-82c2-80c0ec07a3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer:   0%|          | 0/2 [00:00<?, ?step/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac197fcb1cf84e0ba9ecce16baf326e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3407dca78d4f16b651ba9a780eed88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffe138380154d64812108e8f7781395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer:  50%|█████     | 1/2 [00:06<00:06,  6.78s/step]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22be6221286459b9f471144248fc710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f67c3cf8c041b6b5a2bd2178228319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356cceaf53534f2d889d70a27dd617f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer: 100%|██████████| 2/2 [00:07<00:00,  3.95s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "with tqdm(total=2, desc=\"Initializing Model and Tokenizer\", unit=\"step\") as pbar:\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # Automatically map the model to the available device\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    pbar.update(1)\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a48f159d-2f4c-4d7d-a063-d5ac0c034fae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Initialize the pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,  # Pass pre-loaded model\n",
    "    tokenizer=tokenizer,  # Pass pre-loaded tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b5fc33a-396f-46ad-b010-d2448449e0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating Summaries:   0%|          | 0/100 [00:00<?, ?notes/s]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:   2%|▏         | 2/100 [00:10<08:40,  5.31s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:   4%|▍         | 4/100 [00:19<07:33,  4.73s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:   6%|▌         | 6/100 [00:29<07:51,  5.01s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:   8%|▊         | 8/100 [00:36<06:40,  4.35s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  10%|█         | 10/100 [00:47<07:02,  4.70s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  12%|█▏        | 12/100 [00:55<06:42,  4.57s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  14%|█▍        | 14/100 [01:02<05:58,  4.16s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  16%|█▌        | 16/100 [01:13<06:19,  4.52s/notes]\u001b[A\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "\n",
      "Generating Summaries:  18%|█▊        | 18/100 [01:23<06:30,  4.77s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  20%|██        | 20/100 [01:34<06:34,  4.94s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  22%|██▏       | 22/100 [01:41<05:56,  4.58s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  24%|██▍       | 24/100 [01:52<06:03,  4.78s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  26%|██▌       | 26/100 [02:00<05:34,  4.52s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  28%|██▊       | 28/100 [02:10<05:41,  4.75s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  30%|███       | 30/100 [02:20<05:29,  4.71s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  32%|███▏      | 32/100 [02:30<05:30,  4.86s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  34%|███▍      | 34/100 [02:38<05:06,  4.64s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  36%|███▌      | 36/100 [02:49<05:08,  4.82s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  38%|███▊      | 38/100 [02:55<04:29,  4.34s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  40%|████      | 40/100 [03:02<04:01,  4.03s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  42%|████▏     | 42/100 [03:10<03:54,  4.04s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  44%|████▍     | 44/100 [03:19<03:51,  4.13s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  46%|████▌     | 46/100 [03:28<03:54,  4.34s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  48%|████▊     | 48/100 [03:38<03:51,  4.46s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  50%|█████     | 50/100 [03:48<03:55,  4.70s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  52%|█████▏    | 52/100 [03:55<03:29,  4.36s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  54%|█████▍    | 54/100 [04:04<03:18,  4.30s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  56%|█████▌    | 56/100 [04:10<02:55,  3.98s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  58%|█████▊    | 58/100 [04:21<03:05,  4.41s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  60%|██████    | 60/100 [04:29<02:48,  4.22s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  62%|██████▏   | 62/100 [04:39<02:52,  4.54s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  64%|██████▍   | 64/100 [04:45<02:28,  4.12s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  66%|██████▌   | 66/100 [04:52<02:13,  3.94s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  68%|██████▊   | 68/100 [04:59<01:57,  3.69s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  70%|███████   | 70/100 [05:09<02:04,  4.16s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  72%|███████▏  | 72/100 [05:19<02:04,  4.44s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  74%|███████▍  | 74/100 [05:30<02:02,  4.72s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  76%|███████▌  | 76/100 [05:38<01:47,  4.46s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  78%|███████▊  | 78/100 [05:44<01:29,  4.08s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  80%|████████  | 80/100 [05:48<01:07,  3.38s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  82%|████████▏ | 82/100 [05:57<01:07,  3.72s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  84%|████████▍ | 84/100 [06:05<01:00,  3.79s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  86%|████████▌ | 86/100 [06:13<00:54,  3.87s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  88%|████████▊ | 88/100 [06:20<00:45,  3.75s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  90%|█████████ | 90/100 [06:30<00:42,  4.23s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  92%|█████████▏| 92/100 [06:41<00:36,  4.61s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  94%|█████████▍| 94/100 [06:48<00:25,  4.25s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  96%|█████████▌| 96/100 [06:55<00:16,  4.05s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries:  98%|█████████▊| 98/100 [07:04<00:08,  4.16s/notes]\u001b[A\u001b[A\n",
      "\n",
      "Generating Summaries: 100%|██████████| 100/100 [07:15<00:00,  4.35s/notes]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Computational Efficiency Metrics:\n",
      "📌 Average Latency (Time per Summary): 4.3514 seconds\n",
      "📌 Average Throughput: 101.68 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------- Context Retrieval from Neo4j -------------------------\n",
    "def retrieve_context(patient_id, neo4j_graph, limit=2):\n",
    "    \"\"\"Retrieves patient context (problems, tests, treatments) from Neo4j\"\"\"\n",
    "    query = f\"\"\"\n",
    "    MATCH (p:Patient {{id: '{patient_id}'}})-[r]-(n)\n",
    "    WHERE type(r) IN ['HAS_PROBLEM', 'UNDERWENT_TEST', 'WAS_TREATED_WITH']\n",
    "    RETURN n.title AS node, type(r) AS relationship_type\n",
    "    \"\"\"\n",
    "    results = neo4j_graph.query(query)\n",
    "    \n",
    "    context = {\"Problems\": [], \"Treatments\": [], \"Tests\": []}\n",
    "    counts = {\"HAS_PROBLEM\": 0, \"WAS_TREATED_WITH\": 0, \"UNDERWENT_TEST\": 0} \n",
    "\n",
    "    for record in results:\n",
    "        relationship_type = record[\"relationship_type\"]\n",
    "        node = record[\"node\"]\n",
    "        if relationship_type == \"HAS_PROBLEM\" and counts[\"HAS_PROBLEM\"] < limit:\n",
    "            context[\"Problems\"].append(node)\n",
    "            counts[\"HAS_PROBLEM\"] += 1\n",
    "        elif relationship_type == \"WAS_TREATED_WITH\" and counts[\"WAS_TREATED_WITH\"] < limit:\n",
    "            context[\"Treatments\"].append(node)\n",
    "            counts[\"WAS_TREATED_WITH\"] += 1\n",
    "        elif relationship_type == \"UNDERWENT_TEST\" and counts[\"UNDERWENT_TEST\"] < limit:\n",
    "            context[\"Tests\"].append(node)\n",
    "            counts[\"UNDERWENT_TEST\"] += 1\n",
    "        if all(count == limit for count in counts.values()):\n",
    "            break\n",
    "\n",
    "    return context\n",
    "\n",
    "# ------------------------- Few-Shot Prompt Construction -------------------------\n",
    "def construct_few_shot_prompt(reduced_text, context):\n",
    "    def safe_join(items):\n",
    "        return \", \".join(str(item) for item in items if item is not None) if items else \"None\"\n",
    "\n",
    "    prompt = f\"\"\"You are a WORLD-CLASS EXPERT AT WRITING CLINICAL SUMMARIES. Summarize the input text in a clear, cohesive, and medically accurate manner. Do not include the input & prompt in the final summary.\n",
    "\n",
    "### **Patient Data for Summarization**\n",
    "**Input:**  \n",
    "{reduced_text}\n",
    "\n",
    "**Context:**  \n",
    "- Problems: {safe_join(context['Problems'])}  \n",
    "- Tests: {safe_join(context['Tests'])}  \n",
    "- Treatments: {safe_join(context['Treatments'])}  \n",
    "\n",
    "---\n",
    "### **Generated Summary:**  \n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 20,\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "def generate_summaries(df, neo4j_graph, summarizer, tokenizer, generation_params, batch_size=2):\n",
    "    generated_summaries = []\n",
    "    latency_list = []  # Time per summary\n",
    "    throughput_list = []  # Tokens processed per second\n",
    "    main_progress = tqdm(total=len(df), desc=\"Generating Summaries\", unit=\"notes\")\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        prompts = []\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            patient_id = row[\"id\"]\n",
    "            reduced_text = row[\"reduced_text\"]\n",
    "            context = retrieve_context(patient_id, neo4j_graph)\n",
    "            prompt = construct_few_shot_prompt(reduced_text, context)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "        summaries = summarizer(prompts, **generation_params)\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        latency_list.append((batch_end_time - batch_start_time) / len(batch))\n",
    "        throughput_list.append(batch_input_tokens / (batch_end_time - batch_start_time))\n",
    "\n",
    "        # Handle different formats of summaries\n",
    "        for summary in summaries:\n",
    "            # Check if the summary is a list of dictionaries (common in batch processing)\n",
    "            if isinstance(summary, list) and len(summary) > 0 and isinstance(summary[0], dict):\n",
    "                generated_summary = summary[0].get(\"generated_text\", \"\")\n",
    "            elif isinstance(summary, dict):  # Single dictionary output\n",
    "                generated_summary = summary.get(\"generated_text\", \"\")\n",
    "            else:\n",
    "                generated_summary = str(summary)  # Convert to string if unexpected format\n",
    "            \n",
    "            generated_summaries.append(generated_summary)\n",
    "\n",
    "        main_progress.update(len(batch))\n",
    "\n",
    "    main_progress.close()\n",
    "    df[\"generated_summary\"] = generated_summaries\n",
    "    print(\"\\n🔹 Computational Efficiency Metrics:\")\n",
    "    print(f\"📌 Average Latency (Time per Summary): {sum(latency_list) / len(latency_list):.4f} seconds\")\n",
    "    print(f\"📌 Average Throughput: {sum(throughput_list) / len(throughput_list):.2f} tokens/second\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------- Execution -------------------------\n",
    "# Assuming merged_df contains patient data\n",
    "NER_df = generate_summaries(NER_df, neo4j_graph, summarizer, tokenizer, generation_params)\n",
    "\n",
    "# ✅ Save results to CSV if needed\n",
    "NER_df.to_csv(\"summarized_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9120f9c-bbe3-4ac1-952b-1ae838bf51d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64143a9-8568-42e2-a4c2-8707c1820860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53633425-642b-48df-a950-ea81d76b462e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e40b1c-9a55-43f4-91a3-720c16d4d4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570ac19-8c59-48a8-b9f8-ae760bd591ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3710ebf-d246-463e-a624-23f06e5effb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BitsAndBytesConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpandable_segments:True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Configure quantization (8-bit)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m(\n\u001b[1;32m      6\u001b[0m     load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     llm_int8_enable_fp32_cpu_offload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment setup and quantization configuration done.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BitsAndBytesConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configure quantization (8-bit)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "print(\"Environment setup and quantization configuration done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a377856-874c-48fc-a542-1d0f50edbf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer:   0%|          | 0/2 [00:00<?, ?step/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quantization_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing Model and Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      6\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m         model_name,\n\u001b[0;32m----> 9\u001b[0m         quantization_config\u001b[38;5;241m=\u001b[39m\u001b[43mquantization_config\u001b[49m,\n\u001b[1;32m     10\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m#output_attentions=True,  # Enable attention outputs for AGTD\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m#return_dict_in_generate=True  # Ensures attention outputs are generated\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quantization_config' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "with tqdm(total=2, desc=\"Initializing Model and Tokenizer\", unit=\"step\") as pbar:\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        #output_attentions=True,  # Enable attention outputs for AGTD\n",
    "        #return_dict_in_generate=True  # Ensures attention outputs are generated\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    pbar.update(1)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e1b83384-d121-4fd2-9188-13db75bbf709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Initializing Model and Tokenizer:   0%|          | 0/2 [00:00<?, ?step/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Initializing Model and Tokenizer:  50%|█████     | 1/2 [00:01<00:01,  1.85s/step]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Initializing Model and Tokenizer: 100%|██████████| 2/2 [00:02<00:00,  1.24s/step]\u001b[A\u001b[A\u001b[A\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n",
      "Pipeline and generation parameters are ready.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model name\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load the model with quantization\n",
    "print(\"Loading model and tokenizer...\")\n",
    "with tqdm(total=2, desc=\"Initializing Model and Tokenizer\", unit=\"step\") as pbar:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,  # Quantization applied here\n",
    "        device_map=\"auto\",  # Use `accelerate` to manage device allocation\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  # Avoid padding errors\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "# Initialize the pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,  # Pass pre-loaded model\n",
    "    tokenizer=tokenizer,  # Pass pre-loaded tokenizer\n",
    ")\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 100,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "print(\"Pipeline and generation parameters are ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71425d6-950c-4aa1-82b7-2cc6182a4977",
   "metadata": {},
   "source": [
    "### Construct Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1416acb9-5a11-405b-8020-6f25df14d82e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context from KG with limits\n",
    "def retrieve_context(patient_id, neo4j_graph, limit=2):\n",
    "    query = f\"\"\"\n",
    "    MATCH (p:Patient {{id: '{patient_id}'}})-[r]-(n)\n",
    "    WHERE type(r) IN ['HAS_PROBLEM', 'UNDERWENT_TEST', 'WAS_TREATED_WITH']\n",
    "    RETURN n.title AS node, type(r) AS relationship_type\n",
    "    \"\"\"\n",
    "    results = neo4j_graph.query(query)\n",
    "    context = {\"Problems\": [], \"Treatments\": [], \"Tests\": []}\n",
    "    counts = {\"HAS_PROBLEM\": 0, \"WAS_TREATED_WITH\": 0, \"UNDERWENT_TEST\": 0}  # Initialize counts\n",
    "\n",
    "    for record in results:\n",
    "        relationship_type = record[\"relationship_type\"]\n",
    "        node = record[\"node\"]\n",
    "\n",
    "        # Check limits for each category and append if within limit\n",
    "        if relationship_type == \"HAS_PROBLEM\" and counts[\"HAS_PROBLEM\"] < limit:\n",
    "            context[\"Problems\"].append(node)\n",
    "            counts[\"HAS_PROBLEM\"] += 1\n",
    "        elif relationship_type == \"WAS_TREATED_WITH\" and counts[\"WAS_TREATED_WITH\"] < limit:\n",
    "            context[\"Treatments\"].append(node)\n",
    "            counts[\"WAS_TREATED_WITH\"] += 1\n",
    "        elif relationship_type == \"UNDERWENT_TEST\" and counts[\"UNDERWENT_TEST\"] < limit:\n",
    "            context[\"Tests\"].append(node)\n",
    "            counts[\"UNDERWENT_TEST\"] += 1\n",
    "\n",
    "        # Break early if all categories reach their limits\n",
    "        if all(count == limit for count in counts.values()):\n",
    "            break\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "# Function to construct a zero-shot prompt\n",
    "def construct_zero_shot_prompt(reduced_text, context):\n",
    "    prompt = f\"\"\"\n",
    "    ### Task ###\n",
    "    You are a highly knowledgeable and best medical expert working at NIH. Your task is to summarize clinical notes concisely and professionally using only the information provided in the Input and Context.\n",
    "    - Use a clear, organized structure.\n",
    "    - Summarize in a narrative storytelling tone.\n",
    "    - Avoid technical jargon when unnecessary, and focus on readability.\n",
    "    - Ensure the summary is factual, concise, and consistent with the provided data.\n",
    "\n",
    "    ### Task ###\n",
    "    Summarize the following patient note concisely and professionally in a clear and organized manner:\n",
    "    Input: {reduced_text}\n",
    "    Context: Problems: {context['Problems']}, Treatments: {context['Treatments']}, Tests: {context['Tests']}\n",
    "    Generated Summary:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Function to parse generated summaries\n",
    "def parse_summary_output(output):\n",
    "    if isinstance(output, dict) and \"generated_text\" in output:\n",
    "        return output[\"generated_text\"].split(\"Output:\")[-1].strip()\n",
    "    elif isinstance(output, str):\n",
    "        return output.split(\"Output:\")[-1].strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Helper functions defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b68fb9-10cb-4c3c-b9b5-956fabc09fe6",
   "metadata": {},
   "source": [
    "### three shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "93c1b7f4-9dcc-40e0-88a4-d1ac6a2467eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context from KG with limits\n",
    "def retrieve_context(patient_id, neo4j_graph, limit=2):\n",
    "    query = f\"\"\"\n",
    "    MATCH (p:Patient {{id: '{patient_id}'}})-[r]-(n)\n",
    "    WHERE type(r) IN ['HAS_PROBLEM', 'UNDERWENT_TEST', 'WAS_TREATED_WITH']\n",
    "    RETURN n.title AS node, type(r) AS relationship_type\n",
    "    \"\"\"\n",
    "    results = neo4j_graph.query(query)\n",
    "    context = {\"Problems\": [], \"Treatments\": [], \"Tests\": []}\n",
    "    counts = {\"HAS_PROBLEM\": 0, \"WAS_TREATED_WITH\": 0, \"UNDERWENT_TEST\": 0}  # Initialize counts\n",
    "\n",
    "    for record in results:\n",
    "        relationship_type = record[\"relationship_type\"]\n",
    "        node = record[\"node\"]\n",
    "\n",
    "        # Check limits for each category and append if within limit\n",
    "        if relationship_type == \"HAS_PROBLEM\" and counts[\"HAS_PROBLEM\"] < limit:\n",
    "            context[\"Problems\"].append(node)\n",
    "            counts[\"HAS_PROBLEM\"] += 1\n",
    "        elif relationship_type == \"WAS_TREATED_WITH\" and counts[\"WAS_TREATED_WITH\"] < limit:\n",
    "            context[\"Treatments\"].append(node)\n",
    "            counts[\"WAS_TREATED_WITH\"] += 1\n",
    "        elif relationship_type == \"UNDERWENT_TEST\" and counts[\"UNDERWENT_TEST\"] < limit:\n",
    "            context[\"Tests\"].append(node)\n",
    "            counts[\"UNDERWENT_TEST\"] += 1\n",
    "\n",
    "        # Break early if all categories reach their limits\n",
    "        if all(count == limit for count in counts.values()):\n",
    "            break\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "def construct_three_shot_prompt(reduced_text, context):\n",
    "    prompt = f\"\"\"\n",
    "    ### Task ###\n",
    "    You are a highly knowledgeable medical expert working at NIH. Your task is to summarize clinical notes concisely and professionally using only the information provided in the Input and Context. \n",
    "    - Do not include details or assumptions not explicitly mentioned.\n",
    "    - Do not add, infer, or guess any additional information.\n",
    "    - Ensure the summary is factual and consistent with the provided data.\n",
    "\n",
    "    ### Example 1 ###\n",
    "    Input: <SEX> F <CHIEF COMPLAINT> RUQ pain, nausea, vomiting\n",
    "    Context: Problems: [Acute cholecystitis, Obesity], Treatments: [Open cholecystectomy]\n",
    "    Output: Patient underwent cholecystectomy for acute cholecystitis and obesity.\n",
    "\n",
    "    ### Example 2 ###\n",
    "    Input: <SEX> M <CHIEF COMPLAINT> Severe chest pain, left arm pain\n",
    "    Context: Problems: [Myocardial infarction], Treatments: [PCI, Antiplatelet therapy]\n",
    "    Output: Patient had a myocardial infarction treated with PCI and therapy.\n",
    "\n",
    "    ### Example 3 ###\n",
    "    Input: <SEX> F <CHIEF COMPLAINT> Frequent urination, excessive thirst\n",
    "    Context: Problems: [Type 2 diabetes], Treatments: [Insulin therapy]\n",
    "    Output: Patient diagnosed with diabetes and started on insulin therapy.\n",
    "\n",
    "    ### Task ###\n",
    "    Summarize the following patient note concisely and professionally:\n",
    "    Input: {reduced_text}\n",
    "    Context: Problems: {context['Problems']}, Treatments: {context['Treatments']}, Tests: {context['Tests']}\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Function to parse generated summaries\n",
    "def parse_summary_output(output):\n",
    "    if isinstance(output, dict) and \"generated_text\" in output:\n",
    "        return output[\"generated_text\"].split(\"Output:\")[-1].strip()\n",
    "    elif isinstance(output, str):\n",
    "        return output.split(\"Output:\")[-1].strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "219b4773-1bdb-4fb6-91cd-da5d328a4f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context from KG with limits\n",
    "def retrieve_context(patient_id, neo4j_graph, limit=10):\n",
    "    query = f\"\"\"\n",
    "    MATCH (p:Patient {{id: '{patient_id}'}})-[r]-(n)\n",
    "    WHERE type(r) IN ['HAS_PROBLEM', 'UNDERWENT_TEST', 'WAS_TREATED_WITH']\n",
    "    RETURN n.title AS node, type(r) AS relationship_type\n",
    "    \"\"\"\n",
    "    results = neo4j_graph.query(query)\n",
    "    context = {\"Problems\": [], \"Treatments\": [], \"Tests\": []}\n",
    "    counts = {\"HAS_PROBLEM\": 0, \"WAS_TREATED_WITH\": 0, \"UNDERWENT_TEST\": 0}  # Initialize counts\n",
    "\n",
    "    for record in results:\n",
    "        relationship_type = record[\"relationship_type\"]\n",
    "        node = record[\"node\"]\n",
    "\n",
    "        # Check limits for each category and append if within limit\n",
    "        if relationship_type == \"HAS_PROBLEM\" and counts[\"HAS_PROBLEM\"] < limit:\n",
    "            context[\"Problems\"].append(node)\n",
    "            counts[\"HAS_PROBLEM\"] += 1\n",
    "        elif relationship_type == \"WAS_TREATED_WITH\" and counts[\"WAS_TREATED_WITH\"] < limit:\n",
    "            context[\"Treatments\"].append(node)\n",
    "            counts[\"WAS_TREATED_WITH\"] += 1\n",
    "        elif relationship_type == \"UNDERWENT_TEST\" and counts[\"UNDERWENT_TEST\"] < limit:\n",
    "            context[\"Tests\"].append(node)\n",
    "            counts[\"UNDERWENT_TEST\"] += 1\n",
    "\n",
    "        # Break early if all categories reach their limits\n",
    "        if all(count == limit for count in counts.values()):\n",
    "            break\n",
    "\n",
    "    return context\n",
    "\n",
    "def construct_few_shot_prompt(reduced_text, context):\n",
    "    prompt = f\"\"\"You are a WORLD-CLASS EXPERT AT WRITING CLINICAL SUMMARIES. Summarize the input text in a clear, cohesive, and medically accurate manner.\n",
    "\n",
    "### Example 1:\n",
    "**Input:**  \n",
    "<SEX> M <AGE> 67 <SERVICE> CARDIOLOGY <HISTORY> Patient with known CAD and diabetes presented with chest pain and shortness of breath. ECG showed ST elevations in V2-V4. Troponin elevated at 2.3.  \n",
    "**Context:**  \n",
    "Problems: [Acute MI, CAD, Diabetes]  \n",
    "Tests: [ECG, Troponin]  \n",
    "Treatments: [PCI, Anticoagulation]  \n",
    "**Summary:**  \n",
    "67-year-old male with CAD and diabetes presented with chest pain and dyspnea. ECG revealed ST elevations V2-V4 with elevated troponin 2.3, indicating acute MI. Patient underwent PCI and started on anticoagulation.\n",
    "\n",
    "### Example 2:\n",
    "**Input:**  \n",
    "<SEX> F <AGE> 58 <SERVICE> NEUROLOGY <HISTORY> Patient with history of stroke presented with sudden onset left-sided weakness and slurred speech. CT head negative for hemorrhage. tPA administered within window.  \n",
    "**Context:**  \n",
    "Problems: [Acute Ischemic Stroke, Prior CVA]  \n",
    "Tests: [CT Head]  \n",
    "Treatments: [tPA]  \n",
    "**Summary:**  \n",
    "58-year-old female with history of stroke presented with acute left-sided weakness and slurred speech. CT head excluded hemorrhage, and patient received tPA within therapeutic window.\n",
    "\n",
    "---\n",
    "\n",
    "### YOUR TASK:\n",
    "1️⃣ Focus **ONLY on documented findings** – NO assumptions  \n",
    "2️⃣ Include **ALL relevant clinical data** from input  \n",
    "3️⃣ Maintain **chronological order** of events  \n",
    "4️⃣ Use **precise medical terminology**  \n",
    "5️⃣ Keep summary **concise but complete**  \n",
    "\n",
    "---\n",
    "### **Patient Data for Summarization**\n",
    "**Input:**  \n",
    "{reduced_text}\n",
    "\n",
    "**Context:**  \n",
    "- Problems: {\", \".join(context['Problems']) if context['Problems'] else \"None\"}  \n",
    "- Tests: {\", \".join(context['Tests']) if context['Tests'] else \"None\"}  \n",
    "- Treatments: {\", \".join(context['Treatments']) if context['Treatments'] else \"None\"}  \n",
    "\n",
    "---\n",
    "**Generated Summary:**  \n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse generated summaries\n",
    "def parse_summary_output(output):\n",
    "    if isinstance(output, dict) and \"generated_text\" in output:\n",
    "        return output[\"generated_text\"].split(\"Output:\")[-1].strip()\n",
    "    elif isinstance(output, str):\n",
    "        return output.split(\"Output:\")[-1].strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13a75d6e-a111-4b7d-a275-b67fa935c0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context from KG with limits\n",
    "def retrieve_context(patient_id, neo4j_graph, limit=1):\n",
    "    query = f\"\"\"\n",
    "    MATCH (p:Patient {{id: '{patient_id}'}})-[r]-(n)\n",
    "    WHERE type(r) IN ['HAS_PROBLEM', 'UNDERWENT_TEST', 'WAS_TREATED_WITH']\n",
    "    RETURN n.title AS node, type(r) AS relationship_type\n",
    "    \"\"\"\n",
    "    results = neo4j_graph.query(query)\n",
    "    context = {\"Problems\": [], \"Treatments\": [], \"Tests\": []}\n",
    "    counts = {\"HAS_PROBLEM\": 0, \"WAS_TREATED_WITH\": 0, \"UNDERWENT_TEST\": 0}  # Initialize counts\n",
    "\n",
    "    for record in results:\n",
    "        relationship_type = record[\"relationship_type\"]\n",
    "        node = record[\"node\"]\n",
    "\n",
    "        # Check limits for each category and append if within limit\n",
    "        if relationship_type == \"HAS_PROBLEM\" and counts[\"HAS_PROBLEM\"] < limit:\n",
    "            context[\"Problems\"].append(node)\n",
    "            counts[\"HAS_PROBLEM\"] += 1\n",
    "        elif relationship_type == \"WAS_TREATED_WITH\" and counts[\"WAS_TREATED_WITH\"] < limit:\n",
    "            context[\"Treatments\"].append(node)\n",
    "            counts[\"WAS_TREATED_WITH\"] += 1\n",
    "        elif relationship_type == \"UNDERWENT_TEST\" and counts[\"UNDERWENT_TEST\"] < limit:\n",
    "            context[\"Tests\"].append(node)\n",
    "            counts[\"UNDERWENT_TEST\"] += 1\n",
    "\n",
    "        # Break early if all categories reach their limits\n",
    "        if all(count == limit for count in counts.values()):\n",
    "            break\n",
    "\n",
    "    return context\n",
    "\n",
    "def construct_one_shot_prompt(reduced_text, context):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following clinical note concisely while preserving key medical details. Only generate the summary—do not repeat the input text, instructions, or formatting.\n",
    "\n",
    "    Clinical Note:\n",
    "    {reduced_text}\n",
    "\n",
    "    Patient Context:\n",
    "    - Problems: {\", \".join(context['Problems']) if context['Problems'] else \"None\"}\n",
    "    - Tests: {\", \".join(context['Tests']) if context['Tests'] else \"None\"}\n",
    "    - Treatments: {\", \".join(context['Treatments']) if context['Treatments'] else \"None\"}\n",
    "\n",
    "    ### Summary of the Patient's Condition:\n",
    "    \"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "# Function to parse generated summaries\n",
    "def parse_summary_output(output):\n",
    "    if isinstance(output, dict) and \"generated_text\" in output:\n",
    "        return output[\"generated_text\"].split(\"Output:\")[-1].strip()\n",
    "    elif isinstance(output, str):\n",
    "        return output.split(\"Output:\")[-1].strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f75e8439-fc5b-4ec7-8991-e8048d0c43c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Generating Summaries:   0%|          | 0/100 [00:00<?, ?notes/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:   2%|▏         | 2/100 [00:37<30:21, 18.58s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:   4%|▍         | 4/100 [01:15<30:07, 18.83s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:   6%|▌         | 6/100 [01:53<29:39, 18.93s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:   8%|▊         | 8/100 [02:31<29:06, 18.98s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  10%|█         | 10/100 [03:09<28:24, 18.94s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  12%|█▏        | 12/100 [03:40<26:11, 17.86s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  14%|█▍        | 14/100 [04:17<25:50, 18.03s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  16%|█▌        | 16/100 [04:55<25:42, 18.36s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  18%|█▊        | 18/100 [05:33<25:26, 18.61s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  20%|██        | 20/100 [06:08<24:17, 18.22s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  22%|██▏       | 22/100 [06:46<23:59, 18.45s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  24%|██▍       | 24/100 [07:23<23:20, 18.43s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  26%|██▌       | 26/100 [07:55<21:54, 17.77s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  28%|██▊       | 28/100 [08:26<20:32, 17.11s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  30%|███       | 30/100 [09:05<20:38, 17.70s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  32%|███▏      | 32/100 [09:43<20:32, 18.12s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  34%|███▍      | 34/100 [10:20<20:08, 18.31s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  36%|███▌      | 36/100 [10:58<19:46, 18.53s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  38%|███▊      | 38/100 [11:30<18:20, 17.75s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  40%|████      | 40/100 [12:08<18:08, 18.15s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  42%|████▏     | 42/100 [12:37<16:25, 17.00s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  44%|████▍     | 44/100 [13:09<15:31, 16.64s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  46%|████▌     | 46/100 [13:47<15:41, 17.43s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  48%|████▊     | 48/100 [14:20<14:50, 17.12s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  50%|█████     | 50/100 [14:53<14:04, 16.89s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  52%|█████▏    | 52/100 [15:23<13:04, 16.35s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  54%|█████▍    | 54/100 [15:56<12:32, 16.37s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  56%|█████▌    | 56/100 [16:34<12:35, 17.17s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  58%|█████▊    | 58/100 [17:12<12:25, 17.76s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  60%|██████    | 60/100 [17:35<10:36, 15.91s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  62%|██████▏   | 62/100 [18:07<10:06, 15.97s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  64%|██████▍   | 64/100 [18:45<10:04, 16.79s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  66%|██████▌   | 66/100 [19:16<09:18, 16.43s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  68%|██████▊   | 68/100 [19:54<09:08, 17.14s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  70%|███████   | 70/100 [20:30<08:42, 17.41s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  72%|███████▏  | 72/100 [21:05<08:09, 17.47s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  74%|███████▍  | 74/100 [21:43<07:45, 17.90s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  76%|███████▌  | 76/100 [22:15<06:56, 17.36s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  78%|███████▊  | 78/100 [22:50<06:22, 17.40s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  80%|████████  | 80/100 [23:28<05:59, 17.96s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  82%|████████▏ | 82/100 [24:06<05:27, 18.22s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  84%|████████▍ | 84/100 [24:44<04:55, 18.47s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  86%|████████▌ | 86/100 [25:17<04:10, 17.88s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  88%|████████▊ | 88/100 [25:51<03:31, 17.62s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  90%|█████████ | 90/100 [26:29<03:00, 18.02s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  92%|█████████▏| 92/100 [27:08<02:27, 18.39s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  94%|█████████▍| 94/100 [27:45<01:50, 18.49s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  96%|█████████▌| 96/100 [28:23<01:14, 18.68s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries:  98%|█████████▊| 98/100 [29:02<00:37, 18.81s/notes]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating Summaries: 100%|██████████| 100/100 [29:39<00:00, 17.80s/notes]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Computational Efficiency Metrics:\n",
      "📌 Total Input Tokens: 138888\n",
      "📌 Total Output Tokens: 28078\n",
      "📌 Total Time Spent: 1779.75 seconds\n",
      "📌 Average Latency (Time per Summary): 17.7975 seconds\n",
      "📌 Average TTFT (Time to First Token): 35434.25 ms\n",
      "📌 Average Throughput: 94.61 tokens/second\n",
      "📌 Token Efficiency (TE): 0.2022\n",
      "✅ Summarization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------- Context Retrieval from Neo4j -------------------------\n",
    "def retrieve_context(patient_id, neo4j_graph, limit=2):\n",
    "    \"\"\"Retrieves patient context (problems, tests, treatments) from Neo4j\"\"\"\n",
    "    query = f\"\"\"\n",
    "    MATCH (p:Patient {{id: '{patient_id}'}})-[r]-(n)\n",
    "    WHERE type(r) IN ['HAS_PROBLEM', 'UNDERWENT_TEST', 'WAS_TREATED_WITH']\n",
    "    RETURN n.title AS node, type(r) AS relationship_type\n",
    "    \"\"\"\n",
    "    results = neo4j_graph.query(query)\n",
    "    \n",
    "    # Initialize context dictionary\n",
    "    context = {\"Problems\": [], \"Treatments\": [], \"Tests\": []}\n",
    "    counts = {\"HAS_PROBLEM\": 0, \"WAS_TREATED_WITH\": 0, \"UNDERWENT_TEST\": 0} \n",
    "\n",
    "    for record in results:\n",
    "        relationship_type = record[\"relationship_type\"]\n",
    "        node = record[\"node\"]\n",
    "\n",
    "        # Store retrieved context within limit\n",
    "        if relationship_type == \"HAS_PROBLEM\" and counts[\"HAS_PROBLEM\"] < limit:\n",
    "            context[\"Problems\"].append(node)\n",
    "            counts[\"HAS_PROBLEM\"] += 1\n",
    "        elif relationship_type == \"WAS_TREATED_WITH\" and counts[\"WAS_TREATED_WITH\"] < limit:\n",
    "            context[\"Treatments\"].append(node)\n",
    "            counts[\"WAS_TREATED_WITH\"] += 1\n",
    "        elif relationship_type == \"UNDERWENT_TEST\" and counts[\"UNDERWENT_TEST\"] < limit:\n",
    "            context[\"Tests\"].append(node)\n",
    "            counts[\"UNDERWENT_TEST\"] += 1\n",
    "\n",
    "        # Stop early if all categories reach the limit\n",
    "        if all(count == limit for count in counts.values()):\n",
    "            break\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "# ------------------------- Few-Shot Prompt Construction -------------------------\n",
    "def construct_few_shot_prompt(reduced_text, context):\n",
    "    \"\"\"Creates a structured few-shot prompt for summarization.\"\"\"\n",
    "    \n",
    "    def safe_join(items):\n",
    "        \"\"\"Helper function to safely join list items, converting None to empty strings.\"\"\"\n",
    "        return \", \".join(str(item) for item in items if item is not None) if items else \"None\"\n",
    "\n",
    "    prompt = f\"\"\"You are a WORLD-CLASS EXPERT AT WRITING CLINICAL SUMMARIES. Summarize the input text in a clear, cohesive, and medically accurate manner. Do not include the input & prompt in the final summary.\n",
    "\n",
    "### **Patient Data for Summarization**\n",
    "**Input:**  \n",
    "{reduced_text}\n",
    "\n",
    "**Context:**  \n",
    "- Problems: {safe_join(context['Problems'])}  \n",
    "- Tests: {safe_join(context['Tests'])}  \n",
    "- Treatments: {safe_join(context['Treatments'])}  \n",
    "\n",
    "---\n",
    "### **Generated Summary:**  \n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# ------------------------- Batch Summarization with Efficiency Metrics -------------------------\n",
    "def generate_summaries(df, neo4j_graph, summarizer, tokenizer, generation_params, batch_size=2):\n",
    "    \"\"\"Generates summaries for all patient records while tracking efficiency metrics.\"\"\"\n",
    "    generated_summaries = []\n",
    "\n",
    "    # ✅ Initialize Metrics\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    total_time_spent = 0\n",
    "    ttft_list = []  # Time to first token\n",
    "    latency_list = []  # Time per summary\n",
    "    throughput_list = []  # Tokens processed per second\n",
    "\n",
    "    # ✅ Progress bar\n",
    "    main_progress = tqdm(total=len(df), desc=\"Generating Summaries\", unit=\"notes\")\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        prompts = []\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Construct prompts with retrieved context\n",
    "        for _, row in batch.iterrows():\n",
    "            patient_id = row[\"note_id\"]\n",
    "            reduced_text = row[\"reduced_text\"]\n",
    "            \n",
    "            # Retrieve context from KG\n",
    "            context = retrieve_context(patient_id, neo4j_graph)\n",
    "            \n",
    "            # Construct the few-shot prompt\n",
    "            prompt = construct_few_shot_prompt(reduced_text, context)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        try:\n",
    "            # ✅ Tokenization for input size tracking\n",
    "            batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "            # ✅ Generate summaries\n",
    "            single_start_time = time.time()\n",
    "            summaries = summarizer(prompts, **generation_params)\n",
    "            single_end_time = time.time()\n",
    "\n",
    "            # ✅ Measure TTFT\n",
    "            ttft_list.append((single_end_time - single_start_time) * 1000)  # Convert to milliseconds\n",
    "\n",
    "            # ✅ Parse and store summaries\n",
    "            batch_output_tokens = 0\n",
    "            for summary in summaries:\n",
    "                if isinstance(summary, list) and len(summary) > 0:  # Handles nested lists\n",
    "                    summary = summary[0]  # Extract first item if it's a nested list\n",
    "                \n",
    "                if isinstance(summary, dict) and \"generated_text\" in summary:\n",
    "                    generated_text = summary[\"generated_text\"]\n",
    "                else:\n",
    "                    generated_text = str(summary)  # Convert to string if unexpected format\n",
    "                \n",
    "                # ✅ Extract only the summary part after \"### **Generated Summary:**\"\n",
    "                summary_start = generated_text.find(\"### **Generated Summary:**\")\n",
    "                if summary_start != -1:\n",
    "                    generated_summary = generated_text[summary_start + len(\"### **Generated Summary:**\"):].strip()\n",
    "                else:\n",
    "                    generated_summary = generated_text.strip()  # Fallback in case parsing fails\n",
    "\n",
    "                generated_summaries.append(generated_summary)\n",
    "                batch_output_tokens += len(tokenizer.encode(generated_summary))\n",
    "\n",
    "            batch_end_time = time.time()\n",
    "\n",
    "            # ✅ Compute batch metrics\n",
    "            batch_latency = batch_end_time - batch_start_time  # Total batch time\n",
    "            latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "            throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "            # ✅ Update global metrics\n",
    "            total_input_tokens += batch_input_tokens\n",
    "            total_output_tokens += batch_output_tokens\n",
    "            total_time_spent += batch_latency\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing batch starting at index {i}: {e}\")\n",
    "            generated_summaries.extend([\"Error generating summary\"] * len(prompts))  # Fill missing entries\n",
    "\n",
    "        # ✅ Update progress bar\n",
    "        main_progress.update(len(batch))\n",
    "\n",
    "    # ✅ Close progress bar\n",
    "    main_progress.close()\n",
    "\n",
    "    # ✅ Store summaries in dataframe\n",
    "    df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "    # ✅ Compute Final Metrics\n",
    "    average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "    average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0  # Now in milliseconds\n",
    "    average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "    token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "    # ✅ Print Final Metrics (Only Once)\n",
    "    print(\"\\n🔹 Computational Efficiency Metrics:\")\n",
    "    print(f\"📌 Total Input Tokens: {total_input_tokens}\")\n",
    "    print(f\"📌 Total Output Tokens: {total_output_tokens}\")\n",
    "    print(f\"📌 Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "    print(f\"📌 Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "    print(f\"📌 Average TTFT (Time to First Token): {average_ttft:.2f} ms\")  # Display in milliseconds\n",
    "    print(f\"📌 Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "    print(f\"📌 Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "    print(\"✅ Summarization complete!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------- Execution -------------------------\n",
    "# Assuming merged_df contains patient data\n",
    "merged_df = generate_summaries(merged_df, neo4j_graph, summarizer, tokenizer, generation_params)\n",
    "\n",
    "# ✅ Save results to CSV if needed\n",
    "# merged_df.to_csv(\"summarized_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee79ab61-dcb7-4641-a38a-854bb279423d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reduced_text</th>\n",
       "      <th>entities</th>\n",
       "      <th>problems</th>\n",
       "      <th>treatments</th>\n",
       "      <th>tests</th>\n",
       "      <th>generated_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16002318-DS-17</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; Iodine /...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;SEX&gt; F &lt;SERVICE&gt; SURGERY &lt;AL...</td>\n",
       "      <td>{'PROBLEM': ['101', '7 pound weight loss', 'a ...</td>\n",
       "      <td>['101', '7 pound weight loss', 'a fever', 'a l...</td>\n",
       "      <td>['abdominal exercises', 'albuterol sulfate', '...</td>\n",
       "      <td>['b12', 'bmi', 'calcium', 'physical exam']</td>\n",
       "      <td>The patient is a female presenting with morbid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15638884-DS-4</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Augment...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;A...</td>\n",
       "      <td>{'PROBLEM': ['+', '-', '1 cm area', 'a \" cyst ...</td>\n",
       "      <td>['+', '-', '1 cm area', 'a \" cyst \"', 'a 2cm d...</td>\n",
       "      <td>['a bankart repair', 'a nicotine patch', 'a st...</td>\n",
       "      <td>['.', '_', 'a', 'a ct scan', 'absbaso', 'abseo...</td>\n",
       "      <td>The patient presented with painless jaundice a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12435705-DS-14</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; ibuprof...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;A...</td>\n",
       "      <td>{'PROBLEM': ['a 0. 7 x 0. 7 x 0. 7 cm simple c...</td>\n",
       "      <td>['a 0. 7 x 0. 7 x 0. 7 cm simple cyst', 'a 2. ...</td>\n",
       "      <td>['2', 'a prolonged course', 'ampicillin', 'ant...</td>\n",
       "      <td>['16s rdna primer set', 'aa', 'abl', 'acid fas...</td>\n",
       "      <td>The patient presents with a history of rupture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12413577-DS-4</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; OBSTETRICS/GYNECOLOGY &lt;ALLER...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;SEX&gt; F &lt;SERVICE&gt; OBSTETRICS/...</td>\n",
       "      <td>{'PROBLEM': ['a third - degree uterine prolaps...</td>\n",
       "      <td>['a third - degree uterine prolapse', 'abnorma...</td>\n",
       "      <td>['a stool softener', 'acetaminophen', 'admissi...</td>\n",
       "      <td>['hct', 'hgb', 'mch', 'mchc', 'mcv', 'nadr', '...</td>\n",
       "      <td>The patient presents with a history of uterine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17967161-DS-29</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; lisinopr...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;AL...</td>\n",
       "      <td>{'PROBLEM': ['101', 'abuse', 'acute pancreatit...</td>\n",
       "      <td>['101', 'abuse', 'acute pancreatitis', 'angina...</td>\n",
       "      <td>['a', 'a 3 mm x 40 mm balloon percutaneous tra...</td>\n",
       "      <td>['angap', 'blood', 'blood calcium', 'blood ck ...</td>\n",
       "      <td>Peripheral vascular disease, right foot ulcer,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          note_id                                              input  \\\n",
       "0  16002318-DS-17  <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...   \n",
       "1   15638884-DS-4  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Augment...   \n",
       "2  12435705-DS-14  <SEX> M <SERVICE> MEDICINE <ALLERGIES> ibuprof...   \n",
       "3   12413577-DS-4  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
       "4  17967161-DS-29  <SEX> M <SERVICE> SURGERY <ALLERGIES> lisinopr...   \n",
       "\n",
       "                                        reduced_text  \\\n",
       "0  <|begin_of_text|><SEX> F <SERVICE> SURGERY <AL...   \n",
       "1  <|begin_of_text|><SEX> M <SERVICE> MEDICINE <A...   \n",
       "2  <|begin_of_text|><SEX> M <SERVICE> MEDICINE <A...   \n",
       "3  <|begin_of_text|><SEX> F <SERVICE> OBSTETRICS/...   \n",
       "4  <|begin_of_text|><SEX> M <SERVICE> SURGERY <AL...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'PROBLEM': ['101', '7 pound weight loss', 'a ...   \n",
       "1  {'PROBLEM': ['+', '-', '1 cm area', 'a \" cyst ...   \n",
       "2  {'PROBLEM': ['a 0. 7 x 0. 7 x 0. 7 cm simple c...   \n",
       "3  {'PROBLEM': ['a third - degree uterine prolaps...   \n",
       "4  {'PROBLEM': ['101', 'abuse', 'acute pancreatit...   \n",
       "\n",
       "                                            problems  \\\n",
       "0  ['101', '7 pound weight loss', 'a fever', 'a l...   \n",
       "1  ['+', '-', '1 cm area', 'a \" cyst \"', 'a 2cm d...   \n",
       "2  ['a 0. 7 x 0. 7 x 0. 7 cm simple cyst', 'a 2. ...   \n",
       "3  ['a third - degree uterine prolapse', 'abnorma...   \n",
       "4  ['101', 'abuse', 'acute pancreatitis', 'angina...   \n",
       "\n",
       "                                          treatments  \\\n",
       "0  ['abdominal exercises', 'albuterol sulfate', '...   \n",
       "1  ['a bankart repair', 'a nicotine patch', 'a st...   \n",
       "2  ['2', 'a prolonged course', 'ampicillin', 'ant...   \n",
       "3  ['a stool softener', 'acetaminophen', 'admissi...   \n",
       "4  ['a', 'a 3 mm x 40 mm balloon percutaneous tra...   \n",
       "\n",
       "                                               tests  \\\n",
       "0         ['b12', 'bmi', 'calcium', 'physical exam']   \n",
       "1  ['.', '_', 'a', 'a ct scan', 'absbaso', 'abseo...   \n",
       "2  ['16s rdna primer set', 'aa', 'abl', 'acid fas...   \n",
       "3  ['hct', 'hgb', 'mch', 'mchc', 'mcv', 'nadr', '...   \n",
       "4  ['angap', 'blood', 'blood calcium', 'blood ck ...   \n",
       "\n",
       "                                   generated_summary  \n",
       "0  The patient is a female presenting with morbid...  \n",
       "1  The patient presented with painless jaundice a...  \n",
       "2  The patient presents with a history of rupture...  \n",
       "3  The patient presents with a history of uterine...  \n",
       "4  Peripheral vascular disease, right foot ulcer,...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3c2d120d-8c02-4b11-b63d-6e22d166c4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summaries saved to 'summarization_output.csv'\n"
     ]
    }
   ],
   "source": [
    "# ✅ Save the DataFrame to a CSV file\n",
    "merged_df.to_csv(\"summarization_output.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'summarization_output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b081e-f9de-4e96-8cea-b5b535b5277c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf29d86-9bb2-4ddc-9995-4de975b5fe66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d88c89-ca9e-44c6-b501-04ec23fe44e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f4f8b-f61d-409f-bf96-2872465d3a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17907712-b98b-42f1-8558-1161555bb8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20869c-a614-49b5-ba1f-8f22f53c99bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e22802-a29d-432a-af1c-bb4dd25a0353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
