{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0917618-2dd8-4857-aada-c25325a8b1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603f4fc3-4de8-423d-99a8-0c6aefa2a25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'mimicivliza'  # Replace with your bucket name\n",
    "mimic_iv_bhc = f's3://{bucket_name}/sample_data_100.csv'\n",
    "\n",
    "# Load the files\n",
    "mimic_iv_bhc_100 = pd.read_csv(mimic_iv_bhc)\n",
    "\n",
    "# Display the data\n",
    "#mimic_iv_bhc_100.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce8ea37-f6b4-4274-9345-b529891dc5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (100, 5)\n",
      "\n",
      "Columns: ['note_id', 'input', 'target', 'input_tokens', 'target_tokens']\n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame shape:\", mimic_iv_bhc_100.shape)\n",
    "print(\"\\nColumns:\", mimic_iv_bhc_100.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20627f7-dd51-433a-8d02-d25fdf107fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation of long text\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping of content\n",
    "\n",
    "#print(mimic_iv_bhc_100.iloc[9])  # Replace 9 with the desired row index\n",
    "  # Remember: Index starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14d45d7-d40c-4d26-9316-eef7e7b3d50e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54588208-3321-4f5b-a9d9-9fc93cc5813a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fab7e68-bc56-4e89-9e98-fa008e38d44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2f5bf-c97f-4946-9e6b-3f86dcc1a6cd",
   "metadata": {},
   "source": [
    "### Configuring 8-Bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae9017d-5be4-41c2-a179-c90fd635f430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup and quantization configuration done.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configure quantization (8-bit)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "print(\"Environment setup and quantization configuration done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "452cc74f-7d2c-42c2-8a9f-d072ce047ace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer: 100%|██████████| 2/2 [00:04<00:00,  2.26s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "with tqdm(total=2, desc=\"Initializing Model and Tokenizer\", unit=\"step\") as pbar:\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        output_attentions=True,  # Enable attention outputs for AGTD\n",
    "        return_dict_in_generate=True  # Ensures attention outputs are generated\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    pbar.update(1)\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "270764e1-12ad-4413-a0b3-2ceace756614",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization pipeline initialized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(\"Summarization pipeline initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e42d27e-60f8-4210-a376-8ef17833531a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine few-shot examples into a prompt template\n",
    "\n",
    "\n",
    "def construct_few_shot_prompt(row_input):\n",
    "    \"\"\"\n",
    "    Constructs a few-shot prompt dynamically using the row input.\n",
    "\n",
    "    Args:\n",
    "    - row_input (str): The input text from the dataframe row.\n",
    "\n",
    "    Returns:\n",
    "    - str: The constructed prompt for the model.\n",
    "    \"\"\"\n",
    "    prompt = \"You are a medical expert. Please summarize the following input concisely:\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {row_input}\\nSummary:\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc277d6-fa56-4527-8790-d16d402a5203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Optional\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "class AGTDSummarizer:\n",
    "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    def calculate_importance(\n",
    "        self, \n",
    "        tokens: torch.Tensor, \n",
    "        alpha: float = 0.5\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate token importance scores using attention weights and positional bias.\n",
    "        \n",
    "        Args:\n",
    "            tokens: Input token ids.\n",
    "            alpha: Weight factor for layer importance.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of importance scores for each token.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(tokens, output_attentions=True)\n",
    "                attentions = outputs.attentions\n",
    "                \n",
    "                importance_scores = torch.zeros(tokens.size(-1), device=tokens.device)\n",
    "                num_layers = len(attentions)\n",
    "                \n",
    "                # Generate positional weights (linear decay from 1.0 to 0.5)\n",
    "                position_weights = torch.linspace(1.0, 0.5, steps=tokens.size(-1), device=tokens.device)\n",
    "\n",
    "                for l, layer_attention in enumerate(attentions):\n",
    "                    # Calculate layer weight with position-based scaling\n",
    "                    layer_weight = alpha + (1 - alpha) * (l + 1) / num_layers\n",
    "                    \n",
    "                    # Average attention across heads and batches\n",
    "                    avg_attention = layer_attention.mean(dim=1).squeeze()\n",
    "                    token_importance = avg_attention.mean(dim=-1)\n",
    "                    \n",
    "                    # Add positional weighting to importance scores\n",
    "                    importance_scores += layer_weight * token_importance * position_weights\n",
    "                \n",
    "                return importance_scores\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating importance scores: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def summarize(\n",
    "        self,\n",
    "        text: str,\n",
    "        retention_ratio: float = 0.7,\n",
    "        alpha: float = 0.5,\n",
    "        max_length: int = 2048\n",
    "    ) -> Tuple[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Perform Attention-Guided Token Dropping to summarize text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to summarize.\n",
    "            retention_ratio: Fraction of tokens to retain.\n",
    "            alpha: Weight factor for layer importance.\n",
    "            max_length: Maximum input length.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (summarized text, importance scores).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize input text\n",
    "            tokens = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Calculate number of tokens to keep\n",
    "            n = tokens.input_ids.size(-1)\n",
    "            k = int(n * retention_ratio)\n",
    "\n",
    "            # Get importance scores and top k indices\n",
    "            importance_scores = self.calculate_importance(tokens.input_ids, alpha)\n",
    "            _, indices = torch.sort(importance_scores, descending=True)\n",
    "            keep_indices = sorted(indices[:k].tolist())\n",
    "\n",
    "            # Create reduced token sequence\n",
    "            reduced_tokens = tokens.input_ids[0][keep_indices]\n",
    "            \n",
    "            # Decode back to text\n",
    "            reduced_text = self.tokenizer.decode(reduced_tokens)\n",
    "\n",
    "            return reduced_text, importance_scores.tolist()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in summarization process: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def test_summarizer(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    test_text: Optional[str] = None\n",
    "):\n",
    "    \"\"\"Test the AGTD summarizer with sample text.\"\"\"\n",
    "    \n",
    "    if test_text is None:\n",
    "        test_text = \"<SEX> F <SERVICE> SURGERY <CHIEF COMPLAINT> abdominal pain\"\n",
    "    \n",
    "    summarizer = AGTDSummarizer(model, tokenizer)\n",
    "    \n",
    "    try:\n",
    "        reduced_text, importance_scores = summarizer.summarize(\n",
    "            test_text,\n",
    "            retention_ratio=0.7\n",
    "        )\n",
    "        \n",
    "        print(\"Original Text:\\n\", test_text)\n",
    "        print(\"\\nReduced Text:\\n\", reduced_text)\n",
    "        print(\"\\nToken Importance Scores:\", importance_scores[:10])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing summarizer: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c58db52-339b-42ba-9c63-b275716ec868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_dataset_with_agtd(\n",
    "    df: pd.DataFrame,\n",
    "    summarizer: AGTDSummarizer,\n",
    "    input_column: str = \"input\",\n",
    "    retention_ratio: float = 0.7,\n",
    "    alpha: float = 0.5,\n",
    "    max_length: int = 2048\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply AGTD to a dataset and add reduced text and importance scores as new columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe with an `input` column containing text data.\n",
    "        summarizer: An instance of the AGTDSummarizer class.\n",
    "        input_column: Column name in the dataframe containing the input text.\n",
    "        retention_ratio: Fraction of tokens to retain during summarization.\n",
    "        alpha: Weight factor for layer importance.\n",
    "        max_length: Maximum input length for tokenization.\n",
    "    \n",
    "    Returns:\n",
    "        Updated dataframe with new columns: `reduced_text` and `importance_scores`.\n",
    "    \"\"\"\n",
    "    reduced_texts = []\n",
    "    importance_scores_list = []\n",
    "\n",
    "    print(\"Processing dataset with AGTD...\")\n",
    "    with tqdm(total=len(df), desc=\"Processing Dataset\", unit=\"row\") as pbar:\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[input_column]\n",
    "            try:\n",
    "                # Summarize using AGTDSummarizer\n",
    "                reduced_text, importance_scores = summarizer.summarize(\n",
    "                    text,\n",
    "                    retention_ratio=retention_ratio,\n",
    "                    alpha=alpha,\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                reduced_texts.append(reduced_text)\n",
    "                importance_scores_list.append(importance_scores)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {text[:50]}... Error: {e}\")\n",
    "                reduced_texts.append(\"\")\n",
    "                importance_scores_list.append([])\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Add results as new columns to the dataframe\n",
    "    df[\"reduced_text\"] = reduced_texts\n",
    "    df[\"importance_scores\"] = importance_scores_list\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84df9e80-99aa-49c7-8c67-44c5f5259667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset with AGTD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Dataset:   0%|          | 0/100 [00:00<?, ?row/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Processing Dataset: 100%|██████████| 100/100 [00:27<00:00,  3.69row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine / Thallium-201 / Blue Dye / Iodine-Iodine Containing <ATTENDING> ___. <CHIEF COMPLAINT> Morbid obesity, BMI of 51 <MAJOR SURGICAL OR INVASIVE PROCEDURE> lap gastric bypass <HISTORY OF PRESENT ILLNESS> The patient is a ___ woman with history of obesity, multiple medical problems with a history of 7 pound weight loss and regain. Comorbid conditions include sleep apnea, hypothyroidism, back pain, iron deficiency anemia and headaches. The patient has significant allergies particularly to the blue dye and iodine. The patient was evaluated at ___ ___ ___ Program deemed a good candidate for surgical weight loss. She understands the risks, benefits and alternatives of weight loss surgery. She agrees to diet, exercise, support group and lifelong medical follow-up particularly for B12, calcium and folate levels. <PAST MEDICAL HISTORY> Past medical history includes sleep apnea, hypothyroidism, back pain, urticaria for which she is on chronic steroids, iron deficiency anemia, and headaches <SOCIAL HISTORY> ___ <FAMILY HISTORY> Family history of obesity with mother. <PHYSICAL EXAM> PHYSICAL EXAM ON ADMISSION: GEN: no acute distress BMI is 51. Alert and oriented. HEENT: Neck is supple. PULM: Breathing comfortably. Lungs clear to auscultation. CV: RRR ABD: Abdomen is soft and nontender with a lower abdominal scar. Upper and lower extremities, good range of motion, good strength. Gait and station normal. <PERTINENT RESULTS> ___ 12: 07PM HCT-36.3 <MEDICATIONS ON ADMISSION> Levothyroxine 100 mcg daily for hypothyroid; Zyrtec 10 mg twice a day as needed, Benadryl 25 mg at bedtime for seasonal allergies, chronic hives; Albuterol sulfate two puffs q 6 hours if needed for wheezing (has not used so far); Fioricet 50 mg-325 mg-40 mg once a day as needed for migraine headaches; Clobetasol 0.05% ointment to affected areas twice a day for rash; its Tylenol ___ mg-650 mg every 4 to 6 hours as needed for pain/headache; Ferrous sulfate 325 mg for iron deficiency, multivitamin once a day and vitamin D 1000 units once a day for nutritional supplementation <DISCHARGE MEDICATIONS> 1. Roxicet ___ mg/5 mL Solution Sig: Ten (10) mls PO every four (4) hours. Disp: *500 ccs* Refills: *0* 2. Colace 50 mg/5 mL Liquid Sig: ___ mls PO twice a day. Disp: *500 mls* Refills: *0* 3. Zantac 15 mg/mL Syrup Sig: Five (5) mls PO twice a day. Disp: *500 mls* Refills: *2* 4. Levothyroxine 100 mcg Tablet Sig: One (1) Tablet PO DAILY (Daily). 5. Albuterol Sulfate 90 mcg/Actuation HFA Aerosol Inhaler Sig: ___ Puffs Inhalation Q6H (every 6 hours) as needed for wheeze. 6. Diphenhydramine HCl 25 mg Capsule Sig: One (1) Capsule PO Q4H (every 4 hours) as needed for itch/hives. <DISCHARGE DISPOSITION> Home <DISCHARGE DIAGNOSIS> Morbid Obesity <DISCHARGE CONDITION> Afebrile with vital signs stable <FOLLOWUP INSTRUCTIONS> ___ <DISCHARGE INSTRUCTIONS>\n",
      "Processed DataFrame saved to 'processed_reduced_texts.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset to reduce text dynamically\n",
    "processed_df = process_dataset_with_agtd(\n",
    "    df=mimic_iv_bhc_100,\n",
    "    summarizer=AGTDSummarizer(model, tokenizer),\n",
    "    input_column=\"input\",  # The column containing input text\n",
    "    retention_ratio=0.7,  # Retain 80% of the most important tokens\n",
    "    alpha=0.5,            # Importance weighting factor\n",
    "    max_length=2048       # Max tokenization length\n",
    ")\n",
    "\n",
    "# View one reduced text\n",
    "print(processed_df[\"reduced_text\"].iloc[0])  # Replace 0 with the desired row index\n",
    "# Save the processed DataFrame to a CSV file\n",
    "processed_df.to_csv(\"processed_reduced_texts.csv\", index=False)\n",
    "\n",
    "print(\"Processed DataFrame saved to 'processed_reduced_texts.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb7d59a6-ecd0-400c-bcd8-9053018a76b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced text have been saved to 'processed_reduced_texts.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "Reduced_Text = \"processed_reduced_texts.csv\"\n",
    "processed_df.to_csv(Reduced_Text, index=False)\n",
    "\n",
    "print(f\"Reduced text have been saved to '{Reduced_Text}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7d8cd-a257-47bc-8681-9147374d0f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#processed_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcbe7258-9e88-4d7f-8913-ecd6c8f25f03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  77%|███████▋  | 10/13 [22:17<07:02, 140.84s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Batches: 100%|██████████| 13/13 [28:38<00:00, 132.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine.\",\n",
    "        \"target\": \"The patient was admitted to oncology for worsening back pain. Imaging revealed metastatic cancer with thoracic spine involvement. She was started on pain management and referred for palliative radiation therapy.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"<SEX> M <SERVICE> CARDIOLOGY <CHIEF COMPLAINT> chest pain <HISTORY OF PRESENT ILLNESS> A 55-year-old male presented with chest pain radiating to the left arm and jaw. Initial ECG showed ST-segment elevation in the inferior leads. Troponin levels were elevated.\",\n",
    "        \"target\": \"The patient presented to cardiology with chest pain consistent with acute myocardial infarction. He was taken emergently to the cath lab for primary PCI, and a stent was placed in the right coronary artery.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Define parse_summary_output\n",
    "def parse_summary_output(output):\n",
    "    if isinstance(output, dict) and \"generated_text\" in output:\n",
    "        return output[\"generated_text\"].split(\"Summary:\")[-1].strip()\n",
    "    elif isinstance(output, str):\n",
    "        return output.split(\"Summary:\")[-1].strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct prompts for the batch\n",
    "        prompts = [construct_few_shot_prompt(text) for text in batch]\n",
    "\n",
    "        # Generate summaries for the batch\n",
    "        summaries = summarizer(prompts, **generation_params)\n",
    "\n",
    "        # Parse and store summaries\n",
    "        for summary in summaries:\n",
    "            generated_summaries.append(parse_summary_output(summary[0]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Inspect the summaries\n",
    "#print(\"\\nSample of generated sammaries:\")\n",
    "#print(processed_df[[\"reduced_text\", \"generated_summary\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e55aac67-8b33-4f5f-94f4-fc39d02ea995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The patient presents with worsening back pain, shortness of breath, and abdominal pain due to ovarian cancer. The imaging studies reveal metastatic disease to the thoracic spine and abdomen, with ascites and pleural effusions. The patient underwent paracentesis and thoracentesis, which yielded malignant cells consistent with adenocarcinoma. The patient is being considered for chemotherapy before undergoing surgical intervention. The patient's symptoms suggest a possible diagnosis of ovarian cancer, particularly given the presence of malignant cells in the ascitic fluid. The patient's physical examination reveals signs of ascites and pleural effusions, which are consistent with advanced disease. The patient's laboratory results show elevated CA-125 and CEA levels, indicating possible recurrence or progression of the disease. The patient's family history suggests a possible genetic predisposition to ovarian cancer, although further testing is needed to confirm this. The patient's current treatment plan includes chemotherapy and possibly surgery, pending further evaluation and discussion with the multidisciplinary team.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['generated_summary'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e77c6cb3-82f7-4d6b-841e-ec664e3a5d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to 'generated_summaries.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the updated DataFrame to a CSV file\n",
    "output_file_path = \"generated_summaries.csv\"  # Define the desired output file path\n",
    "processed_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Summaries saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d53c3b40-8504-4bb9-a052-5343239575ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring latency for CPTF+LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Latency: 100%|██████████| 100/100 [28:26<00:00, 17.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency for CPTF+LLM: 17.0613 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure your processed DataFrame has the reduced text column\n",
    "if \"reduced_text\" not in processed_df.columns or \"generated_summary\" not in processed_df.columns:\n",
    "    raise ValueError(\"The 'reduced_text' or 'generated_summary' column is missing from the DataFrame.\")\n",
    "\n",
    "# Initialize variables for latency calculation\n",
    "latencies = []\n",
    "\n",
    "print(\"Measuring latency for CPTF+LLM...\")\n",
    "for reduced_text in tqdm(processed_df[\"reduced_text\"], desc=\"Evaluating Latency\"):\n",
    "    try:\n",
    "        # Measure latency per input\n",
    "        latency_start = time.time()\n",
    "        prompt = construct_few_shot_prompt(reduced_text)  # Use the reduced text for the prompt\n",
    "        generated_summary = summarizer(prompt, **generation_params)[0][\"generated_text\"]\n",
    "        latency_end = time.time()\n",
    "\n",
    "        # Append latency for the input\n",
    "        latencies.append(latency_end - latency_start)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing input: {reduced_text[:50]}... Error: {e}\")\n",
    "        latencies.append(float('inf'))  # Add infinity for failed cases\n",
    "\n",
    "# Calculate average latency\n",
    "valid_latencies = [lat for lat in latencies if lat != float('inf')]\n",
    "average_latency = sum(valid_latencies) / len(valid_latencies) if valid_latencies else float('inf')\n",
    "\n",
    "# Print the average latency\n",
    "print(f\"Average Latency for CPTF+LLM: {average_latency:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d968207-218c-4fee-9db4-060b1e9996d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1a20f-5bbc-4fe9-b710-ac09deba3cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38c24d-9c25-45ff-871b-d2e4179b9764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af5327-6ce9-4376-a03b-66444d384aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df6701-4ee6-47c7-bd0b-fa5f4b3a1f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eceaaa64-c8f6-451a-b402-7ea6f7708087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Summaries: 100%|██████████| 100/100 [00:02<00:00, 42.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Token Efficiency (TE): 1.0000\n",
      "Average Information Retention Ratio (IRR): 0.5405\n",
      "Total Time to First Token (TTFT): 0.0236 seconds\n",
      "Throughput: 42.39 summaries/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the processed DataFrame\n",
    "processed_df = pd.read_csv(\"generated_summaries.csv\")  # Replace with actual file path\n",
    "\n",
    "# Ensure required columns exist\n",
    "if \"reduced_text\" not in processed_df.columns or \"generated_summary\" not in processed_df.columns:\n",
    "    raise ValueError(\"The 'reduced_text' or 'generated_summary' column is missing from the DataFrame.\")\n",
    "\n",
    "# Initialize tokenizer and sentence embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize metrics\n",
    "total_tokens = 0\n",
    "total_meaningful_tokens = 0\n",
    "total_irr = 0\n",
    "summaries_count = len(processed_df)\n",
    "\n",
    "# Start time for throughput measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate each row\n",
    "for _, row in tqdm(processed_df.iterrows(), desc=\"Evaluating Summaries\", total=summaries_count):\n",
    "    input_text = row[\"reduced_text\"]\n",
    "    generated_summary = row[\"generated_summary\"]\n",
    "\n",
    "    # Tokenization metrics\n",
    "    total_tokens += len(tokenizer.tokenize(generated_summary))\n",
    "    total_meaningful_tokens += len(tokenizer.tokenize(generated_summary.strip()))\n",
    "\n",
    "    # Information Retention Ratio (IRR)\n",
    "    input_embedding = sbert_model.encode([input_text])\n",
    "    summary_embedding = sbert_model.encode([generated_summary])\n",
    "    similarity = cosine_similarity(input_embedding, summary_embedding)[0][0]\n",
    "    total_irr += similarity\n",
    "\n",
    "# Calculate total time taken\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Final metrics\n",
    "average_te = total_meaningful_tokens / total_tokens if total_tokens > 0 else 0\n",
    "average_irr = total_irr / summaries_count if summaries_count > 0 else 0\n",
    "average_ttft = total_time / summaries_count if summaries_count > 0 else 0\n",
    "throughput = summaries_count / total_time if total_time > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average Token Efficiency (TE): {average_te:.4f}\")\n",
    "print(f\"Average Information Retention Ratio (IRR): {average_irr:.4f}\")\n",
    "print(f\"Total Time to First Token (TTFT): {average_ttft:.4f} seconds\")\n",
    "print(f\"Throughput: {throughput:.2f} summaries/second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f993edf8-15f6-4050-894b-de2da7cf8f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Latency: 100%|██████████| 100/100 [28:23<00:00, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency: 17.0331 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 8  # Or any size suitable for your memory\n",
    "batched_latencies = []\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Evaluating Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"].iloc[i:i+batch_size].tolist()\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        prompts = [construct_few_shot_prompt(text) for text in batch]\n",
    "        summaries = summarizer(prompts, **generation_params)\n",
    "        end_time = time.time()\n",
    "        batched_latencies.append(end_time - start_time)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}: {e}\")\n",
    "        batched_latencies.append(float('inf'))\n",
    "average_batch_latency = sum(batched_latencies) / len(batched_latencies)\n",
    "print(f\"Average Batch Latency: {average_batch_latency:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3cc32-5350-4cf0-a8ff-268b666dde7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
