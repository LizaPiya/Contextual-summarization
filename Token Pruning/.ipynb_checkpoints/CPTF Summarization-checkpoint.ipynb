{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3470902-6191-49f2-990c-3bb9668a35b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f60c1d1-468b-41da-9781-8634049b40e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id                                              input  \\\n",
      "0  16002318-DS-17  <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...   \n",
      "1   15638884-DS-4  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Augment...   \n",
      "2  12435705-DS-14  <SEX> M <SERVICE> MEDICINE <ALLERGIES> ibuprof...   \n",
      "3   12413577-DS-4  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
      "4  17967161-DS-29  <SEX> M <SERVICE> SURGERY <ALLERGIES> lisinopr...   \n",
      "\n",
      "                                              target  input_tokens  \\\n",
      "0  This is a ___ yo F admitted to the hospital af...          1195   \n",
      "1  Mr. ___ is a ___ yo man with CAD with prior MI...          3496   \n",
      "2  Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...          5591   \n",
      "3  On ___, Ms. ___ was admitted to the gynecology...          1119   \n",
      "4  Mr. ___ underwent an angiogram on ___ which sh...          3307   \n",
      "\n",
      "   target_tokens                                       reduced_text  \\\n",
      "0             75  <|begin_of_text|><SEX> F <SERVICE> SURGERY <AL...   \n",
      "1           1143  <|begin_of_text|><SEX> M <SERVICE> MEDICINE <A...   \n",
      "2           1098  <|begin_of_text|><SEX> M <SERVICE> MEDICINE <A...   \n",
      "3            221  <|begin_of_text|><SEX> F <SERVICE> OBSTETRICS/...   \n",
      "4            439  <|begin_of_text|><SEX> M <SERVICE> SURGERY <AL...   \n",
      "\n",
      "                                   importance_scores  \n",
      "0  [0.010251283645629883, 0.010246990248560905, 0...  \n",
      "1  [0.0059814453125, 0.005979984533041716, 0.0059...  \n",
      "2  [0.0059814453125, 0.005979984533041716, 0.0059...  \n",
      "3  [0.010935068130493164, 0.0109328031539917, 0.0...  \n",
      "4  [0.0059814453125, 0.005979984533041716, 0.0059...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "processed_df = pd.read_csv(\"processed_reduced_texts.csv\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(processed_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f9e8b8-beeb-41e3-bf21-4628ca229d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94395844-7106-40fe-aad5-9e7dce09f3a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d07e2c-98fe-405f-b8b5-96007ea65bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab1a9d1-21aa-44c5-bae3-858709206187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup and quantization configuration done.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configure quantization (8-bit)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "print(\"Environment setup and quantization configuration done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da9ea36d-cc3d-4fec-904c-01e80e2d12a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer:   0%|          | 0/2 [00:00<?, ?step/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bade3d67ec4fac8f639396b0886b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3aee5c1adf4421395ef0ceaad3c3d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf088ac06c94dc1a66f9c7a1406e6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer:  50%|█████     | 1/2 [01:12<01:12, 72.00s/step]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5718d1056b7e4b449cd46b5621930b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed05d00c4253444f936b6c32ae02767c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e639d1724cde4233949048e11d5a98f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer: 100%|██████████| 2/2 [01:13<00:00, 36.72s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "with tqdm(total=2, desc=\"Initializing Model and Tokenizer\", unit=\"step\") as pbar:\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16       \n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    pbar.update(1)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d39979-4953-4f2f-aeec-39ba2f022e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization pipeline initialized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(\"Summarization pipeline initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "316a79f8-ef74-4712-9ca0-409dc8f71d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 13/13 [22:17<00:00, 102.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 153085\n",
      "Total Output Tokens: 173773\n",
      "Total Time Spent: 1336.44 seconds\n",
      "Average Latency (Time per Summary): 13.3032 seconds\n",
      "Average TTFT (Time to First Token): 13361.37 ms\n",
      "Average Throughput: 258.51 tokens/second\n",
      "Token Efficiency (TE): 1.1351\n",
      "\n",
      "Summaries saved to 'FLP_fixed_ttft_fewshot_summaries.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ✅ Your Few-Shot Prompt (Updated)\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "You are an EXPERT AT WRITING CLINICAL SUMMARY. Summarize the input text in a very cohesive manner. Maintain storytelling style. \n",
    "Example 1:\n",
    "Input: 45-year-old male with diabetes presented with chest pain and shortness of breath. ECG showed myocardial infarction. Patient treated with aspirin and admitted to cardiac unit.\n",
    "Summary: 45-year-old male with diabetes, chest pain, MI confirmed by ECG, treated with aspirin, admitted.\n",
    "\n",
    "Example 2:\n",
    "Input: 72-year-old female with history of hypertension and stroke admitted with slurred speech, left-sided weakness. MRI confirmed acute ischemic stroke. Started on anticoagulation and monitored in ICU.\n",
    "Summary: 72-year-old female with hypertension, stroke, slurred speech, left-sided weakness, ischemic stroke confirmed by MRI, started on anticoagulation.\n",
    "\n",
    "Example 3:\n",
    "Input: 60-year-old female with chronic kidney disease, admitted for worsening kidney function. Lab tests showed elevated creatinine. Dialysis started.\n",
    "Summary:\n",
    " \n",
    "Now summarize the following clinical note using only provided input. Do not add anything from the prompt. If unsure, say 'UNKNOWN':\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ✅ Generation parameters (Fixed)\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 80,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"use_cache\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,  # Ensures valid padding\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on memory\n",
    "\n",
    "# ✅ Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# ✅ Function to Measure TTFT (NO Streaming Output)\n",
    "def measure_ttft(prompt, model, tokenizer):\n",
    "    start_time = time.time() * 1000  # Convert to milliseconds\n",
    "    first_token_time = None\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    # ✅ Generate output (NO STREAMER → No real-time output)\n",
    "    full_output = model.generate(\n",
    "        **inputs,\n",
    "        **generation_params  # ✅ No streamer, generates silently\n",
    "    )\n",
    "\n",
    "    # Capture TTFT\n",
    "    first_token_time = time.time() * 1000\n",
    "    ttft = first_token_time - start_time\n",
    "\n",
    "    # Decode full response\n",
    "    generated_text = tokenizer.decode(full_output[0], skip_special_tokens=True).strip()\n",
    "    return ttft, {\"generated_text\": generated_text}\n",
    "\n",
    "# ✅ Batch Processing (WITH `tqdm` Only)\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # ✅ Keep Few-Shot Prompt\n",
    "        prompts = [f\"{FEW_SHOT_EXAMPLES}\\n{text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "\n",
    "            # ✅ Get TTFT and Full Response (NO Printing Each Output)\n",
    "            ttft, output = measure_ttft(prompt, model, tokenizer)\n",
    "            ttft_list.append(ttft)\n",
    "\n",
    "            # ✅ Store Summary\n",
    "            summaries.append(output[\"generated_text\"].strip())\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # ✅ Compute Metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total batch time\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # ✅ Update Global Metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # ✅ Store Summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries if failed\n",
    "\n",
    "# ✅ Add Summaries to DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# ✅ Compute Final Metrics\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0  # Now in milliseconds\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# ✅ Print Final Metrics (Only Once)\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.2f} ms\")  # Display in milliseconds\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# ✅ Save Results\n",
    "processed_df.to_csv(\"FLP_fixed_ttft_fewshot_summaries.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_fixed_ttft_fewshot_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb3fa5-d632-403d-9ad9-8fbf03d2c8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722b18c-e669-490a-afee-f07320c3fdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3ec88-8095-46a3-b980-c487f2f339cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd02aa-38b8-40cd-8ec0-27b4b31caaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87140cf-818a-4873-bc42-c7b99b1c505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ✅ YOUR ORIGINAL FEW-SHOT PROMPT (RESTORED)\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "You are an EXPERT AT WRITING CLINICAL SUMMARY. Summarize the input text in a very cohesive manner. Maintain storytelling style. \n",
    "Example 1:\n",
    "Input: 45-year-old male with diabetes presented with chest pain and shortness of breath. ECG showed myocardial infarction. Patient treated with aspirin and admitted to cardiac unit.\n",
    "Summary: 45-year-old male with diabetes, chest pain, MI confirmed by ECG, treated with aspirin, admitted.\n",
    "\n",
    "Example 2:\n",
    "Input: 72-year-old female with history of hypertension and stroke admitted with slurred speech, left-sided weakness. MRI confirmed acute ischemic stroke. Started on anticoagulation and monitored in ICU.\n",
    "Summary: 72-year-old female with hypertension, stroke, slurred speech, left-sided weakness, ischemic stroke confirmed by MRI, started on anticoagulation.\n",
    "\n",
    "Example 3 (Correcting Hallucination):\n",
    "Input: 60-year-old female with chronic kidney disease, admitted for worsening kidney function. Lab tests showed elevated creatinine. Dialysis started.\n",
    "Summary: 60-year-old female with kidney disease, worsening function, elevated creatinine, started on dialysis.\n",
    "\n",
    "Now summarize the following clinical note using only its information. Do not add or infer anything. If unsure, say 'UNKNOWN':\n",
    "\"\"\"\n",
    "\n",
    "# ✅ GENERATION PARAMETERS\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"stream\": True  # Enables real-time token streaming for TTFT\n",
    "}\n",
    "\n",
    "batch_size = 4  # Balanced batch size for 8-bit models\n",
    "\n",
    "# ✅ METRICS TRACKING\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# ✅ FUNCTION TO MEASURE TTFT USING STREAMING INFERENCE\n",
    "def measure_ttft(prompt):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Stream the response and capture first token\n",
    "    for response in summarizer(prompt, **generation_params):\n",
    "        first_token_time = time.time()\n",
    "        return first_token_time - start_time, response  # Return TTFT & first token\n",
    "\n",
    "# ✅ BATCH PROCESSING\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # ✅ KEEP YOUR FEW-SHOT PROMPT UNCHANGED\n",
    "        prompts = [f\"{FEW_SHOT_EXAMPLES}\\n{text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Measure batch start time\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "\n",
    "            # ✅ GET TTFT AND FIRST TOKEN VIA STREAMING\n",
    "            ttft, output = measure_ttft(prompt)\n",
    "            ttft_list.append(ttft)\n",
    "\n",
    "            # ✅ CONTINUE GENERATING REMAINING TOKENS\n",
    "            generated_text = output[0][\"generated_text\"].strip()\n",
    "            summaries.append(generated_text)\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # ✅ COMPUTE METRICS\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total batch time\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # ✅ UPDATE GLOBAL METRICS\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # ✅ STORE SUMMARIES\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# ✅ ADD SUMMARIES TO DATAFRAME\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# ✅ METRICS CALCULATION\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# ✅ PRINT METRICS\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# ✅ SAVE RESULTS\n",
    "processed_df.to_csv(\"FLP_fixed_ttft_fewshot_summaries.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_fixed_ttft_fewshot_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ba452c-7ed3-42a6-b425-366dc1cffd55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   8%|▊         | 2/25 [01:11<13:14, 34.53s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Batches: 100%|██████████| 25/25 [14:55<00:00, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 156585\n",
      "Total Output Tokens: 12955\n",
      "Total Time Spent: 895.29 seconds\n",
      "Average Latency (Time per Summary): 8.9529 seconds\n",
      "Average TTFT (Time to First Token): 8.9529 seconds\n",
      "Average Throughput: 193.03 tokens/second\n",
      "Token Efficiency (TE): 0.0827\n",
      "\n",
      "Summaries saved to 'FLP_strict_generated_summaries.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Stronger Prompt to Prevent Hallucination\n",
    "FEW_SHOT_EXAMPLES = \"\"\"\n",
    "You are an expert at writing clinical summary. Summarize the input text in a very cohesive manner. Maintain storytelling manner. \n",
    "Example 1:\n",
    "Input: 45-year-old male with diabetes presented with chest pain and shortness of breath. ECG showed myocardial infarction. Patient treated with aspirin and admitted to cardiac unit.\n",
    "Summary: 45-year-old male with diabetes, chest pain, MI confirmed by ECG, treated with aspirin, admitted.\n",
    "\n",
    "Example 2:\n",
    "Input: 72-year-old female with history of hypertension and stroke admitted with slurred speech, left-sided weakness. MRI confirmed acute ischemic stroke. Started on anticoagulation and monitored in ICU.\n",
    "Summary: 72-year-old female with hypertension, stroke, slurred speech, left-sided weakness, ischemic stroke confirmed by MRI, started on anticoagulation.\n",
    "\n",
    "Example 3 (Correcting Hallucination):\n",
    "Input: 60-year-old female with chronic kidney disease, admitted for worsening kidney function. Lab tests showed elevated creatinine. Dialysis started.\n",
    "INCORRECT Summary: 60-year-old female with kidney disease, received kidney transplant. \n",
    "CORRECT Summary: 60-year-old female with kidney disease, worsening function, elevated creatinine, started on dialysis.\n",
    "\n",
    "Do not add or infer anything from the few shot example. If unsure, say 'UNKNOWN':\n",
    "\"\"\"\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.6,  # Lowered temperature to reduce hallucination\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"repetition_penalty\": 1.2,  # Increased to avoid prompt repetition\n",
    "    \"return_full_text\": False\n",
    "}\n",
    "\n",
    "batch_size = 4  # Balanced batch size for 8-bit models\n",
    "\n",
    "# Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct Few-Shot Prompt with input text\n",
    "        prompts = [f\"{FEW_SHOT_EXAMPLES}\\n{text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Measure batch start time\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "\n",
    "            # Generate text while measuring TTFT correctly\n",
    "            output = summarizer(prompt, **generation_params)\n",
    "            first_token_time = time.time()  # Capture time when first token is generated\n",
    "\n",
    "            # Store TTFT (time to first token)\n",
    "            ttft_list.append(first_token_time - single_start_time)\n",
    "\n",
    "            # Extract generated text correctly\n",
    "            generated_text = output[0][\"generated_text\"].strip()\n",
    "            summaries.append(generated_text)\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # Compute batch-level metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total batch time\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # Update global metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # Store summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "processed_df.to_csv(\"FLP_strict_generated_summaries.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_strict_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "237ec8b6-e5c5-430d-94f2-61314f78be0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summaries saved to 'FLP_generated_summaries_with_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "processed_df.to_csv(\"FLP_generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'FLP_generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8805fd0-e5bf-4486-aef9-acd78bc2b06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863e87aa-b74b-46ff-b2cc-d09b5d3fc139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 13/13 [10:30<00:00, 48.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 129585\n",
      "Total Output Tokens: 139251\n",
      "Total Time Spent: 629.88 seconds\n",
      "Average Latency (Time per Summary): 6.3091 seconds\n",
      "Average TTFT (Time to First Token): 6.2988 seconds\n",
      "Average Throughput: 428.38 tokens/second\n",
      "Token Efficiency (TE): 1.0746\n",
      "\n",
      "Summaries saved to 'generated_summaries_with_metrics.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}\n",
    "\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Metrics tracking\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "total_time_spent = 0\n",
    "ttft_list = []  # Time to first token\n",
    "latency_list = []  # Time per summary\n",
    "throughput_list = []  # Tokens processed per second\n",
    "\n",
    "# Batch processing\n",
    "print(\"Generating summaries in batches...\")\n",
    "generated_summaries = []\n",
    "\n",
    "for i in tqdm(range(0, len(processed_df), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = processed_df[\"reduced_text\"][i:i + batch_size].tolist()\n",
    "\n",
    "    try:\n",
    "        # Construct prompts for the batch\n",
    "        prompts = [f\"You are a medical expert. {text}\" for text in batch]\n",
    "        batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "        # Generate summaries and measure TTFT and latency\n",
    "        batch_start_time = time.time()\n",
    "        summaries = []\n",
    "        for prompt in prompts:\n",
    "            single_start_time = time.time()\n",
    "            output = summarizer(prompt, **generation_params)\n",
    "            single_end_time = time.time()\n",
    "            ttft_list.append(single_end_time - single_start_time)  # Time to first token\n",
    "            summaries.append(output[0][\"generated_text\"])  # Extract generated text\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        # Calculate batch metrics\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "        batch_latency = batch_end_time - batch_start_time  # Total time for the batch\n",
    "        latency_list.append(batch_latency / len(batch))  # Average latency per summary\n",
    "        throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_latency)\n",
    "\n",
    "        # Update global metrics\n",
    "        total_input_tokens += batch_input_tokens\n",
    "        total_output_tokens += batch_output_tokens\n",
    "        total_time_spent += batch_latency\n",
    "\n",
    "        # Store summaries\n",
    "        generated_summaries.extend(summaries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summaries for batch starting at index {i}: {e}\")\n",
    "        generated_summaries.extend([\"\"] * len(batch))  # Fill with empty summaries in case of failure\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "processed_df[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(latency_list) / len(latency_list) if latency_list else 0\n",
    "average_ttft = sum(ttft_list) / len(ttft_list) if ttft_list else 0\n",
    "average_throughput = sum(throughput_list) / len(throughput_list) if throughput_list else 0\n",
    "token_efficiency = total_output_tokens / total_input_tokens if total_input_tokens else 0\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_output_tokens}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "processed_df.to_csv(\"generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d46cfe-8560-479c-9598-31b099b4511c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
