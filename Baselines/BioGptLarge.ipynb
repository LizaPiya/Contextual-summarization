{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b049fd-a153-4a71-9713-d99659a39367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'mimicivliza'  # Replace with your bucket name\n",
    "mimic_iv_bhc = f's3://{bucket_name}/sample_data_100.csv'\n",
    "\n",
    "# Load the files\n",
    "mimic_iv_bhc_100 = pd.read_csv(mimic_iv_bhc)\n",
    "\n",
    "# Display the data\n",
    "mimic_iv_bhc_100.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e97b23-aaf6-452d-b8ec-2dc1691adef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Hugging Face Transformers\n",
    "!pip install -q transformers\n",
    "!pip install -q sacremoses\n",
    "!pip install -q bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f51402-99dc-4ba7-baef-def718fbe866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,           # or load_in_4bit=True for more compression\n",
    "    llm_int8_threshold=6.0,      # default threshold for 8-bit\n",
    "    llm_int8_skip_modules=None,  # or specify modules to skip\n",
    "    bnb_8bit_compute_dtype=\"float16\"  # use float16 for faster inference\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/BioGPT-Large\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # use this if you're on a GPU or want automatic device placement\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275a4b9-1def-45fe-b24a-32d22fa1e832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Adjusted prompt with clearer instructions\n",
    "def construct_few_shot_prompt(input_text):\n",
    "    prompt = (\n",
    "        \"You are an expert clinician. Summarize the patient case concisely. \"\n",
    "        \"Ensure the summary does not exceed 200 words and avoids directly copying any text.\\n\\n\"\n",
    "        \"Example - Input: A 50-year-old male reports severe chest pain and has a history of heart disease.\\n\"\n",
    "        \"Example - Summary: A male patient with heart disease experienced severe chest pain and received emergency angioplasty.\\n\\n\"\n",
    "        f\"Input: {input_text}\\nSummary:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "subset_df = mimic_iv_bhc_100.iloc[:101].copy()  # Processing the first 101 entries for the example\n",
    "generated_summaries = []\n",
    "\n",
    "for input_text in tqdm(subset_df['input'], desc=\"Generating Summaries with BioGPT-Large\"):\n",
    "    prompt = construct_few_shot_prompt(input_text)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1000  # Adjust max length according to your model's capabilities\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    generation_params = {\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 20,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        original_length = inputs[\"input_ids\"].shape[1]\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_params\n",
    "        )\n",
    "\n",
    "        # Decode only the newly generated tokens\n",
    "        generated_tokens = summary_ids[0, original_length:]\n",
    "        generated_summary = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    generated_summaries.append(generated_summary)\n",
    "    torch.cuda.empty_cache()  # Optional: Clear CUDA cache after each generation to free memory\n",
    "\n",
    "# Add generated summaries to DataFrame and save\n",
    "subset_df['generated_summary_biogpt_large'] = generated_summaries\n",
    "subset_df.to_csv(\"biogpt_large_generated_summaries.csv\", index=False)\n",
    "print(\"âœ… Subset summaries saved to 'biogpt_large_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb90a6-c63a-4172-9b0c-abdc4d54648f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323849d-f537-4f50-b655-33bd1148ebf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(subset_df['input'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76930973-093d-4c97-b28b-88468041b6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(subset_df['generated_summary_biogpt_large'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311f5ba-759a-42e5-aa0a-c0c06ab6a02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8b176-fbce-4721-8599-e4fa898e78a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Few-shot examples for prompt construction\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> M <SERVICE> CARDIOLOGY <CHIEF COMPLAINT> chest pain <HISTORY OF PRESENT ILLNESS> A 55-year-old male presented with chest pain radiating to his left arm and jaw. He reported associated shortness of breath and nausea. Initial ECG showed ST-segment elevation in the inferior leads, and troponin levels were elevated, consistent with an acute myocardial infarction. The patient was emergently taken to the catheterization lab, where coronary angiography revealed 100% occlusion of the right coronary artery. A drug-eluting stent was successfully placed, and dual antiplatelet therapy was initiated. He was monitored in the cardiac care unit for 48 hours with no complications.\",\n",
    "        \"Summary\": \"A 55-year-old male with a myocardial infarction underwent PCI with stenting of the right coronary artery. He was monitored and started on dual antiplatelet therapy.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"<SEX> M <SERVICE> NEUROSURGERY <CHIEF COMPLAINT> headache past ten days <HISTORY OF PRESENT ILLNESS> The patient is a 60-year-old male with a history of renal cell carcinoma, presenting with progressively worsening right-sided headaches over 10 days. Imaging revealed a large intracranial mass in the right temporoparietal lobe with associated hemorrhage, necrosis, and vasogenic edema. He was started on Decadron to reduce cerebral edema and transferred for neurosurgical evaluation. On hospital day 5, the patient underwent an image-guided right craniotomy for tumor resection. Postoperative MRI showed no significant residual tumor. He recovered without complications and was discharged home on a tapering dose of Decadron, with follow-up in the brain tumor clinic.\",\n",
    "        \"Summary\": \"A 60-year-old male with an intracranial mass underwent a craniotomy for tumor resection. He recovered well and was discharged with follow-up.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> GENERAL MEDICINE <CHIEF COMPLAINT> shortness of breath <HISTORY OF PRESENT ILLNESS> A 72-year-old female with a history of COPD presented with progressive shortness of breath over two weeks. Physical examination revealed decreased breath sounds and crackles in the left lower lobe. Imaging confirmed left lower lobe consolidation consistent with pneumonia. The patient was started on broad-spectrum antibiotics and oxygen therapy. Arterial blood gas analysis showed mild hypoxemia, which improved with oxygen supplementation. She remained afebrile throughout her stay and reported gradual improvement in symptoms. Discharge planning included antibiotics, a pulmonary follow-up, and instructions for home oxygen therapy.\",\n",
    "        \"Summary\": \"A 72-year-old female with COPD was treated for pneumonia with antibiotics and oxygen therapy. She showed improvement and was discharged with follow-up.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to construct the prompt (unchanged)\n",
    "def construct_few_shot_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Please summarize the following input concisely  and cohesively:\\n\\n. Do not include anything from the exmple.\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Step 2: Generate Summaries with BioGPT-Large\n",
    "generated_summaries = []\n",
    "\n",
    "for input_text in tqdm(mimic_iv_bhc_100['input'], desc=\"Generating Summaries with BioGPT-Large\"):\n",
    "    # Construct the prompt\n",
    "    prompt = construct_few_shot_prompt(input_text)\n",
    "\n",
    "    # Tokenize prompt for BioGPT (causal language model)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Move tokenized input tensors to model's device (e.g., CUDA)\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate the summary using causal generation\n",
    "    generation_params = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 40,\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        **generation_params\n",
    "    )\n",
    "\n",
    "    # Decode and post-process the output\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    generated_summary = generated_summary.split(\"Summary:\")[-1].strip()  # Optional cleanup\n",
    "    generated_summaries.append(generated_summary)\n",
    "\n",
    "# Step 3: Add Generated Summaries to the DataFrame\n",
    "mimic_iv_bhc_100['generated_summary_biogpt_large'] = generated_summaries\n",
    "\n",
    "# Optional: Save the results\n",
    "mimic_iv_bhc_100.to_csv(\"biogpt_large_generated_summaries.csv\", index=False)\n",
    "print(\"Summaries generated and saved to 'biogpt_large_generated_summaries.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c2243-6ad8-4d1a-b2d3-e5461e2134ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
