{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215db1a4-1c1b-4539-a86c-72269d8a78e0",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121dc8ef-ef1c-462b-90dd-450a0c4245f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ba1083-1d25-4380-850b-7c829c5e1615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>target_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16002318-DS-17</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; Iodine /...</td>\n",
       "      <td>This is a ___ yo F admitted to the hospital af...</td>\n",
       "      <td>1195</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15638884-DS-4</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Augment...</td>\n",
       "      <td>Mr. ___ is a ___ yo man with CAD with prior MI...</td>\n",
       "      <td>3496</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12435705-DS-14</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; ibuprof...</td>\n",
       "      <td>Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...</td>\n",
       "      <td>5591</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12413577-DS-4</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; OBSTETRICS/GYNECOLOGY &lt;ALLER...</td>\n",
       "      <td>On ___, Ms. ___ was admitted to the gynecology...</td>\n",
       "      <td>1119</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17967161-DS-29</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; lisinopr...</td>\n",
       "      <td>Mr. ___ underwent an angiogram on ___ which sh...</td>\n",
       "      <td>3307</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16956007-DS-20</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; Codeine ...</td>\n",
       "      <td>Mr. ___ is a ___ who underwent an exploratory ...</td>\n",
       "      <td>4168</td>\n",
       "      <td>1209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16919911-DS-15</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Penicil...</td>\n",
       "      <td>This is a ___ year old female with a recent di...</td>\n",
       "      <td>2059</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15682570-DS-25</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; No Know...</td>\n",
       "      <td>___ w/ h/o CAD ___ CABG LIMA to LAD, SVG to D1...</td>\n",
       "      <td>2215</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12135369-DS-24</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Compazi...</td>\n",
       "      <td>Ms ___ is a ___ year old woman with a history ...</td>\n",
       "      <td>2132</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11906321-DS-20</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; NEUROSURGERY &lt;ALLERGIES&gt; Pat...</td>\n",
       "      <td>The patient was admitted to the neurosurgery s...</td>\n",
       "      <td>2347</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17384661-DS-20</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; UROLOGY &lt;ALLERGIES&gt; No Known...</td>\n",
       "      <td>Mr. ___ was admitted to Dr. ___ Urology servic...</td>\n",
       "      <td>1452</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11162709-DS-18</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Sulfa (...</td>\n",
       "      <td>___ yoM with PMH notable for hx of left ICA an...</td>\n",
       "      <td>2469</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19473355-DS-13</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; No Known...</td>\n",
       "      <td>Mr. ___ is a ___ year old male with AAA who wa...</td>\n",
       "      <td>1593</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14711846-DS-15</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Gold Sa...</td>\n",
       "      <td>Ms. ___ was admitted for diagnosis and treatme...</td>\n",
       "      <td>3140</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18094090-DS-35</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Penicil...</td>\n",
       "      <td>Patient is a ___ year old woman with history o...</td>\n",
       "      <td>2346</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16047293-DS-34</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Penicil...</td>\n",
       "      <td>___ year-old male with PMH of diastolic heart ...</td>\n",
       "      <td>1747</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15690160-DS-11</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; NEUROLOGY &lt;ALLERGIES&gt; No Dru...</td>\n",
       "      <td>Her neuro exam has been asymptomatic since adm...</td>\n",
       "      <td>2438</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18477272-DS-18</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; OBSTETRICS/GYNECOLOGY &lt;ALLER...</td>\n",
       "      <td>On ___, Ms. ___ was admitted to the gynecology...</td>\n",
       "      <td>1192</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10476129-DS-14</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; No Know...</td>\n",
       "      <td>BRIEF COURSE: ___ with limited PMH presented w...</td>\n",
       "      <td>2998</td>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19868904-DS-20</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; NEUROLOGY &lt;ALLERGIES&gt; No Kno...</td>\n",
       "      <td>Ms. ___ was admitted to the neurology service ...</td>\n",
       "      <td>2072</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           note_id                                              input  \\\n",
       "0   16002318-DS-17  <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...   \n",
       "1    15638884-DS-4  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Augment...   \n",
       "2   12435705-DS-14  <SEX> M <SERVICE> MEDICINE <ALLERGIES> ibuprof...   \n",
       "3    12413577-DS-4  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
       "4   17967161-DS-29  <SEX> M <SERVICE> SURGERY <ALLERGIES> lisinopr...   \n",
       "5   16956007-DS-20  <SEX> M <SERVICE> SURGERY <ALLERGIES> Codeine ...   \n",
       "6   16919911-DS-15  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Penicil...   \n",
       "7   15682570-DS-25  <SEX> M <SERVICE> MEDICINE <ALLERGIES> No Know...   \n",
       "8   12135369-DS-24  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Compazi...   \n",
       "9   11906321-DS-20  <SEX> M <SERVICE> NEUROSURGERY <ALLERGIES> Pat...   \n",
       "10  17384661-DS-20  <SEX> M <SERVICE> UROLOGY <ALLERGIES> No Known...   \n",
       "11  11162709-DS-18  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Sulfa (...   \n",
       "12  19473355-DS-13  <SEX> M <SERVICE> SURGERY <ALLERGIES> No Known...   \n",
       "13  14711846-DS-15  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Gold Sa...   \n",
       "14  18094090-DS-35  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Penicil...   \n",
       "15  16047293-DS-34  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Penicil...   \n",
       "16  15690160-DS-11  <SEX> F <SERVICE> NEUROLOGY <ALLERGIES> No Dru...   \n",
       "17  18477272-DS-18  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
       "18  10476129-DS-14  <SEX> F <SERVICE> MEDICINE <ALLERGIES> No Know...   \n",
       "19  19868904-DS-20  <SEX> F <SERVICE> NEUROLOGY <ALLERGIES> No Kno...   \n",
       "\n",
       "                                               target  input_tokens  \\\n",
       "0   This is a ___ yo F admitted to the hospital af...          1195   \n",
       "1   Mr. ___ is a ___ yo man with CAD with prior MI...          3496   \n",
       "2   Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...          5591   \n",
       "3   On ___, Ms. ___ was admitted to the gynecology...          1119   \n",
       "4   Mr. ___ underwent an angiogram on ___ which sh...          3307   \n",
       "5   Mr. ___ is a ___ who underwent an exploratory ...          4168   \n",
       "6   This is a ___ year old female with a recent di...          2059   \n",
       "7   ___ w/ h/o CAD ___ CABG LIMA to LAD, SVG to D1...          2215   \n",
       "8   Ms ___ is a ___ year old woman with a history ...          2132   \n",
       "9   The patient was admitted to the neurosurgery s...          2347   \n",
       "10  Mr. ___ was admitted to Dr. ___ Urology servic...          1452   \n",
       "11  ___ yoM with PMH notable for hx of left ICA an...          2469   \n",
       "12  Mr. ___ is a ___ year old male with AAA who wa...          1593   \n",
       "13  Ms. ___ was admitted for diagnosis and treatme...          3140   \n",
       "14  Patient is a ___ year old woman with history o...          2346   \n",
       "15  ___ year-old male with PMH of diastolic heart ...          1747   \n",
       "16  Her neuro exam has been asymptomatic since adm...          2438   \n",
       "17  On ___, Ms. ___ was admitted to the gynecology...          1192   \n",
       "18  BRIEF COURSE: ___ with limited PMH presented w...          2998   \n",
       "19  Ms. ___ was admitted to the neurology service ...          2072   \n",
       "\n",
       "    target_tokens  \n",
       "0              75  \n",
       "1            1143  \n",
       "2            1098  \n",
       "3             221  \n",
       "4             439  \n",
       "5            1209  \n",
       "6             208  \n",
       "7             451  \n",
       "8             416  \n",
       "9             316  \n",
       "10            171  \n",
       "11            420  \n",
       "12            259  \n",
       "13            415  \n",
       "14            528  \n",
       "15            144  \n",
       "16             18  \n",
       "17            154  \n",
       "18           1119  \n",
       "19            221  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'mimicivliza'  # Replace with your bucket name\n",
    "mimic_iv_bhc = f's3://{bucket_name}/sample_data_100.csv'\n",
    "\n",
    "# Load the files\n",
    "mimic_iv_bhc_100 = pd.read_csv(mimic_iv_bhc)\n",
    "\n",
    "# Display the data\n",
    "mimic_iv_bhc_100.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a199bd63-cba4-4e32-aa70-b4647fadfe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166a474a-5b2c-46bd-94e2-bdf6982bf803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83855680-8a92-4de0-b01d-26fef625a6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "note_id          100\n",
       "input            100\n",
       "target           100\n",
       "input_tokens     100\n",
       "target_tokens     91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_iv_bhc_100.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9993a7-2e70-4555-8bf3-641d46cf8939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76406c6d037843a2aa4158ed6e6d1e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Summarizing clinical notes:   1%|          | 1/100 [35:15<58:10:32, 2115.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Apply summarization with progress bar\u001b[39;00m\n\u001b[1;32m     55\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarizing clinical notes\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Enable tqdm for pandas\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m mimic_iv_bhc_100[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmimic_iv_bhc_100\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummarize_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m     59\u001b[0m mimic_iv_bhc_100\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_summaries_100.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msummarize_text\u001b[39m(text):\n\u001b[1;32m     50\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m construct_prompt(text)\n\u001b[0;32m---> 51\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_params\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:285\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py:1362\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1355\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         )\n\u001b[1;32m   1360\u001b[0m     )\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py:1369\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1368\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1369\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py:1269\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1268\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1269\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:383\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    381\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 383\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:832\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 832\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:591\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    580\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    581\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m         position_embeddings,\n\u001b[1;32m    589\u001b[0m     )\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:344\u001b[0m, in \u001b[0;36mGemmaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    343\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 344\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    347\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:90\u001b[0m, in \u001b[0;36mGemmaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 90\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Load model and tokenizer for Gemma\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Ensure tokenizer settings\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding is defined\n",
    "\n",
    "# Initialize text-generation pipeline for Gemma\n",
    "summarizer = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generation parameters for Gemma\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,  # Controlled sampling\n",
    "    \"temperature\": 0.7,  # Less randomness for focused outputs\n",
    "    \"top_k\": 40,  # Limit next token options\n",
    "    \"max_new_tokens\": 100,  # Limit maximum length of output\n",
    "    \"repetition_penalty\": 1.1  # Reduce repetition\n",
    "}\n",
    "\n",
    "# Few-shot examples for summarization\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine. She reported increasing discomfort despite over-the-counter pain relievers. Neurological exam was unremarkable, and there were no signs of cord compression. Pain management and radiation oncology were consulted, and palliative radiation therapy was planned. The patient also discussed advanced care planning during her stay.\",\n",
    "        \"summary\": \"A 45-year-old female with metastatic breast cancer presented with worsening back pain. Imaging showed thoracic spine fractures, and she received palliative radiation therapy.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"<SEX> M <SERVICE> CARDIOLOGY <CHIEF COMPLAINT> shortness of breath <HISTORY OF PRESENT ILLNESS> A 60-year-old male with a history of hypertension and diabetes presented with progressive shortness of breath over the past month. Echocardiogram showed reduced ejection fraction. The patient was started on diuretics and beta-blockers. Cardiology team planned for close outpatient follow-up.\",\n",
    "        \"summary\": \"\"  # Leave empty for model to infer\n",
    "    }\n",
    "]\n",
    "\n",
    "# Construct few-shot prompt\n",
    "def construct_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Summarize the following clinical note.\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nSummary: {example['summary']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Summarization function with tqdm for progress tracking\n",
    "def summarize_text(text):\n",
    "    prompt = construct_prompt(text)\n",
    "    output = summarizer(prompt, **generation_params)[0][\"generated_text\"]\n",
    "    return output.split(\"Summary:\")[-1].strip()\n",
    "\n",
    "# Apply summarization with progress bar\n",
    "tqdm.pandas(desc=\"Summarizing clinical notes\")  # Enable tqdm for pandas\n",
    "mimic_iv_bhc_100[\"generated_summary\"] = mimic_iv_bhc_100[\"input\"].progress_apply(summarize_text)\n",
    "\n",
    "# Save results\n",
    "mimic_iv_bhc_100.to_csv(\"generated_summaries_100.csv\", index=False)\n",
    "print(\"Summarization complete. Results saved to 'generated_summaries_100.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3a4c9e-f309-4a37-a451-08c69e2e91d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>target_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16002318-DS-17</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; Iodine /...</td>\n",
       "      <td>This is a ___ yo F admitted to the hospital af...</td>\n",
       "      <td>1195</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15638884-DS-4</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; Augment...</td>\n",
       "      <td>Mr. ___ is a ___ yo man with CAD with prior MI...</td>\n",
       "      <td>3496</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12435705-DS-14</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; ibuprof...</td>\n",
       "      <td>Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...</td>\n",
       "      <td>5591</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12413577-DS-4</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; OBSTETRICS/GYNECOLOGY &lt;ALLER...</td>\n",
       "      <td>On ___, Ms. ___ was admitted to the gynecology...</td>\n",
       "      <td>1119</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17967161-DS-29</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; lisinopr...</td>\n",
       "      <td>Mr. ___ underwent an angiogram on ___ which sh...</td>\n",
       "      <td>3307</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>19859533-DS-16</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; NEUROSURGERY &lt;ALLERGIES&gt; No ...</td>\n",
       "      <td>Mrs. ___ was taken to the angiography suite on...</td>\n",
       "      <td>1155</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>17018866-DS-21</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; MEDICINE &lt;ALLERGIES&gt; No Know...</td>\n",
       "      <td>SUMMARY STATEMENT ================= Ms. ___ is...</td>\n",
       "      <td>4371</td>\n",
       "      <td>1346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>12627553-DS-13</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; SURGERY &lt;ALLERGIES&gt; No Known...</td>\n",
       "      <td>Patient was admitted to the transplant surgery...</td>\n",
       "      <td>2291</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>15710368-DS-12</td>\n",
       "      <td>&lt;SEX&gt; F &lt;SERVICE&gt; ORTHOPAEDICS &lt;ALLERGIES&gt; Bex...</td>\n",
       "      <td>Ms. ___ is an ___ year old female who had a me...</td>\n",
       "      <td>2258</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>17989869-DS-18</td>\n",
       "      <td>&lt;SEX&gt; M &lt;SERVICE&gt; ORTHOPAEDICS &lt;ALLERGIES&gt; No ...</td>\n",
       "      <td>The patient presented from ___ to ___ ___ wher...</td>\n",
       "      <td>4473</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           note_id                                              input  \\\n",
       "0   16002318-DS-17  <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...   \n",
       "1    15638884-DS-4  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Augment...   \n",
       "2   12435705-DS-14  <SEX> M <SERVICE> MEDICINE <ALLERGIES> ibuprof...   \n",
       "3    12413577-DS-4  <SEX> F <SERVICE> OBSTETRICS/GYNECOLOGY <ALLER...   \n",
       "4   17967161-DS-29  <SEX> M <SERVICE> SURGERY <ALLERGIES> lisinopr...   \n",
       "..             ...                                                ...   \n",
       "95  19859533-DS-16  <SEX> F <SERVICE> NEUROSURGERY <ALLERGIES> No ...   \n",
       "96  17018866-DS-21  <SEX> F <SERVICE> MEDICINE <ALLERGIES> No Know...   \n",
       "97  12627553-DS-13  <SEX> M <SERVICE> SURGERY <ALLERGIES> No Known...   \n",
       "98  15710368-DS-12  <SEX> F <SERVICE> ORTHOPAEDICS <ALLERGIES> Bex...   \n",
       "99  17989869-DS-18  <SEX> M <SERVICE> ORTHOPAEDICS <ALLERGIES> No ...   \n",
       "\n",
       "                                               target  input_tokens  \\\n",
       "0   This is a ___ yo F admitted to the hospital af...          1195   \n",
       "1   Mr. ___ is a ___ yo man with CAD with prior MI...          3496   \n",
       "2   Mr. ___ is a ___ w/ Ph+ve ALL on dasatanib and...          5591   \n",
       "3   On ___, Ms. ___ was admitted to the gynecology...          1119   \n",
       "4   Mr. ___ underwent an angiogram on ___ which sh...          3307   \n",
       "..                                                ...           ...   \n",
       "95  Mrs. ___ was taken to the angiography suite on...          1155   \n",
       "96  SUMMARY STATEMENT ================= Ms. ___ is...          4371   \n",
       "97  Patient was admitted to the transplant surgery...          2291   \n",
       "98  Ms. ___ is an ___ year old female who had a me...          2258   \n",
       "99  The patient presented from ___ to ___ ___ wher...          4473   \n",
       "\n",
       "    target_tokens  \n",
       "0              75  \n",
       "1            1143  \n",
       "2            1098  \n",
       "3             221  \n",
       "4             439  \n",
       "..            ...  \n",
       "95            152  \n",
       "96           1346  \n",
       "97            626  \n",
       "98            291  \n",
       "99            764  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_iv_bhc_100.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf30f2b7-5d64-4920-a2b2-951c6059c661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Processing Rows: 100%|| 100/100 [01:53<00:00,  1.14s/row]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to 'generated_summaries_100.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set environment variable for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # Use FP16 for reduced memory\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Fix the padding warning\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize the pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine. She reported increasing discomfort despite over-the-counter pain relievers. Neurological exam was unremarkable, and there were no signs of cord compression. Pain management and radiation oncology were consulted, and palliative radiation therapy was planned. The patient also discussed advanced care planning during her stay.\",\n",
    "        \"target\": \"A 45-year-old female with metastatic breast cancer presented with worsening back pain. Imaging showed thoracic spine fractures, and she received palliative radiation therapy.\"\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Combine few-shot examples into a prompt template\n",
    "def construct_few_shot_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Summarize. Do not include prompt/Input in your generated summary\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,  # Slightly more restrictive sampling\n",
    "    \"temperature\": 0.7,  # Less randomness for focused outputs\n",
    "    \"top_k\": 40,  # Fewer options for next token selection\n",
    "    \"max_new_tokens\": 40,  # Limit maximum length of generated text\n",
    "    \"repetition_penalty\": 1.1  # Discourage repeating content\n",
    "}\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = mimic_iv_bhc_100[\"input\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "def clean_generated_text(generated_text):\n",
    "    # Extract only the generated summary part\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Process a batch of inputs\n",
    "def process_batch(batch):\n",
    "    try:\n",
    "        # Construct few-shot prompts for the batch\n",
    "        prompts = [construct_few_shot_prompt(input_text) for input_text in batch]\n",
    "        \n",
    "        # Generate outputs\n",
    "        outputs = summarizer(\n",
    "            prompts,\n",
    "            **generation_params,\n",
    "        )\n",
    "        return outputs if isinstance(outputs, list) else [outputs]\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"OOM error, retrying with batch size {len(batch)//2}\")\n",
    "            if len(batch) > 1:\n",
    "                half = len(batch) // 2\n",
    "                return (\n",
    "                    process_batch(batch[:half]) +\n",
    "                    process_batch(batch[half:])\n",
    "                )\n",
    "        raise e\n",
    "\n",
    "# Process with progress bar\n",
    "with tqdm(total=len(inputs), desc=\"Processing Rows\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        \n",
    "        # Process batch\n",
    "        summaries = process_batch(batch)\n",
    "        \n",
    "        # Extract and clean generated text dynamically\n",
    "        if isinstance(summaries[0], dict) and 'generated_text' in summaries[0]:\n",
    "            generated_texts = [clean_generated_text(summary['generated_text']) for summary in summaries]\n",
    "        elif isinstance(summaries[0], list) and isinstance(summaries[0][0], dict) and 'generated_text' in summaries[0][0]:\n",
    "            generated_texts = [clean_generated_text(summary[0]['generated_text']) for summary in summaries]\n",
    "        elif isinstance(summaries[0], str):\n",
    "            generated_texts = [clean_generated_text(summary) for summary in summaries]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected format for summaries.\")\n",
    "        \n",
    "        # Append cleaned summaries to results\n",
    "        generated_summaries.extend(generated_texts)\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update progress\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# Add generated summaries to DataFrame and save\n",
    "mimic_iv_bhc_100[\"generated_summary\"] = generated_summaries\n",
    "mimic_iv_bhc_100.to_csv(\"generated_summaries_100.csv\", index=False)\n",
    "print(\"Summaries saved to 'generated_summaries_100.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8f0b9-137f-4203-9bd3-18d7e20ae377",
   "metadata": {},
   "source": [
    "### Code with computational efficiency metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6820e8ca-2ff7-4a4e-b777-bffba2f1f35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Processing Rows: 100%|| 100/100 [08:48<00:00,  5.28s/row]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computational Efficiency Metrics:\n",
      "Total Input Tokens: 315831\n",
      "Total Output Tokens: 335686\n",
      "Total Time Spent: 526.26 seconds\n",
      "Average Latency (Time per Summary): 5.2736 seconds\n",
      "Average TTFT (Time to First Token): 5.2626 seconds\n",
      "Average Throughput: 1267.15 tokens/second\n",
      "Token Efficiency (TE): 1.0629\n",
      "\n",
      "Summaries saved to 'generated_summaries_with_metrics.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize summarization pipeline\n",
    "summarizer = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,  # Slightly more restrictive sampling\n",
    "    \"temperature\": 0.9,  # Less randomness for focused outputs\n",
    "    \"top_k\": 40,  # Fewer options for next token selection\n",
    "    \"max_new_tokens\": 300,  # Limit maximum length of generated text\n",
    "    \"repetition_penalty\": 1.1  # Discourage repeating content\n",
    "}\n",
    "\n",
    "# Data inputs\n",
    "inputs = mimic_iv_bhc_100[\"input\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "# Metrics tracking\n",
    "total_tokens_generated = 0\n",
    "total_input_tokens = 0\n",
    "total_time_spent = 0\n",
    "time_to_first_token = []\n",
    "throughput_list = []\n",
    "\n",
    "# Few-shot prompt constructor\n",
    "def construct_few_shot_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Please summarize the following input concisely:\\n\\n\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Input: {example['input']}\\nTarget: {example['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# Process a batch of inputs\n",
    "def process_batch(batch):\n",
    "    prompts = [construct_few_shot_prompt(input_text) for input_text in batch]\n",
    "    batch_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in prompts)\n",
    "\n",
    "    batch_start_time = time.time()  # Start timing for the batch\n",
    "    outputs = []\n",
    "\n",
    "    # Generate outputs with timing for TTFT\n",
    "    for prompt in prompts:\n",
    "        single_start_time = time.time()  # Start timing for TTFT\n",
    "        output = summarizer(prompt, **generation_params)\n",
    "        single_end_time = time.time()  # End timing for TTFT\n",
    "\n",
    "        ttft = single_end_time - single_start_time  # Time to first token\n",
    "        ttft_list.append(ttft)  # Store TTFT for this input\n",
    "        outputs.append(output)\n",
    "\n",
    "    batch_end_time = time.time()  # End timing for the batch\n",
    "    batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "    # Flatten nested outputs if necessary\n",
    "    if isinstance(outputs[0], list):\n",
    "        outputs = [item for sublist in outputs for item in sublist]\n",
    "\n",
    "    # Extract summaries and count output tokens\n",
    "    if isinstance(outputs[0], dict) and 'generated_text' in outputs[0]:\n",
    "        summaries = [output['generated_text'] for output in outputs]\n",
    "        batch_output_tokens = sum(len(tokenizer.encode(summary)) for summary in summaries)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected format for outputs: {outputs}\")\n",
    "\n",
    "    # Update metrics\n",
    "    global total_tokens_generated, total_input_tokens, total_time_spent, total_summaries\n",
    "    total_tokens_generated += batch_output_tokens\n",
    "    total_input_tokens += batch_input_tokens\n",
    "    total_time_spent += batch_time\n",
    "    total_summaries += len(batch)  # Count the number of summaries processed\n",
    "    time_per_input.append(batch_time / len(batch))  # Latency per input (full summary time)\n",
    "    throughput_list.append((batch_input_tokens + batch_output_tokens) / batch_time)  # Tokens/second\n",
    "\n",
    "    return summaries\n",
    "\n",
    "# Initialize metrics\n",
    "total_summaries = 0\n",
    "time_per_input = []  # Latency per summary\n",
    "ttft_list = []  # Time to first token\n",
    "throughput_list = []\n",
    "\n",
    "# Process inputs in batches\n",
    "batch_size = 8\n",
    "with tqdm(total=len(inputs), desc=\"Processing Rows\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        generated_summaries.extend(process_batch(batch))\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# Metrics Calculation\n",
    "average_latency = sum(time_per_input) / len(time_per_input)  # Average time per summary\n",
    "average_ttft = sum(ttft_list) / len(ttft_list)  # Average TTFT\n",
    "average_throughput = sum(throughput_list) / len(throughput_list)  # Average tokens per second\n",
    "token_efficiency = total_tokens_generated / total_input_tokens  # Output/Input token ratio\n",
    "\n",
    "# Print metrics\n",
    "print(f\"\\nComputational Efficiency Metrics:\")\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Total Output Tokens: {total_tokens_generated}\")\n",
    "print(f\"Total Time Spent: {total_time_spent:.2f} seconds\")\n",
    "print(f\"Average Latency (Time per Summary): {average_latency:.4f} seconds\")\n",
    "print(f\"Average TTFT (Time to First Token): {average_ttft:.4f} seconds\")\n",
    "print(f\"Average Throughput: {average_throughput:.2f} tokens/second\")\n",
    "print(f\"Token Efficiency (TE): {token_efficiency:.4f}\")\n",
    "\n",
    "# Save results\n",
    "mimic_iv_bhc_100[\"generated_summary\"] = generated_summaries\n",
    "mimic_iv_bhc_100.to_csv(\"generated_summaries_with_metrics.csv\", index=False)\n",
    "print(\"\\nSummaries saved to 'generated_summaries_with_metrics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9123090-a28f-4f06-a6b2-6b11546ec6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8e75c-ffd0-4abe-919f-0b3a6ca0f533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965c0d7-580e-4b98-aa11-36463ee06c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011ac18-7527-4cb0-b23e-eb0dc9dca337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf59c1-b9e5-42e7-8c01-90836dc22557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36524d2-58f7-4e86-b3a7-1f175ed30460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631e3cf-eb43-4e62-b993-b8844048bf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbf6bb-dcbf-4760-830a-6a6f119ad7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b11220-4f29-4c55-b678-ca92febc02af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23752dc7-c1be-44ec-9a0a-bb49408e7f17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical expert. Please summarize the following input concisely:\n",
      "\n",
      "Input: <SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain over the last two weeks. Imaging revealed compression fractures in the thoracic spine. She reported increasing discomfort despite over-the-counter pain relievers. Neurological exam was unremarkable, and there were no signs of cord compression. Pain management and radiation oncology were consulted, and palliative radiation therapy was planned. The patient also discussed advanced care planning during her stay.\n",
      "Target: A 45-year-old female with metastatic breast cancer presented with worsening back pain. Imaging showed thoracic spine fractures, and she received palliative radiation therapy.\n",
      "\n",
      "Input: <SEX> M <SERVICE> CARDIOLOGY <CHIEF COMPLAINT> chest pain <HISTORY OF PRESENT ILLNESS> A 55-year-old male presented with chest pain radiating to his left arm and jaw. He reported associated shortness of breath and nausea. Initial ECG showed ST-segment elevation in the inferior leads, and troponin levels were elevated, consistent with an acute myocardial infarction. The patient was emergently taken to the catheterization lab, where coronary angiography revealed 100% occlusion of the right coronary artery. A drug-eluting stent was successfully placed, and dual antiplatelet therapy was initiated. He was monitored in the cardiac care unit for 48 hours with no complications.\n",
      "Target: A 55-year-old male with a myocardial infarction underwent PCI with stenting of the right coronary artery. He was monitored and started on dual antiplatelet therapy.\n",
      "\n",
      "Input: <SEX> M <SERVICE> NEUROSURGERY <CHIEF COMPLAINT> headache past ten days <HISTORY OF PRESENT ILLNESS> The patient is a 60-year-old male with a history of renal cell carcinoma, presenting with progressively worsening right-sided headaches over 10 days. Imaging revealed a large intracranial mass in the right temporoparietal lobe with associated hemorrhage, necrosis, and vasogenic edema. He was started on Decadron to reduce cerebral edema and transferred for neurosurgical evaluation. On hospital day 5, the patient underwent an image-guided right craniotomy for tumor resection. Postoperative MRI showed no significant residual tumor. He recovered without complications and was discharged home on a tapering dose of Decadron, with follow-up in the brain tumor clinic.\n",
      "Target: A 60-year-old male with an intracranial mass underwent a craniotomy for tumor resection. He recovered well and was discharged with follow-up.\n",
      "\n",
      "Input: <SEX> F <SERVICE> GENERAL MEDICINE <CHIEF COMPLAINT> shortness of breath <HISTORY OF PRESENT ILLNESS> A 72-year-old female with a history of COPD presented with progressive shortness of breath over two weeks. Physical examination revealed decreased breath sounds and crackles in the left lower lobe. Imaging confirmed left lower lobe consolidation consistent with pneumonia. The patient was started on broad-spectrum antibiotics and oxygen therapy. Arterial blood gas analysis showed mild hypoxemia, which improved with oxygen supplementation. She remained afebrile throughout her stay and reported gradual improvement in symptoms. Discharge planning included antibiotics, a pulmonary follow-up, and instructions for home oxygen therapy.\n",
      "Target: A 72-year-old female with COPD was treated for pneumonia with antibiotics and oxygen therapy. She showed improvement and was discharged with follow-up.\n",
      "\n",
      "Input: <SEX> M <SERVICE> NEUROSURGERY <ALLERGIES> Patient recorded as having No Known Allergies to Drugs <ATTENDING> ___. <CHIEF COMPLAINT> headache past ten days <MAJOR SURGICAL OR INVASIVE PROCEDURE> Image guided right craniotomy for tumor resection <HISTORY OF PRESENT ILLNESS> Mr. ___ is a ___ y/o male who was in good health until 10 days ago, when he began having gradually worsening headaches. These were right-sided and throbbing and initially controlled with over the counter analgesics, but are now refractory to these. He has had no visual changes in association, no N/V or drowsiness. He does not appreciate worsening with cough/strain or sneezing. The patient denies other difficulties, such as weakness, numbness/tingling, visual loss or diplopia, speech abnormalities, gait difficulties, vertigo, dysarthria or dysphagia. He was taken to an outside hospital today, where head CT revealed an intracranial mass lesion. He was given decadron and sent to ___ ER. <PAST MEDICAL HISTORY> renal cell carcinoma s/p L nephrectomy in ___, no xrt/chemo <SOCIAL HISTORY> ___ <FAMILY HISTORY> negative for past malignancies <PHYSICAL EXAM> VS 97.9 60 150/100 12 97% Gen Awake, cooperative, NAD HEENT NC/AT, no scleral icterus noted, MMM, no lesions noted in oropharynx Neck Supple, no carotid bruits appreciated. No nuchal rigidity Lungs CTA bilaterally CV RRR, nl S1S2, no M/R/G noted Abd soft, NT/ND, normoactive bowel sounds, no masses or organomegaly noted Ext No C/C/E b/l Skin no rashes or lesions noted NEURO EXAM MS ___, alert and oriented x 3. Speech fluent, with normal naming, reading, comprehension and repetition. Able to follow both midline and appendicular commands. No apraxia. No dysarthria. CN I: not tested CN II: Visual fields were full to confrontation, but with extinction on the left to double simultaneous stimulation. Pupils 4->2 b/l. CN III, IV, VI: EOMI no nystagmus or diplopia CN V: intact to LT throughout; CN VII: slight L NLF flattening and asymmetry CN VIII: hearing intact to FR b/l CN IX, X: palate rises symmetrically CN XI: shrug ___ and symmetric CN XII: tongue midline and agile Motor Normal bulk and tone. No pronator drift D B T WE FE FF IP Q H DF PF TE L ___ ___ Sensory intact to light touch, pinprick, joint position sense, vibration throughout. No extinction to double simultaneous stimulation. Reflexes Br Bi Tri Pat Ach Toes L ___ 2 1 down R ___ 2 1 down Coordination Fine finger movements, rapid alternating movements, finger-to-nose, and heel-to-shin were all normal <PERTINENT RESULTS> ___ 04: 30PM ___ PTT-26.8 ___ ___ 04: 30PM PLT COUNT-156 ___ 04: 30PM NEUTS-87.7* LYMPHS-10.9* MONOS-1.0* EOS-0.3 BASOS-0.1 ___ 04: 30PM WBC-6.7 RBC-5.41 HGB-15.8 HCT-47.0 MCV-87 MCH-29.2 MCHC-33.7 RDW-13.2 ___ 04: 30PM estGFR-Using this ___ 04: 30PM GLUCOSE-107* UREA N-19 CREAT-1.4* SODIUM-140 POTASSIUM-4.4 CHLORIDE-106 TOTAL CO2-25 ANION GAP-13 MR HEAD W & W/O CONTRAST ___ 5: 37 AM MR HEAD W & W/O CONTRAST Reason: evaluate lesion Contrast: MAGNEVIST UNDERLYING MEDICAL CONDITION: ___ year old man with intracranial mass lesion REASON FOR THIS EXAMINATION: evaluate lesion CONTRAINDICATIONS for IV CONTRAST: None. INDICATION: ___ mid patient, with endocranial mass lesion, to evaluate lesion. PRIOR STUDIES: CT of the head done on ___. TECHNIQUE: Multiplanar T1- and T2-weighted imaging of the head was performed without and with IV contrast. FINDINGS: There is a large heterogeneous mass lesion, in the right temporoparietal lobes, with areas of hemorrhage, cystic change, necrosis and large solid enhancing component. The solid competent measures 5.0 x 4.3 cm. There is entrapment of the right temporal horn. There is surrounding significant vasogenic edema, with a leftward shift of subfalcine herniation; measuring approximately 1 cm is unchanged, allowing for technical differences, compared to the prior CT. Mass effect on the right lateral ventricle, is unchanged. There is a 1.7 x 1.6 cm round focus of increased signal on the FLAIR, with enhancement on the post-contrast images in the left parapharyngeal space, which corresponds to an abnormally enlarged lymph node, on correlation with the CT angiogram performed on the same day (series 9, image 1), however, this is not completely included on our present study. IMPRESSION: 1. Large heterogeneous mass lesion in the right parietal and temporal lobes, with cystic, necrotic, hemorrhagic and solid components, the solid competent measuring 5.0 cm, with significant surrounding edema, mass effect and subfalcine herniation, and entrapment of the right temporal horn without significant change since the CT head done the day before. 2. 1.7-cm enhancing focus in the left parapharyngeal region, representing an abnormally enlarged lymph node, representing metastatic involvement. However, this is not completely included on our present study. MR HEAD W & W/O CONTRAST ___ 2: 41 ___ MR HEAD W & W/O CONTRAST Reason: residual mass? Contrast: MAGNEVIST UNDERLYING MEDICAL CONDITION: ___ year old man with parietal mass s/p resection REASON FOR THIS EXAMINATION: residual mass? CONTRAINDICATIONS for IV CONTRAST: None. GADOLINIUM-ENHANCED MR SCAN OF THE BRAIN HISTORY: Status post resection of right parietal mass. Assess for residual tumor. TECHNIQUE: Multiplanar T1- and T2-weighted brain imaging was obtained pre- and post-gadolinium administration. COMPARISON STUDY: MR scan of the brain from ___. FINDINGS: There is a lace-like pattern of enhancement along the anteromedial aspect of the right temporal lobe, apparently surrounding the right temporal horn body. There is persistent mild dilatation of the right temporal horn tip, but substantially reduced in extent compared to the prior preoperative study. Additionally, there is presumed hemorrhage along the operative tract, more peripherally situated within the posterior aspect of the right temporal lobe. The extensive edema surrounding the formerly very large tumor appears unaltered in extent. However, overall, there is somewhat less mass effect, though there is still leftward subfalcine herniation present. Numerous tiny areas of elevated T2 signal are seen within the white matter of both cerebral hemispheres, presumably representing chronic small vessel infarctions or post- inflammatory residua. There is soft tissue swelling, subgaleal in locale at the craniotomy site and there is a possible fluid collection or surgical material within the crescent- shaped space between the dura spanning the craniotomy flap and the inner table of the flap itself. There is redemonstration of the well- defined, rounded 21 mm area of contrast enhancement in the left parapharyngeal fat area. Its sharp margination seems most consistent with a benign neoplastic process (question neurogenic tumor). This location would be very unusual for a lymph node or metastatic disease, as was suggested on the previous report. CONCLUSION: Findings suggest that there is residual tumor in the region of the right temporal lobe surrounding the right temporal horn, with mild residual entrapment of this portion of the right lateral ventricle, as described above. Pathology pending at time of discharge <MEDICATIONS ON ADMISSION> None <DISCHARGE MEDICATIONS> 1. Acetaminophen 325 mg Tablet Sig: ___ Tablets PO Q6H (every 6 hours) as needed for pain or fever. 2. Atenolol 25 mg Tablet Sig: 0.5 Tablet PO BID (2 times a day). Disp: *30 Tablet(s)* Refills: *0* 3. Phenytoin Sodium Extended 100 mg Capsule Sig: One (1) Capsule PO TID (3 times a day). Disp: *90 Capsule(s)* Refills: *0* 4. Docusate Sodium 100 mg Capsule Sig: One (1) Capsule PO BID (2 times a day). Disp: *60 Capsule(s)* Refills: *0* 5. Dexamethasone 2 mg Tablet Sig: One (1) Tablet PO tid (). Disp: *90 Tablet(s)* Refills: *0* <DISCHARGE DISPOSITION> Home <DISCHARGE DIAGNOSIS> Brain tumor <DISCHARGE CONDITION> Stable <FOLLOWUP INSTRUCTIONS> ___ <DISCHARGE INSTRUCTIONS> DISCHARGE INSTRUCTIONS FOR CRANIOTOMY/HEAD INJURY -Have a family member check your incision daily for signs of infection -Take your pain medicine as prescribed -Exercise should be limited to walking; no lifting, straining, excessive bending -You may wash your hair only after sutures and/or staples have been removed -You may shower before this time with assistance and use of a shower cap -Increase your intake of fluids and fiber as pain medicine (narcotics) can cause constipation -Unless directed by your doctor, do not take any anti-inflammatory medicines such as Motrin, aspirin, Advil, Ibuprofen etc. -If you have been prescribed an anti-seizure medicine, take it as prescribed and follow up with laboratory blood drawing as ordered -Clearance to drive and return to work will be addressed at your post-operative office visit CALL YOUR SURGEON IMMEDIATELY IF YOU EXPERIENCE ANY OF THE FOLLOWING: -New onset of tremors or seizures -Any confusion or change in mental status -Any numbness, tingling, weakness in your extremities -Pain or headache that is continually increasing or not relieved by pain medication -Any signs of infection at the wound site: redness, swelling, tenderness, drainage -Fever greater than or equal to 101 F \n",
      "Summary: \n",
      "\n",
      "The patient's chief complaints include worsening back pain, chest pain, headache, and neurological issues. Radiological investigations showed various findings, including metastases, lymphadenopathy, and tumors. Treatment plans involved palliative radiation therapy, surgery for a parietal mass lesion, chemotherapy for renal cell carcinoma, and endoscopic procedures for brain tumors. The patient experienced significant side effects from medications, including acetaminophen overdose, and required close monitoring for infections and seizures. The patient was discharged with clearances to resume driving and work activities.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Prevent truncation of long text\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping of content\n",
    "\n",
    "print(mimic_iv_bhc_100['generated_summary'].iloc[9])  # Replace 9 with the desired row index\n",
    "  # Remember: Index starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9dbc10-e1b2-4226-b7f6-3e148508c78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
