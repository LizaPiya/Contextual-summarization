{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b35428-19bc-4fde-bfb2-0836285be95d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id                                              input  \\\n",
      "0  16002318-DS-17  <SEX> F <SERVICE> SURGERY <ALLERGIES> Iodine /...   \n",
      "1   15638884-DS-4  <SEX> M <SERVICE> MEDICINE <ALLERGIES> Augment...   \n",
      "\n",
      "                                              target  input_tokens  \\\n",
      "0  This is a ___ yo F admitted to the hospital af...          1195   \n",
      "1  Mr. ___ is a ___ yo man with CAD with prior MI...          3496   \n",
      "\n",
      "   target_tokens                                  generated_summary  \n",
      "0             75  A 60-year-old male with a history of hypertens...  \n",
      "1           1143  A 45-year-old female with metastatic breast ca...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved CSV file\n",
    "generated_summaries = pd.read_csv(\"generated_summaries_100.csv\")\n",
    "\n",
    "# Verify the data\n",
    "print(generated_summaries.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890d4d40-0ba0-40e6-aa94-a873baedb542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a random sample of 25 rows\n",
    "sampled_20 = generated_summaries.sample(n=20, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ee1274-bb78-45c2-be62-570c0ccc4864",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.30.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28c67b4-a97a-4b0f-b54c-9972eaa55345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fd0ac5-81de-4b3a-a094-19cd87eab6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Hugging Face Transformers\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q transformers\n",
    "!pip install -q sacremoses\n",
    "!pip install -q bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619bff8d-542f-457f-a598-eeb1fc674dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70ac923ae0a447b8b588bb67a5edb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f81f5d43104721a8daa196abc21a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87448c9c0b14f05a8e5e5f2dc8bd9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60e5911491849fe960e7bf00c12bc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2248a2cadaa34501909dfb52a1d4694a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc29e6edeee74bf2965c430a90a7ab1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9c2f17fa8f4ce3b7edc8bcdd98e3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ade0bda3ae4eb19cbafde3516983af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Load your sampled dataset\n",
    "# ---------------------------------------\n",
    "df = sampled_20.copy()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Load Gemma 3 1B Instruction-Tuned\n",
    "# ---------------------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70ef021-e246-40e2-bc6b-cbe5606be162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [13:08<00:00, 39.43s/it]\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(reference, generated):\n",
    "    return f\"\"\"\n",
    "You are a clinical expert. Evaluate the quality of the generated summary by comparing it to the reference summary. Rate each criterion from 1 (poor) to 5 (excellent):\n",
    "\n",
    "1. Main Idea Retention: Does the generated summary capture the main ideas of the reference?\n",
    "2. Coherence: Is the generated summary logically and grammatically coherent?\n",
    "3. Factual Consistency: Is the generated summary factually correct compared to the reference?\n",
    "\n",
    "---\n",
    "\n",
    "Reference Summary:\n",
    "{reference}\n",
    "\n",
    "Generated Summary:\n",
    "{generated}\n",
    "\n",
    "Respond in this format:\n",
    "Main Idea Retention: #\n",
    "Coherence: #\n",
    "Factual Consistency: #\n",
    "Overall Comments: ...\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Inference loop\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    prompt = build_prompt(row[\"target\"], row[\"generated_summary\"])\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Parse numeric scores using regex\n",
    "    matches = re.findall(r'(?i)(Main Idea Retention|Coherence|Factual Consistency):\\s*(\\d)', decoded_output)\n",
    "    scores = {k.strip().lower().replace(\" \", \"_\"): int(v) for k, v in matches}\n",
    "    \n",
    "    # Append the original fields and scores\n",
    "    scores.update({\n",
    "        \"note_id\": row.get(\"note_id\", \"\"),\n",
    "        \"input\": row[\"input\"],\n",
    "        \"target\": row[\"target\"],\n",
    "        \"generated_summary\": row[\"generated_summary\"]\n",
    "    })\n",
    "    results.append(scores)\n",
    "\n",
    "# Convert to DataFrame\n",
    "judge_df = pd.DataFrame(results)\n",
    "\n",
    "# Add average score column\n",
    "judge_df[\"avg_score\"] = judge_df[\n",
    "    [\"main_idea_retention\", \"coherence\", \"factual_consistency\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "# Save results if needed\n",
    "judge_df.to_csv(\"llm_judge_scores_gemma.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f0c25-a880-4c1f-83d6-eb08c9c6ea2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "judge_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a6f52-52af-4de0-af4b-f15b40f9e7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add dataset and model labels appropriately\n",
    "judge_df[\"dataset\"] = \"MIMIC-BHC\"\n",
    "judge_df[\"model\"] = \"LLaMA 3.2\"\n",
    "\n",
    "# Formatting function\n",
    "def fmt(x):\n",
    "    return f\"{x.mean():.2f} $\\\\pm$ {x.std():.2f}\"\n",
    "\n",
    "# LaTeX row printer\n",
    "def print_latex_row(dataset, model, df):\n",
    "    mi = fmt(df[\"main_idea_retention\"])\n",
    "    co = fmt(df[\"coherence\"])\n",
    "    fa = fmt(df[\"factual_consistency\"])\n",
    "    avg = fmt(df[\"avg_score\"])\n",
    "    model_fmt = f\"\\\\textbf{{{model}}}\" if \"ConTextual\" in model else model\n",
    "    print(f\"{dataset} & {model_fmt} & {mi} & {co} & {fa} & {avg} \\\\\\\\\")\n",
    "\n",
    "# ðŸŸ¢ Print LaTeX row for MIMIC-BHC, LLaMA 3.2\n",
    "print_latex_row(\"MIMIC-BHC\", \"LLaMA 3.2\", judge_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e328bfc-f9db-49e7-aed2-e5018bd29209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Load your sampled dataset\n",
    "# ---------------------------------------\n",
    "df = sampled_20.copy()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Load Gemma 3 1B Instruction-Tuned\n",
    "# ---------------------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Define prompt with clearer instructions\n",
    "# ---------------------------------------\n",
    "def create_prompt(input_text, reference_summary, generated_summary):\n",
    "    # Gemma chat format according to official docs\n",
    "    return f\"\"\"<start_of_turn>user\n",
    "You are a clinical NLP evaluation assistant. Please evaluate this medical summary.\n",
    "\n",
    "Reference Summary: {reference_summary}\n",
    "\n",
    "Generated Summary: {generated_summary}\n",
    "\n",
    "Evaluate based on:\n",
    "1. Main ideas captured (Yes/No)\n",
    "2. Coherence (Yes/No) \n",
    "3. Issues/inaccuracies\n",
    "4. Overall score (1-5)\n",
    "\n",
    "Format your response EXACTLY like:\n",
    "- Captures main ideas: Yes\n",
    "- Coherence: Yes  \n",
    "- Issues: None\n",
    "- Score: 4\n",
    "<end_of_turn>\n",
    "\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Evaluate single example to debug\n",
    "# ---------------------------------------\n",
    "print(\"===== DEBUGGING WITH SINGLE EXAMPLE =====\")\n",
    "\n",
    "# First get a sample with a non-empty generated summary\n",
    "test_row = None\n",
    "for i, row in df.iterrows():\n",
    "    if not pd.isna(row['generated_summary']) and row['generated_summary'].strip() != \"\":\n",
    "        test_row = row\n",
    "        break\n",
    "\n",
    "if test_row is not None:\n",
    "    print(\"\\nTEST EXAMPLE:\")\n",
    "    print(f\"Reference: {test_row['target'][:100]}...\")\n",
    "    print(f\"Generated: {test_row['generated_summary'][:100]}...\")\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = create_prompt(test_row['input'], test_row['target'], test_row['generated_summary'])\n",
    "    \n",
    "    # Print the prompt for debugging\n",
    "    print(\"\\nPROMPT SENT TO MODEL:\")\n",
    "    print(prompt[:500] + \"...\")\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=150,\n",
    "        temperature=0.1,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    \n",
    "    # Get full response including prompt\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\nFULL RESPONSE FROM MODEL:\")\n",
    "    print(full_response)\n",
    "    \n",
    "    # Try to extract the model's response portion\n",
    "    cleaned_response = full_response\n",
    "    \n",
    "    # Try various cleaning methods\n",
    "    print(\"\\nTRYING DIFFERENT EXTRACTION METHODS:\")\n",
    "    \n",
    "    # Method 1: Split by model tag\n",
    "    if \"<start_of_turn>model\" in full_response:\n",
    "        method1 = full_response.split(\"<start_of_turn>model\", 1)[1].strip()\n",
    "        print(\"\\nMethod 1 (Split by model tag):\")\n",
    "        print(method1)\n",
    "    else:\n",
    "        print(\"\\nMethod 1 failed: No model tag in response\")\n",
    "    \n",
    "    # Method 2: Look for first pattern match\n",
    "    method2_match = re.search(r'[Cc]aptures main ideas:\\s*(\\w+)', full_response)\n",
    "    if method2_match:\n",
    "        start_idx = method2_match.start()\n",
    "        method2 = full_response[start_idx:]\n",
    "        print(\"\\nMethod 2 (First pattern match):\")\n",
    "        print(method2)\n",
    "    else:\n",
    "        print(\"\\nMethod 2 failed: No 'Captures main ideas:' in response\")\n",
    "    \n",
    "    # Method 3: Extract after user turn\n",
    "    if \"<end_of_turn>\" in full_response:\n",
    "        parts = full_response.split(\"<end_of_turn>\")\n",
    "        if len(parts) > 1:\n",
    "            method3 = parts[1].strip()\n",
    "            print(\"\\nMethod 3 (After user turn):\")\n",
    "            print(method3)\n",
    "        else:\n",
    "            print(\"\\nMethod 3 failed: No content after end_of_turn\")\n",
    "    else:\n",
    "        print(\"\\nMethod 3 failed: No end_of_turn in response\")\n",
    "    \n",
    "    # Apply all regex patterns to entire response\n",
    "    print(\"\\nREGEX PATTERN MATCHING ON FULL RESPONSE:\")\n",
    "    \n",
    "    main_idea_match = re.search(r'[Cc]aptures main ideas:\\s*(\\w+)', full_response)\n",
    "    coherence_match = re.search(r'[Cc]oherence:\\s*(\\w+)', full_response)\n",
    "    issues_match = re.search(r'[Ii]ssues:\\s*(.+?)(?:\\n|$)', full_response)\n",
    "    score_match = re.search(r'[Ss]core:\\s*(\\d+)', full_response)\n",
    "    \n",
    "    if main_idea_match:\n",
    "        print(f\"Found main ideas: '{main_idea_match.group(1)}'\")\n",
    "        print(f\"  Is 'yes'? {main_idea_match.group(1).lower() == 'yes'}\")\n",
    "    else:\n",
    "        print(\"No main ideas match found\")\n",
    "        \n",
    "    if coherence_match:\n",
    "        print(f\"Found coherence: '{coherence_match.group(1)}'\")\n",
    "        print(f\"  Is 'yes'? {coherence_match.group(1).lower() == 'yes'}\")\n",
    "    else:\n",
    "        print(\"No coherence match found\")\n",
    "        \n",
    "    if issues_match:\n",
    "        print(f\"Found issues: '{issues_match.group(1)}'\")\n",
    "        print(f\"  Contains 'none'? {'none' in issues_match.group(1).lower()}\")\n",
    "    else:\n",
    "        print(\"No issues match found\")\n",
    "        \n",
    "    if score_match:\n",
    "        print(f\"Found score: '{score_match.group(1)}'\")\n",
    "        try:\n",
    "            score = int(score_match.group(1))\n",
    "            print(f\"  Parsed score: {score}\")\n",
    "        except:\n",
    "            print(\"  Could not parse score as integer\")\n",
    "    else:\n",
    "        print(\"No score match found\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5. Run the full evaluation with more debugging\n",
    "# ---------------------------------------\n",
    "print(\"\\n===== RUNNING FULL EVALUATION WITH MORE DEBUGGING =====\")\n",
    "results = []\n",
    "parsed_results = {\n",
    "    'main_idea': [],\n",
    "    'coherence': [],\n",
    "    'factuality': [],\n",
    "    'score': []\n",
    "}\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Evaluating summaries\"):\n",
    "    try:\n",
    "        # Skip rows with missing summaries\n",
    "        if pd.isna(row['generated_summary']) or row['generated_summary'].strip() == \"\":\n",
    "            print(f\"Skipping row {index} - empty generated summary\")\n",
    "            results.append(\"EMPTY SUMMARY\")\n",
    "            parsed_results['main_idea'].append(False)\n",
    "            parsed_results['coherence'].append(False)\n",
    "            parsed_results['factuality'].append(False)\n",
    "            parsed_results['score'].append(0)\n",
    "            continue\n",
    "            \n",
    "        # Create the prompt\n",
    "        prompt = create_prompt(row['input'], row['target'], row['generated_summary'])\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=150,\n",
    "            temperature=0.1,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        \n",
    "        # Get the full response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        results.append(full_response)  # Store full response for debugging\n",
    "        \n",
    "        # Apply all regex patterns to entire response\n",
    "        main_idea_match = re.search(r'[Cc]aptures main ideas:\\s*(\\w+)', full_response)\n",
    "        coherence_match = re.search(r'[Cc]oherence:\\s*(\\w+)', full_response)\n",
    "        issues_match = re.search(r'[Ii]ssues:\\s*(.+?)(?:\\n|$)', full_response)\n",
    "        score_match = re.search(r'[Ss]core:\\s*(\\d+)', full_response)\n",
    "        \n",
    "        # Print pattern matches for first few examples\n",
    "        if index < 3:\n",
    "            print(f\"\\nResponse patterns for example {index}:\")\n",
    "            print(f\"Main ideas: {main_idea_match.group(0) if main_idea_match else 'Not found'}\")\n",
    "            print(f\"Coherence: {coherence_match.group(0) if coherence_match else 'Not found'}\")\n",
    "            print(f\"Issues: {issues_match.group(0) if issues_match else 'Not found'}\")\n",
    "            print(f\"Score: {score_match.group(0) if score_match else 'Not found'}\")\n",
    "        \n",
    "        # Extract values with better fallbacks\n",
    "        main_idea = False\n",
    "        if main_idea_match:\n",
    "            main_idea = 'yes' in main_idea_match.group(1).lower()\n",
    "            \n",
    "        coherence_flag = False\n",
    "        if coherence_match:\n",
    "            coherence_flag = 'yes' in coherence_match.group(1).lower()\n",
    "            \n",
    "        factual = False\n",
    "        if issues_match:\n",
    "            issues_text = issues_match.group(1).lower().strip()\n",
    "            factual = 'none' in issues_text or 'no issues' in issues_text\n",
    "            \n",
    "        score_val = 0\n",
    "        if score_match:\n",
    "            try:\n",
    "                score_val = int(score_match.group(1))\n",
    "                # Ensure score is in range 1-5\n",
    "                score_val = max(1, min(5, score_val))\n",
    "            except:\n",
    "                score_val = 0\n",
    "                \n",
    "        # Store parsed results\n",
    "        parsed_results['main_idea'].append(main_idea)\n",
    "        parsed_results['coherence'].append(coherence_flag)\n",
    "        parsed_results['factuality'].append(factual)\n",
    "        parsed_results['score'].append(score_val)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "        results.append(f\"ERROR: {str(e)}\")\n",
    "        parsed_results['main_idea'].append(False)\n",
    "        parsed_results['coherence'].append(False)\n",
    "        parsed_results['factuality'].append(False)\n",
    "        parsed_results['score'].append(0)\n",
    "    \n",
    "    # Free up GPU memory\n",
    "    if index % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Add results to the dataframe\n",
    "df['evaluation_gemma'] = results\n",
    "df['main_idea'] = parsed_results['main_idea']\n",
    "df['coherence'] = parsed_results['coherence']\n",
    "df['factuality'] = parsed_results['factuality']\n",
    "df['score'] = parsed_results['score']\n",
    "\n",
    "# ---------------------------------------\n",
    "# 6. Compute and print metrics\n",
    "# ---------------------------------------\n",
    "def pct_std(arr):\n",
    "    a = np.array(arr).astype(float)\n",
    "    return f\"{100*a.mean():.1f} Â± {100*a.std():.1f}\"\n",
    "\n",
    "def score_avg(arr):\n",
    "    a = np.array(arr).astype(float)\n",
    "    return f\"{a.mean():.2f} Â± {a.std():.2f}\"\n",
    "\n",
    "print(\"\\nðŸ“Š Table-Ready Metrics (Gemma 3 as Judge):\")\n",
    "print(\"Main Ideas:\", pct_std(df['main_idea']))\n",
    "print(\"Coherence:\", pct_std(df['coherence']))\n",
    "print(\"Factuality:\", pct_std(df['factuality']))\n",
    "print(\"Avg. Score:\", score_avg(df['score']))\n",
    "\n",
    "# Print count of successful extractions\n",
    "print(f\"\\nSuccessful extractions:\")\n",
    "print(f\"Main ideas: {sum(df['main_idea'] == True)}/{len(df)}\")\n",
    "print(f\"Coherence: {sum(df['coherence'] == True)}/{len(df)}\")\n",
    "print(f\"Factuality: {sum(df['factuality'] == True)}/{len(df)}\")\n",
    "print(f\"Non-zero scores: {sum(df['score'] > 0)}/{len(df)}\")\n",
    "\n",
    "# Save your results\n",
    "df.to_csv(\"sampled_20_evaluated_gemma3_debug.csv\", index=False)\n",
    "print(\"\\nResults saved to 'sampled_20_evaluated_gemma3_debug.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a76915-68fa-4fbc-b752-311856e25b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
