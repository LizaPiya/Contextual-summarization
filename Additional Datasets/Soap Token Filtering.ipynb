{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9ec7d-2500-4b4e-9f51-71c54f97cf20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sample = pd.read_csv(\"sample_summary.csv\")\n",
    "# Display the first few rows\n",
    "print(df_sample.head())\n",
    "\n",
    "# Check DataFrame info\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a37b01-5afd-4370-84af-f5679f589e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddd374-6093-41e9-a87c-cb48bcba545a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use token from environment variable (safer)\n",
    "login(os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b274c0-690c-4552-a148-5f032c377485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48faaacc-c64c-46f2-90ed-63d9703e54a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ecd73-41d5-4050-9329-c55396c73e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa98b4-d5eb-46a8-8708-356267d0aeb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define the CPTF Summarizer Class\n",
    "# -----------------------------\n",
    "class CPTFSummarizer:\n",
    "    def __init__(self, model: torch.nn.Module, tokenizer: AutoTokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    def calculate_importance(\n",
    "        self, \n",
    "        tokens: torch.Tensor, \n",
    "        alpha: float = 0.6\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate token importance scores using attention weights and positional bias.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(tokens, output_attentions=True)\n",
    "                attentions = outputs.attentions\n",
    "                \n",
    "                importance_scores = torch.zeros(tokens.size(-1), device=tokens.device)\n",
    "                num_layers = len(attentions)\n",
    "                \n",
    "                # Generate positional weights (linear decay from 1.0 to 0.5)\n",
    "                position_weights = torch.linspace(1.0, 0.5, steps=tokens.size(-1), device=tokens.device)\n",
    "\n",
    "                for l, layer_attention in enumerate(attentions):\n",
    "                    # Calculate layer weight with position-based scaling\n",
    "                    layer_weight = alpha + (1 - alpha) * (l + 1) / num_layers\n",
    "                    \n",
    "                    # Average attention across heads and batches\n",
    "                    avg_attention = layer_attention.mean(dim=1).squeeze()\n",
    "                    token_importance = avg_attention.mean(dim=-1)\n",
    "                    \n",
    "                    # Add positional weighting to importance scores\n",
    "                    importance_scores += layer_weight * token_importance * position_weights\n",
    "                \n",
    "                return importance_scores\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating importance scores: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def summarize(\n",
    "        self,\n",
    "        text: str,\n",
    "        retention_ratio: float = 0.8,\n",
    "        alpha: float = 0.6,\n",
    "        max_length: int = 2500\n",
    "    ) -> Tuple[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Perform Context Preserving Token Filtering to summarize text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to summarize.\n",
    "            retention_ratio: Fraction of tokens to retain.\n",
    "            alpha: Weight factor for layer importance.\n",
    "            max_length: Maximum input length.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (summarized text, importance scores).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize input text\n",
    "            tokens = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Calculate number of tokens to keep\n",
    "            n = tokens.input_ids.size(-1)\n",
    "            k = int(n * retention_ratio)\n",
    "\n",
    "            # Get importance scores and top k indices\n",
    "            importance_scores = self.calculate_importance(tokens.input_ids, alpha)\n",
    "            _, indices = torch.sort(importance_scores, descending=True)\n",
    "            keep_indices = sorted(indices[:k].tolist())\n",
    "\n",
    "            # Create reduced token sequence\n",
    "            reduced_tokens = tokens.input_ids[0][keep_indices]\n",
    "            \n",
    "            # Decode back to text\n",
    "            reduced_text = self.tokenizer.decode(reduced_tokens)\n",
    "\n",
    "            return reduced_text, importance_scores.tolist()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in summarization process: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load your dataset\n",
    "# -----------------------------\n",
    "# Replace \"sample_summary.csv\" with the filename of your dataset.\n",
    "df_sample = pd.read_csv(\"sample_summary.csv\")\n",
    "print(df_sample.head())\n",
    "print(df_sample.info())\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Initialize the Model and Tokenizer\n",
    "# -----------------------------\n",
    "# Change the model name as needed; ensure the model supports output_attentions.\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Create a CPTF Summarizer instance\n",
    "# -----------------------------\n",
    "cptf_summarizer = CPTFSummarizer(model, tokenizer)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Process the dataset with CPTF summarizer\n",
    "# -----------------------------\n",
    "# We'll iterate over each \"input\" text and generate a summary.\n",
    "cptf_summaries = []\n",
    "for text in tqdm(df_sample[\"input\"].tolist(), desc=\"Processing CPTF Summaries\"):\n",
    "    try:\n",
    "        summary, _ = cptf_summarizer.summarize(text, retention_ratio=0.7)\n",
    "        cptf_summaries.append(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        cptf_summaries.append(\"\")  # Append empty string if there's an error\n",
    "\n",
    "# Add the generated summaries to the DataFrame\n",
    "df_sample[\"cptf_summary\"] = cptf_summaries\n",
    "\n",
    "# Optionally, save the updated DataFrame to a new CSV file\n",
    "df_sample.to_csv(\"soap_cptf_generated_summaries.csv\", index=False)\n",
    "print(\"CPTF summaries saved to 'soap_cptf_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135de721-acf6-411d-959e-e0d4355b082d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_dataset_with_cptf(\n",
    "    df: pd.DataFrame,\n",
    "    summarizer: CPTFSummarizer,\n",
    "    input_column: str = \"input\",\n",
    "    retention_ratio: float = 0.8,\n",
    "    alpha: float = 0.5,\n",
    "    max_length: int = 2048\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply CPTF to a dataset and add reduced text and importance scores as new columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe with an `input` column containing text data.\n",
    "        summarizer: An instance of the CPTFSummarizer class.\n",
    "        input_column: Column name in the dataframe containing the input text.\n",
    "        retention_ratio: Fraction of tokens to retain during summarization.\n",
    "        alpha: Weight factor for layer importance.\n",
    "        max_length: Maximum input length for tokenization.\n",
    "    \n",
    "    Returns:\n",
    "        Updated dataframe with new columns: `reduced_text` and `importance_scores`.\n",
    "    \"\"\"\n",
    "    reduced_texts = []\n",
    "    importance_scores_list = []\n",
    "\n",
    "    print(\"Processing dataset with CPTF...\")\n",
    "    with tqdm(total=len(df), desc=\"Processing Dataset\", unit=\"row\") as pbar:\n",
    "        for _, row in df.iterrows():\n",
    "            text = row[input_column]\n",
    "            try:\n",
    "                # Summarize using CPTFSummarizer\n",
    "                reduced_text, importance_scores = summarizer.summarize(\n",
    "                    text,\n",
    "                    retention_ratio=retention_ratio,\n",
    "                    alpha=alpha,\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                reduced_texts.append(reduced_text)\n",
    "                importance_scores_list.append(importance_scores)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {text[:50]}... Error: {e}\")\n",
    "                reduced_texts.append(\"\")\n",
    "                importance_scores_list.append([])\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Add results as new columns to the dataframe\n",
    "    df[\"reduced_text\"] = reduced_texts\n",
    "    df[\"importance_scores\"] = importance_scores_list\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f7153-26e9-4fd6-a595-1b40c7eade0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the dataset to reduce text dynamically\n",
    "processed_df = process_dataset_with_cptf(\n",
    "    df=df_sample,\n",
    "    summarizer=CPTFSummarizer(model, tokenizer),\n",
    "    input_column=\"input\",  # The column containing input text\n",
    "    retention_ratio=0.8,  # Retain 70% of the most important tokens\n",
    "    alpha=0.5,            # Importance weighting factor\n",
    "    max_length=2048       # Max tokenization length\n",
    ")\n",
    "\n",
    "# View one reduced text\n",
    "print(processed_df[\"reduced_text\"].iloc[0])  # Replace 0 with the desired row index\n",
    "# Save the processed DataFrame to a CSV file\n",
    "processed_df.to_csv(\"soap_processed_reduced_texts.csv\", index=False)\n",
    "\n",
    "print(\"Processed DataFrame saved to 'soap_processed_reduced_texts.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9234b2-c576-4afe-b8ad-84fc9c3e21cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6285-012e-4bc1-8a4c-f2112fb99dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(processed_df['reduced_text'].iloc[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dbeed-d149-4d14-b8dd-d0bfa3cd19b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(processed_df['input'].iloc[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6db5aa-f61c-44cd-9c65-59fa7a7e9f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
