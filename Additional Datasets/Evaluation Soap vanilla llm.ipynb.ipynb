{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4202ebb2-349c-49ae-8cff-a90fb57ed79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q nltk bert-score\n",
    "!pip install -q rouge-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1201475f-ead4-4657-aae7-711d2cc7ae84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                              output  \\\n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...   \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...   \n",
      "2  Subjective:\\n- No known allergies to medicatio...   \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...   \n",
      "4  Subjective:\\n- Presenting with chest pain for ...   \n",
      "\n",
      "                                   generated_summary  \n",
      "0  The 75-year-old man has been experiencing lowe...  \n",
      "1  , but as it went along, the smell started comi...  \n",
      "2  The defendant is charged with murder in connec...  \n",
      "3  that you could have contracted something, poss...  \n",
      "4  into this further and try to find out what's g...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved CSV file\n",
    "generated_summaries_soap = pd.read_csv(\"soap_generated_summaries.csv\")\n",
    "\n",
    "# Verify the data\n",
    "print(generated_summaries_soap.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17a176-273c-4db4-b75f-e5b1777c6018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score\n",
    "from rouge_metric import PyRouge\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Text Cleaning\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Lowercase, strip spaces, split, and rejoin to normalize whitespace\n",
    "    return ' '.join(text.strip().lower().split())\n",
    "\n",
    "# -----------------------------\n",
    "# 2. BLEU Computation\n",
    "# -----------------------------\n",
    "def compute_bleu_scores(reference, candidate):\n",
    "    \"\"\"Compute BLEU-1 and BLEU-2 scores.\"\"\"\n",
    "    try:\n",
    "        smoothing_function = SmoothingFunction().method1\n",
    "        \n",
    "        # Compute BLEU-1 (unigrams)\n",
    "        bleu1 = sentence_bleu(\n",
    "            [reference.split()],\n",
    "            candidate.split(),\n",
    "            weights=(1.0, 0, 0, 0),\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "        \n",
    "        # Compute BLEU-2 (unigrams + bigrams)\n",
    "        bleu2 = sentence_bleu(\n",
    "            [reference.split()],\n",
    "            candidate.split(),\n",
    "            weights=(0.5, 0.5, 0, 0),\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "        \n",
    "        # Convert to percentages\n",
    "        return bleu1 * 100, bleu2 * 100\n",
    "    except Exception as e:\n",
    "        print(f\"BLEU Error: {e}\")\n",
    "        print(f\"Reference: '{reference[:50]}...'\")\n",
    "        print(f\"Candidate: '{candidate[:50]}...'\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ROUGE-L Computation\n",
    "# -----------------------------\n",
    "def compute_rouge_l(reference, candidate):\n",
    "    \"\"\"Compute ROUGE-L score.\"\"\"\n",
    "    rouge = PyRouge(\n",
    "        rouge_n=(1, 2),\n",
    "        rouge_l=True,\n",
    "        rouge_w=False,\n",
    "        rouge_w_weight=1.2,\n",
    "        rouge_s=False,\n",
    "        rouge_su=False,\n",
    "        skip_gap=4\n",
    "    )\n",
    "    try:\n",
    "        scores = rouge.evaluate([candidate], [[reference]])\n",
    "        return scores['rouge-l']['f'] * 100  # Convert to percentage\n",
    "    except Exception as e:\n",
    "        print(f\"ROUGE-L Error: {e}\")\n",
    "        print(f\"Reference: '{reference[:50]}...'\")\n",
    "        print(f\"Candidate: '{candidate[:50]}...'\")\n",
    "        return 0.0\n",
    "\n",
    "# -----------------------------\n",
    "# 4. BERTScore in Batches\n",
    "# -----------------------------\n",
    "def compute_bert_score_batched(references, candidates, batch_size=32):\n",
    "    \"\"\"Compute BERTScore in batches.\"\"\"\n",
    "    all_P, all_R, all_F1 = [], [], []\n",
    "    for i in range(0, len(references), batch_size):\n",
    "        batch_refs = references[i:i + batch_size]\n",
    "        batch_cands = candidates[i:i + batch_size]\n",
    "        try:\n",
    "            P, R, F1 = score(batch_cands, batch_refs, lang=\"en\", verbose=False)\n",
    "            # Convert to percentages\n",
    "            all_P.extend([p * 100 for p in P.tolist()])\n",
    "            all_R.extend([r * 100 for r in R.tolist()])\n",
    "            all_F1.extend([f * 100 for f in F1.tolist()])\n",
    "        except Exception as e:\n",
    "            print(f\"BERTScore Error in batch {i}: {e}\")\n",
    "            batch_len = len(batch_refs)\n",
    "            all_P.extend([0.0] * batch_len)\n",
    "            all_R.extend([0.0] * batch_len)\n",
    "            all_F1.extend([0.0] * batch_len)\n",
    "    return all_P, all_R, all_F1\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Main Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate_summaries(df):\n",
    "    bleu1_scores, bleu2_scores, rouge_l_scores = [], [], []\n",
    "    print(\"Computing BLEU and ROUGE-L scores...\")\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Processing Rows\", unit=\"row\") as pbar:\n",
    "        for _, row in df.iterrows():\n",
    "            # Replace 'target' with 'output' since your CSV has ground truth in 'output'\n",
    "            reference = clean_text(row['output'])\n",
    "            candidate = clean_text(row['generated_summary'])\n",
    "            \n",
    "            if not reference or not candidate:\n",
    "                print(f\"Empty text - Reference: '{reference}', Candidate: '{candidate}'\")\n",
    "                bleu1_scores.append(0.0)\n",
    "                bleu2_scores.append(0.0)\n",
    "                rouge_l_scores.append(0.0)\n",
    "            else:\n",
    "                bleu1, bleu2 = compute_bleu_scores(reference, candidate)\n",
    "                bleu1_scores.append(bleu1)\n",
    "                bleu2_scores.append(bleu2)\n",
    "                rouge_l_scores.append(compute_rouge_l(reference, candidate))\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(\"\\nComputing BERTScore...\")\n",
    "    # For BERTScore, we need lists of references and candidates\n",
    "    references = [clean_text(text) for text in df['output'].tolist()]\n",
    "    candidates = [clean_text(text) for text in df['generated_summary'].tolist()]\n",
    "    \n",
    "    bert_p, bert_r, bert_f1 = compute_bert_score_batched(references, candidates)\n",
    "    \n",
    "    # Add all scores to DataFrame\n",
    "    df['bleu1'] = bleu1_scores\n",
    "    df['bleu2'] = bleu2_scores\n",
    "    df['rouge_l'] = rouge_l_scores\n",
    "    df['bert_p'] = bert_p\n",
    "    df['bert_r'] = bert_r\n",
    "    df['bert_f1'] = bert_f1\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"\\nEvaluation Metrics (in percentages):\")\n",
    "    print(\"Average BLEU-1:\", df['bleu1'].mean(), \"%\")\n",
    "    print(\"Average BLEU-2:\", df['bleu2'].mean(), \"%\")\n",
    "    print(\"Average ROUGE-L:\", df['rouge_l'].mean(), \"%\")\n",
    "    print(\"Average BERT P:\", df['bert_p'].mean(), \"%\")\n",
    "    print(\"Average BERT R:\", df['bert_r'].mean(), \"%\")\n",
    "    print(\"Average BERT F1:\", df['bert_f1'].mean(), \"%\")\n",
    "    \n",
    "    # Print standard deviations\n",
    "    print(\"\\nStandard Deviations (in percentages):\")\n",
    "    print(\"BLEU-1 Std:\", df['bleu1'].std(), \"%\")\n",
    "    print(\"BLEU-2 Std:\", df['bleu2'].std(), \"%\")\n",
    "    print(\"ROUGE-L Std:\", df['rouge_l'].std(), \"%\")\n",
    "    print(\"BERT F1 Std:\", df['bert_f1'].std(), \"%\")\n",
    "    print(\"BERT P Std:\", df['bert_p'].std(), \"%\")  # Print standard deviation for BERT P\n",
    "    print(\"BERT R Std:\", df['bert_r'].std(), \"%\")  # Print standard deviation for BERT R\n",
    "    \n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Load CSV & Evaluate\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the CSV with 'input', 'output', 'generated_summary'\n",
    "    df_summaries = pd.read_csv(\"soap_generated_summaries.csv\")\n",
    "    print(df_summaries.head())\n",
    "\n",
    "    # Evaluate\n",
    "    results_df = evaluate_summaries(df_summaries)\n",
    "\n",
    "    # Save the DataFrame with the metrics\n",
    "    results_df.to_csv(\"soap_evaluation_results.csv\", index=False)\n",
    "    print(\"\\nResults saved to 'soap_evaluation_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0855b-0736-4c50-8614-a4e236a94f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
