{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "197dc1c5-4622-4ba5-a6b0-1b202ad621fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                              output  \\\n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...   \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...   \n",
      "2  Subjective:\\n- No known allergies to medicatio...   \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...   \n",
      "4  Subjective:\\n- Presenting with chest pain for ...   \n",
      "\n",
      "                                        cptf_summary  \\\n",
      "0  <|begin_of_text|>Good afternoon, champ, how yo...   \n",
      "1  <|begin_of_text|>What brings you in here today...   \n",
      "2  <|begin_of_text|>Do you have any known allergi...   \n",
      "3  <|begin_of_text|>How may I help you today? Yea...   \n",
      "4  <|begin_of_text|>It sounds like that you're ex...   \n",
      "\n",
      "                                        reduced_text  \\\n",
      "0  <|begin_of_text|>Good afternoon, champ, how yo...   \n",
      "1  <|begin_of_text|>What brings you in here today...   \n",
      "2  <|begin_of_text|>Do you have any known allergi...   \n",
      "3  <|begin_of_text|>How may I help you today? Yea...   \n",
      "4  <|begin_of_text|>It sounds like that you're ex...   \n",
      "\n",
      "                                   importance_scores  \n",
      "0  [0.02965068817138672, 0.029625179246068, 0.029...  \n",
      "1  [0.0059814453125, 0.005979984533041716, 0.0059...  \n",
      "2  [1.02056884765625, 0.9742085337638855, 0.92784...  \n",
      "3  [0.011168479919433594, 0.011161953210830688, 0...  \n",
      "4  [0.00801396369934082, 0.008009910583496094, 0....  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   input              100 non-null    object\n",
      " 1   output             100 non-null    object\n",
      " 2   cptf_summary       100 non-null    object\n",
      " 3   reduced_text       100 non-null    object\n",
      " 4   importance_scores  100 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 4.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reduced = pd.read_csv(\"soap_processed_reduced_texts.csv\")\n",
    "# Display the first few rows\n",
    "print(reduced.head())\n",
    "\n",
    "# Check DataFrame info\n",
    "print(reduced.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61be4ebe-b654-42fd-9e8b-188e6a575ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                        reduced_text  \\\n",
      "0  <|begin_of_text|>Good afternoon, champ, how yo...   \n",
      "1  <|begin_of_text|>What brings you in here today...   \n",
      "2  <|begin_of_text|>Do you have any known allergi...   \n",
      "3  <|begin_of_text|>How may I help you today? Yea...   \n",
      "4  <|begin_of_text|>It sounds like that you're ex...   \n",
      "\n",
      "                                              output  \n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...  \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...  \n",
      "2  Subjective:\\n- No known allergies to medicatio...  \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...  \n",
      "4  Subjective:\\n- Presenting with chest pain for ...  \n"
     ]
    }
   ],
   "source": [
    "# Select only the 'input' and 'reduced_text' columns\n",
    "selected_columns = reduced[['input', 'reduced_text', 'output']]\n",
    "\n",
    "# Optional: Filter out rows where 'reduced_text' is empty or contains only whitespace\n",
    "filtered_columns = selected_columns[selected_columns['reduced_text'].str.strip() != \"\"]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(filtered_columns.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "680104f1-43c1-452d-9c79-791b2b0f1509",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66d3d24f-b715-4b24-91b0-95cbfe3b77d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8af64d-fd9f-459d-b1c4-07166d14ca29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:   0%|          | 0/100 [00:00<?, ?row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:   8%|▊         | 8/100 [00:43<08:19,  5.43s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  16%|█▌        | 16/100 [01:26<07:34,  5.41s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  24%|██▍       | 24/100 [02:13<07:08,  5.64s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  32%|███▏      | 32/100 [02:54<06:07,  5.41s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  40%|████      | 40/100 [03:40<05:32,  5.54s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  48%|████▊     | 48/100 [04:29<04:58,  5.74s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  56%|█████▌    | 56/100 [05:20<04:20,  5.93s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  64%|██████▍   | 64/100 [06:24<03:58,  6.61s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  72%|███████▏  | 72/100 [07:18<03:06,  6.65s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  80%|████████  | 80/100 [07:59<02:03,  6.19s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  88%|████████▊ | 88/100 [08:42<01:11,  5.94s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  96%|█████████▌| 96/100 [09:26<00:23,  5.80s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries: 100%|██████████| 100/100 [09:49<00:00,  5.90s/row]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to 'soap_cptf_generated_summaries.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Environment setup (optional but often helpful)\n",
    "# -----------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Load your model and tokenizer\n",
    "# -----------------------------------------------------\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example model name\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",           # Automatic GPU/CPU placement\n",
    "    torch_dtype=torch.float16     # Use FP16 for reduced memory usage\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Define a prompt construction function\n",
    "# -----------------------------------------------------\n",
    "def construct_prompt(input_text):\n",
    "    \"\"\"\n",
    "    Constructs an instruction-based prompt for summarization.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a medical expert. Summarize the following case in an excellent manner. \"\n",
    "        \"Do not include any extra or verbatim text from the input. \"\n",
    "        f\"Case:\\n{input_text}\\n\\nSummary:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Set your generation parameters\n",
    "# -----------------------------------------------------\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 20,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Load your sample DataFrame (df_sample) with columns \"input\" and \"output\"\n",
    "# -----------------------------------------------------\n",
    "df_sample = filtered_columns  # This is now your DataFrame with filtered columns\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Summarize your df_sample DataFrame using partial decoding\n",
    "# -----------------------------------------------------\n",
    "batch_size = 8  # Adjust as needed\n",
    "inputs_list = df_sample[\"reduced_text\"].tolist()  # Using the reduced_text column\n",
    "generated_summaries = []\n",
    "\n",
    "def process_batch(batch_inputs):\n",
    "    batch_generated = []\n",
    "    for text in batch_inputs:\n",
    "        prompt = construct_prompt(text)\n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1000)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Generate output tokens\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_params\n",
    "        )\n",
    "        # Slice out only the tokens that were generated after the prompt\n",
    "        generated_tokens = summary_ids[0, prompt_length:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        batch_generated.append(generated_text)\n",
    "    return batch_generated\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Process the DataFrame in batches with a progress bar\n",
    "# -----------------------------------------------------\n",
    "with tqdm(total=len(inputs_list), desc=\"Generating Summaries\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs_list), batch_size):\n",
    "        batch = inputs_list[i:i + batch_size]\n",
    "        try:\n",
    "            batch_generated = process_batch(batch)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Out of memory error; try reducing batch size.\")\n",
    "            raise e\n",
    "        generated_summaries.extend(batch_generated)\n",
    "        torch.cuda.empty_cache()\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 8. Store and Save\n",
    "# -----------------------------------------------------\n",
    "# Add generated summaries as a new column in df_sample\n",
    "df_sample[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Save the DataFrame with input, reduced_text, and generated_summary\n",
    "df_sample.to_csv(\"soap_cptf_generated_summaries.csv\", index=False)\n",
    "print(\"Summaries saved to 'soap_cptf_generated_summaries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4208b2-1c3e-440e-9021-0a0f474a440b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully: meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:   0%|          | 0/100 [00:00<?, ?row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:   8%|▊         | 8/100 [2:46:49<31:58:32, 1251.22s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  16%|█▌        | 16/100 [4:41:17<23:48:13, 1020.16s/row]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Environment setup (optional but often helpful)\n",
    "# -----------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Load your model and tokenizer\n",
    "# -----------------------------------------------------\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example model name\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",            # Automatic GPU/CPU placement\n",
    "    torch_dtype=torch.float16     # Use FP16 for reduced memory usage\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Model and tokenizer loaded successfully: {model_name}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Define a prompt construction function\n",
    "# -----------------------------------------------------\n",
    "def construct_prompt(input_text):\n",
    "    \"\"\"\n",
    "    Constructs an instruction-based prompt for summarization.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a world class medical expert. Summarize the following case in an excellent manner. \"\n",
    "        \"Do not include any extra or verbatim text from the input. \"\n",
    "        f\"Case:\\n{input_text}\\n\\nSummary:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Set your generation parameters\n",
    "# -----------------------------------------------------\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 20,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Load your sample DataFrame (df_sample) with columns \"input\" and \"reduced_text\"\n",
    "# -----------------------------------------------------\n",
    "df_sample = filtered_columns  # Assumes 'filtered_columns' is already defined\n",
    "batch_size = 8\n",
    "inputs_list = df_sample[\"reduced_text\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Define batch processing function\n",
    "# -----------------------------------------------------\n",
    "def process_batch(batch_inputs):\n",
    "    batch_generated = []\n",
    "    for text in batch_inputs:\n",
    "        prompt = construct_prompt(text)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1000)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_params\n",
    "        )\n",
    "        generated_tokens = summary_ids[0, prompt_length:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        batch_generated.append(generated_text)\n",
    "    return batch_generated\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Generate summaries with performance measurement\n",
    "# -----------------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=len(inputs_list), desc=\"Generating Summaries\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs_list), batch_size):\n",
    "        batch = inputs_list[i:i + batch_size]\n",
    "        try:\n",
    "            batch_generated = process_batch(batch)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Out of memory error; try reducing batch size.\")\n",
    "            raise e\n",
    "        generated_summaries.extend(batch_generated)\n",
    "        torch.cuda.empty_cache()\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_samples = len(inputs_list)\n",
    "throughput = total_samples / total_time\n",
    "latency = total_time / total_samples\n",
    "\n",
    "print(f\"\\n--- Inference Performance Metrics ---\")\n",
    "print(f\"Total examples processed: {total_samples}\")\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "print(f\"Average latency per example: {latency:.4f} seconds\")\n",
    "print(f\"Throughput: {throughput:.2f} examples/second\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 8. Store and Save\n",
    "# -----------------------------------------------------\n",
    "df_sample[\"generated_summary\"] = generated_summaries\n",
    "df_sample.to_csv(\"soap_cptf_generated_summaries.csv\", index=False)\n",
    "print(\"Summaries saved to 'soap_cptf_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17607725-a154-4b43-acc3-eb4e90ad9abf",
   "metadata": {},
   "source": [
    "### Efficiency Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28e2272e-2c4d-4dba-b128-60eaf17e5514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Environment setup (optional but often helpful)\n",
    "# -----------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3886d737-05fb-4208-9a0a-22301ebbd1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully: meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 2. Load your model and tokenizer\n",
    "# -----------------------------------------------------\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example model name\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",           # Automatic GPU/CPU placement\n",
    "    torch_dtype=torch.float16     # Use FP16 for reduced memory usage\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Print confirmation that model and tokenizer are loaded\n",
    "print(f\"Model and tokenizer loaded successfully: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "727a0055-5a24-4ba7-8290-4d0e784da993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# 3. Define a prompt construction function\n",
    "# -----------------------------------------------------\n",
    "def construct_prompt(input_text):\n",
    "    \"\"\"\n",
    "    Constructs an instruction-based prompt for summarization.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a world class medical expert. Summarize the following case in an excellent manner. \"\n",
    "        \"Do not include any extra or verbatim text from the input. \"\n",
    "        f\"Case:\\n{input_text}\\n\\nSummary:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b98f43c-a7e3-4456-a0c8-7e63b1c7bb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# 4. Set your generation parameters\n",
    "# -----------------------------------------------------\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 20,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb4ea4de-1a9b-4996-a46c-2f2e3310a96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# 5. Load your sample DataFrame (df_sample) with columns \"input\" and \"output\"\n",
    "# -----------------------------------------------------\n",
    "df_sample = filtered_columns  # This is now your DataFrame with filtered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e527de6-8884-40d8-a000-aabf4135b0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# 6. Initialize latency and throughput tracking\n",
    "# -----------------------------------------------------\n",
    "latency_list = []\n",
    "throughput_list = []\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Summarize your df_sample DataFrame using partial decoding\n",
    "# -----------------------------------------------------\n",
    "batch_size = 4  # Reduced from 8 to avoid memory issues\n",
    "inputs_list = df_sample[\"reduced_text\"].tolist()\n",
    "generated_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45f8c540-8b6f-4d07-b48a-a8dac57aebbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch(batch_inputs):\n",
    "    batch_generated = []\n",
    "    \n",
    "    for text in batch_inputs:\n",
    "        prompt = construct_prompt(text)\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1000)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "        input_token_count = prompt_length\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate output tokens\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_params\n",
    "        )\n",
    "        \n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        generated_tokens = summary_ids[0, prompt_length:]\n",
    "        output_token_count = len(generated_tokens)\n",
    "        total_token_count = input_token_count + output_token_count\n",
    "        \n",
    "        # Latency in seconds\n",
    "        latency = end_time - start_time\n",
    "        latency_list.append(latency)\n",
    "        \n",
    "        # Throughput in tokens per second\n",
    "        throughput = total_token_count / latency if latency > 0 else 0\n",
    "        throughput_list.append(throughput)\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        batch_generated.append(generated_text)\n",
    "        \n",
    "        # Print progress for each item to ensure script is running\n",
    "        print(f\"Processed item: Latency={latency:.2f}s, Throughput={throughput:.2f} tokens/s\")\n",
    "        \n",
    "    return batch_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc086d7-c1d6-4ef3-a87d-f360af1f9a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# 8. Process the DataFrame in batches with a progress bar\n",
    "# -----------------------------------------------------\n",
    "print(f\"Starting to process {len(inputs_list)} items with batch size {batch_size}\")\n",
    "\n",
    "with tqdm(total=len(inputs_list), desc=\"Generating Summaries\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs_list), batch_size):\n",
    "        batch = inputs_list[i:i + batch_size]\n",
    "        batch_size_actual = len(batch)\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(inputs_list) + batch_size - 1)//batch_size}, size={batch_size_actual}\")\n",
    "        \n",
    "        try:\n",
    "            batch_generated = process_batch(batch)\n",
    "            generated_summaries.extend(batch_generated)\n",
    "            print(f\"Batch {i//batch_size + 1} completed successfully\")\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"Out of memory error in batch {i//batch_size + 1}; try reducing batch size.\")\n",
    "                # Try again with smaller batch\n",
    "                for single_text in batch:\n",
    "                    try:\n",
    "                        result = process_batch([single_text])\n",
    "                        generated_summaries.extend(result)\n",
    "                        print(\"Processed single item successfully after batch failure\")\n",
    "                    except Exception as inner_e:\n",
    "                        print(f\"Error processing single item: {inner_e}\")\n",
    "                        generated_summaries.append(\"Error generating summary\")\n",
    "            else:\n",
    "                print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "                # Add placeholders for failed batch\n",
    "                generated_summaries.extend([\"Error generating summary\"] * len(batch))\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        pbar.update(batch_size_actual)\n",
    "        print(f\"Progress: {min(i + batch_size, len(inputs_list))}/{len(inputs_list)} items processed\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 9. Calculate and print metrics\n",
    "# -----------------------------------------------------\n",
    "if latency_list:\n",
    "    mean_latency = np.mean(latency_list)\n",
    "    std_latency = np.std(latency_list)\n",
    "    mean_throughput = np.mean(throughput_list)\n",
    "    std_throughput = np.std(throughput_list)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nComputational Efficiency Metrics:\")\n",
    "    print(f\"Average Latency (Time per Summary): {mean_latency:.2f} ± {std_latency:.2f} seconds\")\n",
    "    print(f\"Average Throughput: {mean_throughput:.2f} ± {std_throughput:.2f} tokens/second\")\n",
    "\n",
    "    # Print formatted for LaTeX table\n",
    "    print(\"\\nFor LaTeX Table:\")\n",
    "    print(f\"${mean_throughput:.2f} \\\\pm {std_throughput:.2f}$ & ${mean_latency:.2f} \\\\pm {std_latency:.2f}$ \\\\\\\\\")\n",
    "\n",
    "    # Save metrics to a separate file\n",
    "    with open(\"cptf_soap_efficiency_metrics.txt\", \"w\") as f:\n",
    "        f.write(f\"Model: {model_name} with CPTF for SOAP\\n\")\n",
    "        f.write(f\"Parameters: Token=300, Temp=0.2\\n\")\n",
    "        f.write(f\"Throughput: {mean_throughput:.2f} ± {std_throughput:.2f} tokens/second\\n\")\n",
    "        f.write(f\"Latency: {mean_latency:.2f} ± {std_latency:.2f} seconds\\n\")\n",
    "        f.write(f\"\\nFor LaTeX Table:\\n\")\n",
    "        f.write(f\"${mean_throughput:.2f} \\\\pm {std_throughput:.2f}$ & ${mean_latency:.2f} \\\\pm {std_latency:.2f}$ \\\\\\\\\\n\")\n",
    "else:\n",
    "    print(\"No metrics were collected.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 10. Store and Save\n",
    "# -----------------------------------------------------\n",
    "# Make sure the lengths match\n",
    "if len(generated_summaries) < len(df_sample):\n",
    "    print(f\"Warning: Generated {len(generated_summaries)} summaries but dataframe has {len(df_sample)} rows\")\n",
    "    # Pad with error messages if needed\n",
    "    generated_summaries.extend([\"Error generating summary\"] * (len(df_sample) - len(generated_summaries)))\n",
    "elif len(generated_summaries) > len(df_sample):\n",
    "    print(f\"Warning: Generated {len(generated_summaries)} summaries but dataframe has {len(df_sample)} rows\")\n",
    "    generated_summaries = generated_summaries[:len(df_sample)]\n",
    "\n",
    "# Add generated summaries as a new column in df_sample\n",
    "df_sample[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Save the DataFrame with input, reduced_text, and generated_summary\n",
    "df_sample.to_csv(\"soap_cptf_generated_summaries.csv\", index=False)\n",
    "print(\"Summaries saved to 'soap_cptf_generated_summaries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64a31c-7959-4a73-97f4-0b4c8a51f405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
