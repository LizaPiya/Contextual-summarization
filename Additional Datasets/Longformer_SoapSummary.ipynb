{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93681f-de77-4655-9c29-bed3ed3000f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sample = pd.read_csv(\"sample_summary.csv\")\n",
    "# Display the first few rows\n",
    "print(df_sample.head())\n",
    "\n",
    "# Check DataFrame info\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8c6ee6-8e04-420f-92c3-e7d205c9e593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada0d6a4-e467-463a-98e1-81e95a3bbd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use token from environment variable (safer)\n",
    "login(os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fed48a8-65e6-420e-867d-25d8b5856368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:   0%|          | 0/100 [00:00<?, ?row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:   8%|▊         | 8/100 [00:04<00:50,  1.82row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  16%|█▌        | 16/100 [00:08<00:45,  1.85row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  24%|██▍       | 24/100 [00:12<00:40,  1.86row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  32%|███▏      | 32/100 [00:17<00:36,  1.87row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  40%|████      | 40/100 [00:21<00:32,  1.87row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  48%|████▊     | 48/100 [00:25<00:27,  1.89row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  56%|█████▌    | 56/100 [00:29<00:23,  1.91row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  64%|██████▍   | 64/100 [00:33<00:18,  1.90row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  72%|███████▏  | 72/100 [00:38<00:14,  1.89row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  80%|████████  | 80/100 [00:42<00:10,  1.89row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  88%|████████▊ | 88/100 [00:46<00:06,  1.89row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  96%|█████████▌| 96/100 [00:50<00:02,  1.88row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries: 100%|██████████| 100/100 [00:53<00:00,  1.88row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to 'Longformer_soap_generated_summaries.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Environment setup (optional but often helpful)\n",
    "# -----------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Load your model and tokenizer\n",
    "# -----------------------------------------------------\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example model name\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",           # Automatic GPU/CPU placement\n",
    "    torch_dtype=torch.float16     # Use FP16 for reduced memory usage\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Some LLaMA-based models need a special EOS token setup\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Define a prompt construction function\n",
    "# -----------------------------------------------------\n",
    "def construct_prompt(input_text):\n",
    "    \"\"\"\n",
    "    Constructs an instruction-based prompt for summarization.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Summarize the following case. \"\n",
    "        \"Do not include any extra or verbatim text from the input. \"\n",
    "        f\"Case:\\n{input_text}\\n\\nSummary:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Set your generation parameters\n",
    "# -----------------------------------------------------\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 1,\n",
    "    \"top_k\": 10,\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Load your sample DataFrame (df_sample) with columns \"input\" and \"output\"\n",
    "#    For example, if you've already saved and loaded your CSV:\n",
    "# -----------------------------------------------------\n",
    "# df_sample = pd.read_csv(\"sample_summary.csv\")\n",
    "# For demonstration, if you need to create a dummy DataFrame:\n",
    "# df_sample = pd.DataFrame({\"input\": [\"Your input text here...\"], \"output\": [\"Ground truth summary here...\"]})\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Summarize your df_sample DataFrame using partial decoding\n",
    "# -----------------------------------------------------\n",
    "batch_size = 8  # Adjust as needed\n",
    "inputs_list = df_sample[\"input\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "def process_batch(batch_inputs):\n",
    "    batch_generated = []\n",
    "    for text in batch_inputs:\n",
    "        prompt = construct_prompt(text)\n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1000)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Generate output tokens\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_params\n",
    "        )\n",
    "        # Slice out only the tokens that were generated after the prompt\n",
    "        generated_tokens = summary_ids[0, prompt_length:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        batch_generated.append(generated_text)\n",
    "    return batch_generated\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Process the DataFrame in batches with a progress bar\n",
    "# -----------------------------------------------------\n",
    "with tqdm(total=len(inputs_list), desc=\"Generating Summaries\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs_list), batch_size):\n",
    "        batch = inputs_list[i:i + batch_size]\n",
    "        try:\n",
    "            batch_generated = process_batch(batch)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Out of memory error; try reducing batch size.\")\n",
    "            raise e\n",
    "        generated_summaries.extend(batch_generated)\n",
    "        torch.cuda.empty_cache()\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 8. Store and Save\n",
    "# -----------------------------------------------------\n",
    "# Add generated summaries as a new column in df_sample\n",
    "df_sample[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Save the DataFrame with input, output, and generated_summary\n",
    "df_sample.to_csv(\"Longformer_soap_generated_summaries.csv\", index=False)\n",
    "print(\"Summaries saved to 'Longformer_soap_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f33705-b0e2-4095-b651-e2294352f112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
