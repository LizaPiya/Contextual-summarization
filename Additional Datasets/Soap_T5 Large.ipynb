{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1aa480-34b6-4e80-a923-475bb4c0e681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                              output  \n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...  \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...  \n",
      "2  Subjective:\\n- No known allergies to medicatio...  \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...  \n",
      "4  Subjective:\\n- Presenting with chest pain for ...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   100 non-null    object\n",
      " 1   output  100 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_sample = pd.read_csv(\"sample_summary.csv\")\n",
    "# Display the first few rows\n",
    "print(df_sample.head())\n",
    "\n",
    "# Check DataFrame info\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefa68f5-1823-4934-bd4b-32ed53b7365a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1333b5-0b74-4a01-a6cf-489ccf79af30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub langchain_community\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q neo4j\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c472cf78-63b0-4d7c-a9b5-242edc7f4a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b924029f-0aeb-4287-947b-247609573188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d66cfbbc8404444832255413f92facb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974fc258cbb14082ab7ed5225c48c45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862eb12bc84c426a88f69a25c441d090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c25850363dd4201aab999e9f55fa3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6c8031fd9540968dbc614b2b77a178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 inputs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.60s/row]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Summaries saved to 'Biobart_soap_generated_summaries.csv'\n",
      "\n",
      "üîπ Efficiency Metrics\n",
      "üìå Average Latency: 5.5957 sec (¬±3.2815)\n",
      "üìå Average Throughput: 253.13 tokens/sec (¬±189.37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load model and tokenizer\n",
    "# -------------------------------\n",
    "model_name = \"google-t5/t5-large\"  # You can replace this with your own fine-tuned model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load DataFrame (first 2 rows)\n",
    "# -------------------------------\n",
    "df_sample = df_sample.iloc[:2].copy()\n",
    "inputs_list = df_sample[\"input\"].tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Few-shot example + prompt constructor\n",
    "# -------------------------------\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"<SEX> F <SERVICE> ONCOLOGY <CHIEF COMPLAINT> worsening back pain <HISTORY OF PRESENT ILLNESS> The patient is a 45-year-old female with a history of metastatic breast cancer presenting with worsening back pain...\",\n",
    "        \"target\": \"A 45-year-old female with metastatic breast cancer presented with worsening back pain. Imaging showed thoracic spine fractures...\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def construct_prompt(input_text):\n",
    "    prompt = \"You are a medical expert. Please summarize the following input concisely:\\n\\n\"\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Input: {ex['input']}\\nTarget: {ex['target']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_text}\\nSummary:\"\n",
    "    return prompt\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Generation config\n",
    "# -------------------------------\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Summarization + Metrics\n",
    "# -------------------------------\n",
    "generated_summaries = []\n",
    "latencies, throughputs = [], []\n",
    "\n",
    "print(f\"Processing {len(inputs_list)} inputs...\\n\")\n",
    "\n",
    "for idx, text in enumerate(tqdm(inputs_list, desc=\"Generating Summaries\", unit=\"row\")):\n",
    "    prompt = construct_prompt(text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**inputs, **generation_params)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Extract generated tokens only\n",
    "    generated_tokens = outputs[0, prompt_length:]\n",
    "    summary = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Save results\n",
    "    generated_summaries.append(summary)\n",
    "\n",
    "    # Compute metrics\n",
    "    latency = end_time - start_time\n",
    "    total_tokens = prompt_length + generated_tokens.shape[0]\n",
    "    latencies.append(latency)\n",
    "    throughputs.append(total_tokens / latency)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Store & Evaluate\n",
    "# -------------------------------\n",
    "df_sample[\"generated_summary\"] = generated_summaries\n",
    "df_sample.to_csv(\"Biobart_soap_generated_summaries.csv\", index=False)\n",
    "print(\"‚úîÔ∏è Summaries saved to 'Biobart_soap_generated_summaries.csv'\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Print Efficiency Metrics\n",
    "# -------------------------------\n",
    "print(\"\\nüîπ Efficiency Metrics\")\n",
    "print(f\"üìå Average Latency: {np.mean(latencies):.4f} sec (¬±{np.std(latencies):.4f})\")\n",
    "print(f\"üìå Average Throughput: {np.mean(throughputs):.2f} tokens/sec (¬±{np.std(throughputs):.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa94ece3-1654-432e-bb3e-01a26d883b60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>generated_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon, champ, how you holding up? Goo...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Lower back pain, radi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What brings you in here today? Hi, I'm um, I'm...</td>\n",
       "      <td>Subjective:\\n- Presenting with dry cough for 1...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Good afternoon, champ, how you holding up? Goo...   \n",
       "1  What brings you in here today? Hi, I'm um, I'm...   \n",
       "\n",
       "                                              output generated_summary  \n",
       "0  Subjective:\\n- Symptoms: Lower back pain, radi...                    \n",
       "1  Subjective:\\n- Presenting with dry cough for 1...                    "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00bfdfc3-7082-4867-ba37-9b9bee51d6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summaries saved to 'summarization_output.csv'\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Define the local file path in SageMaker's instance storage\n",
    "output_csv_path = \"summarization_output.csv\"  # Saves in the current working directory\n",
    "\n",
    "# ‚úÖ Save the DataFrame to a CSV file\n",
    "mimic_iv_bhc_100.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# ‚úÖ Confirm file save location\n",
    "print(f\"\\nSummaries saved to '{output_csv_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2e590-ce21-4fa4-8aab-0cee07d3f251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
