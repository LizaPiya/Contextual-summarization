{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fa5dae-fa6a-42ae-8c4c-1d4c3b347a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Good afternoon, champ, how you holding up? Goo...   \n",
      "1  What brings you in here today? Hi, I'm um, I'm...   \n",
      "2  Do you have any known allergies to medications...   \n",
      "3  How may I help you today? Yeah I've had, a fev...   \n",
      "4  It sounds like that you're experiencing some c...   \n",
      "\n",
      "                                              output  \\\n",
      "0  Subjective:\\n- Symptoms: Lower back pain, radi...   \n",
      "1  Subjective:\\n- Presenting with dry cough for 1...   \n",
      "2  Subjective:\\n- No known allergies to medicatio...   \n",
      "3  Subjective:\\n- Fever and dry cough started 4 d...   \n",
      "4  Subjective:\\n- Presenting with chest pain for ...   \n",
      "\n",
      "                                   generated_summary  \n",
      "0  The 75-year-old man has been experiencing lowe...  \n",
      "1  , but as it went along, the smell started comi...  \n",
      "2  The defendant is charged with murder in connec...  \n",
      "3  that you could have contracted something, poss...  \n",
      "4  into this further and try to find out what's g...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   input              100 non-null    object\n",
      " 1   output             100 non-null    object\n",
      " 2   generated_summary  100 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_sample = pd.read_csv(\"soap_generated_summaries.csv\")\n",
    "# Display the first few rows\n",
    "print(df_sample.head())\n",
    "\n",
    "# Check DataFrame info\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a310404-1338-4919-8e4c-de8202473371",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdbb596-9340-410e-b730-44d92a883663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae9b13-c34b-4fde-96e8-d9e6867b571f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Use your Hugging Face token\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "356f355d-f4ea-4811-bd27-6579d659eebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries:   0%|          | 0/100 [00:00<?, ?row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:   8%|▊         | 8/100 [00:06<01:18,  1.18row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  16%|█▌        | 16/100 [00:13<01:08,  1.22row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  24%|██▍       | 24/100 [00:19<01:02,  1.23row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  32%|███▏      | 32/100 [00:26<00:55,  1.23row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  40%|████      | 40/100 [00:32<00:49,  1.22row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  48%|████▊     | 48/100 [00:39<00:43,  1.21row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  56%|█████▌    | 56/100 [00:46<00:37,  1.18row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  64%|██████▍   | 64/100 [00:52<00:29,  1.21row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  72%|███████▏  | 72/100 [01:00<00:23,  1.18row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  80%|████████  | 80/100 [01:07<00:17,  1.16row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  88%|████████▊ | 88/100 [01:13<00:10,  1.18row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries:  96%|█████████▌| 96/100 [01:20<00:03,  1.19row/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating Summaries: 100%|██████████| 100/100 [01:23<00:00,  1.19row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to 'soap_generated_summaries.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Environment setup (optional but often helpful)\n",
    "# -----------------------------------------------------\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Load your model and tokenizer\n",
    "# -----------------------------------------------------\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example model name\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",           # Automatic GPU/CPU placement\n",
    "    torch_dtype=torch.float16     # Use FP16 for reduced memory usage\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Some LLaMA-based models need a special EOS token setup\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Define a prompt construction function\n",
    "# -----------------------------------------------------\n",
    "def construct_prompt(input_text):\n",
    "    \"\"\"\n",
    "    Constructs an instruction-based prompt for summarization.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Summarize the following case. \"\n",
    "        \"Do not include any extra or verbatim text from the input. \"\n",
    "        f\"Case:\\n{input_text}\\n\\nSummary:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Set your generation parameters\n",
    "# -----------------------------------------------------\n",
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 10,\n",
    "    \"max_new_tokens\": 30,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Load your sample DataFrame (df_sample) with columns \"input\" and \"output\"\n",
    "#    For example, if you've already saved and loaded your CSV:\n",
    "# -----------------------------------------------------\n",
    "# df_sample = pd.read_csv(\"sample_summary.csv\")\n",
    "# For demonstration, if you need to create a dummy DataFrame:\n",
    "# df_sample = pd.DataFrame({\"input\": [\"Your input text here...\"], \"output\": [\"Ground truth summary here...\"]})\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Summarize your df_sample DataFrame using partial decoding\n",
    "# -----------------------------------------------------\n",
    "batch_size = 8  # Adjust as needed\n",
    "inputs_list = df_sample[\"input\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "def process_batch(batch_inputs):\n",
    "    batch_generated = []\n",
    "    for text in batch_inputs:\n",
    "        prompt = construct_prompt(text)\n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1000)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Generate output tokens\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_params\n",
    "        )\n",
    "        # Slice out only the tokens that were generated after the prompt\n",
    "        generated_tokens = summary_ids[0, prompt_length:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        batch_generated.append(generated_text)\n",
    "    return batch_generated\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Process the DataFrame in batches with a progress bar\n",
    "# -----------------------------------------------------\n",
    "with tqdm(total=len(inputs_list), desc=\"Generating Summaries\", unit=\"row\") as pbar:\n",
    "    for i in range(0, len(inputs_list), batch_size):\n",
    "        batch = inputs_list[i:i + batch_size]\n",
    "        try:\n",
    "            batch_generated = process_batch(batch)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Out of memory error; try reducing batch size.\")\n",
    "            raise e\n",
    "        generated_summaries.extend(batch_generated)\n",
    "        torch.cuda.empty_cache()\n",
    "        pbar.update(len(batch))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 8. Store and Save\n",
    "# -----------------------------------------------------\n",
    "# Add generated summaries as a new column in df_sample\n",
    "df_sample[\"generated_summary\"] = generated_summaries\n",
    "\n",
    "# Save the DataFrame with input, output, and generated_summary\n",
    "df_sample.to_csv(\"soap_generated_summaries.csv\", index=False)\n",
    "print(\"Summaries saved to 'soap_generated_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5657a504-83b0-47dc-9c77-e5c182d2a2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon, champ, how you holding up? Goo...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Lower back pain, radi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What brings you in here today? Hi, I'm um, I'm...</td>\n",
       "      <td>Subjective:\\n- Presenting with dry cough for 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Good afternoon, champ, how you holding up? Goo...   \n",
       "1  What brings you in here today? Hi, I'm um, I'm...   \n",
       "\n",
       "                                              output  \n",
       "0  Subjective:\\n- Symptoms: Lower back pain, radi...  \n",
       "1  Subjective:\\n- Presenting with dry cough for 1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb8d9795-6a81-4a6f-a7fc-2ff9f69466cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning, young lady, how old are you? Good morning, doctor. I'm thirteen. Good, and what seems to be the problem today? Mom, can you explain for me? Guest_family: Well, if you look, doctor, her back posture is very rounded. I think, it's rounding about the thoracic spine. Is there a family history of this problem? Guest_family: Yes, on my side, my aunt and grandfather had, um, kyphosis. Yes, that's what this is. This is thoracic kyphosis to be specific. Has she seen another doctor for this? Guest_family: Yes, we saw another orthopedist. What did they recommend? Guest_family: They recommended we come in for further observation, so we're here for a second opinion. Good, is there any back pain, numbness or tingling? No, I don't have any of that. Is there any weakness, numbness or tingling in your legs and arms, my dear? No, I'm very strong, especially for my age. Are you going to the bathroom with no problem? Yes, doctor, everything is regular there.\n"
     ]
    }
   ],
   "source": [
    "print(df_sample['input'].iloc[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98b8cc02-f6ee-4b31-893d-90000348a4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjective:\n",
      "- Patient is a 13-year-old girl.\n",
      "- Complaints of rounded back posture (thoracic spine).\n",
      "- Family history of kyphosis (aunt and grandfather).\n",
      "- No back pain, numbness, or tingling reported.\n",
      "- No weakness, numbness, or tingling in legs and arms.\n",
      "- Patient states she feels very strong for her age.\n",
      "- Regular bathroom habits reported.\n",
      "\n",
      "Objective:\n",
      "- Observed rounded back posture (thoracic kyphosis).\n",
      "\n",
      "Assessment:\n",
      "- Thoracic kyphosis.\n",
      "\n",
      "Plan:\n",
      "- Further observation as recommended by the previous orthopedist.\n",
      "- Consideration for a second opinion on management or treatment options.\n"
     ]
    }
   ],
   "source": [
    "print(df_sample['output'].iloc[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a038d55-a2c3-46fe-bf81-c4db98f34384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 13-year-old girl visits an orthopedic specialist because she has a rounded back posture (thoracic kyphosis) and her mother mentions that her aunt and grandfather had similar problems. The specialist recommends further observation and a second opinion, but the patient does not experience any symptoms such as back pain, numbness, or weakness.\n"
     ]
    }
   ],
   "source": [
    "print(df_sample['generated_summary'].iloc[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc1a1c-e019-4549-bbe2-3de4a1c4c845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
